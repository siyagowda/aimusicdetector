{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e2a677-50de-4dc3-bd8e-81cc7cdd6499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@imabhi1216/fine-tuning-a-pre-trained-resnet-18-model-for-image-classification-on-custom-dataset-with-pytorch-02df12e83c2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "701a2146-8100-411c-8b95-52ed0d453327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO\n"
     ]
    }
   ],
   "source": [
    "print(\"HELLO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d85019ff-7e24-4d26-b8e3-dc15ca22aeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0bf7ce45-40ae-420c-8ed9-8205ce733db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    # Load the pre-trained ResNet-18 model\n",
    "    model = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "\n",
    "    # Modify the last layer of the model\n",
    "    num_classes = 2 # number of classes in dataset\n",
    "    model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "993bbd8e-5a49-444b-9113-ec87ffa5c9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 22736\n",
      "Validation set size: 4871\n",
      "Test set size: 4875\n",
      "AI Aug Test set size: 14149\n"
     ]
    }
   ],
   "source": [
    "# Directory paths for the segments and lyrics\n",
    "ai_segments_path = \"/vol/bitbucket/sg2121/fypdataset/dataset_large2/normal_data/ai_segments\"\n",
    "human_segments_path = \"/vol/bitbucket/sg2121/fypdataset/dataset_large2/normal_data/human\"\n",
    "ai_plp_path = \"/vol/bitbucket/sg2121/fypdataset/dataset_large2/features/ai/PLP\"\n",
    "human_plp_path = \"/vol/bitbucket/sg2121/fypdataset/dataset_large2/features/human/PLP\"\n",
    "\n",
    "ai_aug_segments_path = \"/vol/bitbucket/sg2121/fypdataset/dataset_large2/normal_data/augmented_ai\"\n",
    "ai_aug_plp_path = \"/vol/bitbucket/sg2121/fypdataset/dataset_large2/features/ai_aug/PLP\"\n",
    "\n",
    "# List to store the results\n",
    "ai_aug_test_files = []\n",
    "\n",
    "# Loop through files in the directory\n",
    "for filename in os.listdir(ai_aug_plp_path):\n",
    "    if filename.endswith(\".png\"):\n",
    "        full_path = os.path.join(ai_aug_plp_path, filename)\n",
    "        ai_aug_test_files.append((full_path, 1))\n",
    "\n",
    "# Helper function to read file paths from a text file\n",
    "def read_file_paths(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        return [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Read all file paths from the text files\n",
    "train_files = read_file_paths('/vol/bitbucket/sg2121/fyp/aimusicdetector/train_test_split/bitbucket/train_files_large.txt')\n",
    "val_files = read_file_paths('/vol/bitbucket/sg2121/fyp/aimusicdetector/train_test_split/bitbucket/val_files_large.txt')\n",
    "test_files = read_file_paths('/vol/bitbucket/sg2121/fyp/aimusicdetector/train_test_split/bitbucket/test_files_large.txt')\n",
    "\n",
    "# Function to convert segment file path to lyric file path\n",
    "def convert_to_plp_path(file_path, is_ai):\n",
    "    if is_ai:\n",
    "        if file_path.startswith(ai_segments_path):\n",
    "            base_plp_path = ai_plp_path\n",
    "        elif file_path.startswith(ai_aug_segments_path):\n",
    "            base_plp_path = ai_aug_plp_path\n",
    "        else:\n",
    "            return\n",
    "    else:\n",
    "        if file_path.startswith(human_segments_path):\n",
    "            base_plp_path = human_plp_path\n",
    "        else:\n",
    "            return\n",
    "\n",
    "    # Convert filename to plp filename\n",
    "    file_name = os.path.basename(file_path).replace('.mp3', '_plp.png')\n",
    "    return os.path.join(base_plp_path, file_name)\n",
    "\n",
    "\n",
    "# Process the file lists and create tuples of (lyric_path, label)\n",
    "def process_file_paths(file_paths, is_ai):\n",
    "    return [(convert_to_plp_path(file_path, is_ai), 0 if is_ai else 1) for file_path in file_paths]\n",
    "\n",
    "# Convert all file paths from the train, validation, and test sets\n",
    "ai_train_files = process_file_paths(train_files, is_ai=True)\n",
    "human_train_files = process_file_paths(train_files, is_ai=False)\n",
    "\n",
    "ai_val_files = process_file_paths(val_files, is_ai=True)\n",
    "human_val_files = process_file_paths(val_files, is_ai=False)\n",
    "\n",
    "ai_test_files = process_file_paths(test_files, is_ai=True)\n",
    "human_test_files = process_file_paths(test_files, is_ai=False)\n",
    "\n",
    "def clean(paths):\n",
    "    return [(p, l) for p, l in paths if p is not None]\n",
    "\n",
    "train_files_combined = clean(ai_train_files) + clean(human_train_files)\n",
    "val_files_combined = clean(ai_val_files) + clean(human_val_files)\n",
    "test_files_combined = clean(ai_test_files) + clean(human_test_files)\n",
    "\n",
    "# Shuffle the data if needed\n",
    "random.shuffle(train_files_combined)\n",
    "random.shuffle(val_files_combined)\n",
    "random.shuffle(test_files_combined)\n",
    "random.shuffle(ai_aug_test_files)\n",
    "\n",
    "# Example of how you might check the splits\n",
    "print(f\"Training set size: {len(train_files_combined)}\")\n",
    "print(f\"Validation set size: {len(val_files_combined)}\")\n",
    "print(f\"Test set size: {len(test_files_combined)}\")\n",
    "print(f\"AI Aug Test set size: {len(ai_aug_test_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "694aa3d8-3fd9-4268-8664-0c36ffb8d528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class\n",
    "class PLPDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.data[idx]\n",
    "        filename = os.path.basename(path)\n",
    "\n",
    "        try:\n",
    "            image = Image.open(path).convert('RGB')\n",
    "        except FileNotFoundError:\n",
    "            print(f\"{filename} not found.\")\n",
    "            raise IndexError(f\"{filename} not found\")  # Or raise a dummy image if you want to proceed\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label, filename\n",
    "        \n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e897ebf9-9a1c-4e00-b124-3467a46b31b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 22736 samples\n",
      "Val set: 4871 samples\n",
      "Test set: 4875 samples\n",
      "AI Aug Test set: 14149 samples\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "train_dataset = PLPDataset(train_files_combined, transform=transform)\n",
    "val_dataset = PLPDataset(val_files_combined, transform=transform)\n",
    "test_dataset = PLPDataset(test_files_combined, transform=transform)\n",
    "\n",
    "ai_aug_test_dataset = PLPDataset(ai_aug_test_files, transform=transform)\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "ai_aug_test_loader = DataLoader(ai_aug_test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Confirm sizes\n",
    "print(f\"Train set: {len(train_dataset)} samples\")\n",
    "print(f\"Val set: {len(val_dataset)} samples\")\n",
    "print(f\"Test set: {len(test_dataset)} samples\")\n",
    "print(f\"AI Aug Test set: {len(ai_aug_test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "860831a5-b6b9-4b45-90a5-1c1ab10bbb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Determine whether to use GPU (if available) or CPU\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, num_epochs):\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Set the model to training mode\n",
    "        model.train()\n",
    "\n",
    "        # Initialize running loss and correct predictions count for training\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        # Iterate over the training data loader\n",
    "        for inputs, labels, filenames in train_loader:\n",
    "            # Move inputs and labels to the device (GPU or CPU)\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Reset the gradients to zero before the backward pass\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass: compute the model output\n",
    "            outputs = model(inputs)\n",
    "            # Get the predicted class (with the highest score)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            # Compute the loss between the predictions and actual labels\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass: compute gradients\n",
    "            loss.backward()\n",
    "            # Perform the optimization step to update model parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate the running loss and the number of correct predictions\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        # Compute average training loss and accuracy for this epoch\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_acc = running_corrects.float() / len(train_loader.dataset)\n",
    "\n",
    "        # Set the model to evaluation mode for validation\n",
    "        model.eval()\n",
    "        # Initialize running loss and correct predictions count for validation\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        # Disable gradient computation for validation (saves memory and computations)\n",
    "        with torch.no_grad():\n",
    "            # Iterate over the validation data loader\n",
    "            for inputs, labels, filenames in val_loader:\n",
    "                # Move inputs and labels to the device (GPU or CPU)\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Forward pass: compute the model output\n",
    "                outputs = model(inputs)\n",
    "                # Get the predicted class (with the highest score)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                # Compute the loss between the predictions and actual labels\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Accumulate the running loss and the number of correct predictions\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        # Compute average validation loss and accuracy for this epoch\n",
    "        val_loss = running_loss / len(val_loader.dataset)\n",
    "        val_acc = running_corrects.float() / len(val_loader.dataset)\n",
    "\n",
    "        # Print the results for the current epoch\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], train loss: {train_loss:.4f}, train acc: {train_acc:.4f}, val loss: {val_loss:.4f}, val acc: {val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "701d704d-0000-4434-b08f-aa84e280e869",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "lr = 0.001\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.fc.parameters(), lr, momentum=0.9)\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "71db891f-d270-4589-8332-cb41f0a21e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "lr = 0.001\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.fc.parameters(), lr, momentum=0.9, weight_decay=1e-07)\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df85e827-8692-42e7-9773-44f12a8096b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], train loss: 0.4282, train acc: 0.8129, val loss: 0.3827, val acc: 0.8411\n",
      "Epoch [2/5], train loss: 0.3974, train acc: 0.8338, val loss: 0.3818, val acc: 0.8407\n",
      "Epoch [3/5], train loss: 0.3951, train acc: 0.8318, val loss: 0.3805, val acc: 0.8409\n",
      "Epoch [4/5], train loss: 0.3899, train acc: 0.8354, val loss: 0.3856, val acc: 0.8436\n",
      "Epoch [5/5], train loss: 0.3881, train acc: 0.8368, val loss: 0.3772, val acc: 0.8436\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, val_loader, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "caa2aded-43e3-457f-843e-4f1439615d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device, hyperparams=None):\n",
    "    log_file = \"/vol/bitbucket/sg2121/fyp/aimusicdetector/music_cnn/large/plp/training_large_logfile.txt\"\n",
    "\n",
    "    model.eval()\n",
    "    correct_pred = {classname: 0 for classname in ['ai', 'human']}\n",
    "    total_pred = {classname: 0 for classname in ['ai', 'human']}\n",
    "\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, filenames in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "            for label, prediction in zip(labels, preds):\n",
    "                classname = 'ai' if label.item() == 0 else 'human'\n",
    "                if label == prediction:\n",
    "                    correct_pred[classname] += 1\n",
    "                total_pred[classname] += 1\n",
    "\n",
    "    end_time = time.time()\n",
    "    avg_inference_time = (end_time - start_time) / len(test_loader.dataset)\n",
    "\n",
    "    accuracy_per_class = {\n",
    "        classname: correct_pred[classname] / total_pred[classname]\n",
    "        if total_pred[classname] > 0 else 0\n",
    "        for classname in ['ai', 'human']\n",
    "    }\n",
    "\n",
    "    overall_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=None, labels=[0, 1])\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    fpr = {}\n",
    "    for i, classname in enumerate(['ai', 'human']):\n",
    "        FP = cm[:, i].sum() - cm[i, i]\n",
    "        TN = cm.sum() - (cm[i, :].sum() + cm[:, i].sum() - cm[i, i])\n",
    "        fpr[classname] = FP / (FP + TN) if (FP + TN) > 0 else 0\n",
    "\n",
    "    # Logging\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    log_lines = [f\"===== Evaluation at {timestamp} =====\\n\"]\n",
    "\n",
    "    if hyperparams:\n",
    "        log_lines.append(\"Hyperparameters:\")\n",
    "        for key, value in hyperparams.items():\n",
    "            log_lines.append(f\"{key}: {value}\")\n",
    "    else:\n",
    "        log_lines.append(\"No hyperparameters provided.\")\n",
    "    log_lines.append(\"\")\n",
    "\n",
    "    log_lines.append(\"Accuracy per class:\")\n",
    "    for classname, acc in accuracy_per_class.items():\n",
    "        log_lines.append(f\"{classname}: {acc:.4f}\")\n",
    "    log_lines.append(\"\\nPrecision, Recall, F1:\")\n",
    "    for i, classname in enumerate(['ai', 'human']):\n",
    "        log_lines.append(f\"{classname} → Precision: {precision[i]:.4f}, Recall: {recall[i]:.4f}, F1: {f1[i]:.4f}\")\n",
    "    log_lines.append(\"\\nFalse Positive Rate:\")\n",
    "    for classname, rate in fpr.items():\n",
    "        log_lines.append(f\"{classname}: {rate:.4f}\")\n",
    "    log_lines.append(f\"\\nOverall Accuracy: {overall_accuracy:.4f}\")\n",
    "    log_lines.append(f\"Average Inference Time per Sample: {avg_inference_time:.6f} seconds\")\n",
    "    log_lines.append(\"=\" * 40 + \"\\n\\n\")\n",
    "\n",
    "    print(\"\\n\".join(log_lines))\n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(\"\\n\".join(log_lines))\n",
    "\n",
    "    return overall_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e0ba7486-9b85-46cc-89b5-a83662fe7f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▎                                          | 1/153 [00:03<08:40,  3.42s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m model.eval()\n\u001b[32m      2\u001b[39m results = []\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Unpack the filename from the dataset\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/sg2121/fyp/aimusicdetector/venvNew/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/sg2121/fyp/aimusicdetector/venvNew/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/sg2121/fyp/aimusicdetector/venvNew/lib/python3.12/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/sg2121/fyp/aimusicdetector/venvNew/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mPLPDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# Or raise a dummy image if you want to proceed\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform:\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     image = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m image, label, filename\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/sg2121/fyp/aimusicdetector/venvNew/lib/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/sg2121/fyp/aimusicdetector/venvNew/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/sg2121/fyp/aimusicdetector/venvNew/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/sg2121/fyp/aimusicdetector/venvNew/lib/python3.12/site-packages/torchvision/transforms/transforms.py:354\u001b[39m, in \u001b[36mResize.forward\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m    347\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    348\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    349\u001b[39m \u001b[33;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    352\u001b[39m \u001b[33;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/sg2121/fyp/aimusicdetector/venvNew/lib/python3.12/site-packages/torchvision/transforms/functional.py:477\u001b[39m, in \u001b[36mresize\u001b[39m\u001b[34m(img, size, interpolation, max_size, antialias)\u001b[39m\n\u001b[32m    475\u001b[39m         warnings.warn(\u001b[33m\"\u001b[39m\u001b[33mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    476\u001b[39m     pil_interpolation = pil_modes_mapping[interpolation]\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m F_t.resize(img, size=output_size, interpolation=interpolation.value, antialias=antialias)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/sg2121/fyp/aimusicdetector/venvNew/lib/python3.12/site-packages/torchvision/transforms/_functional_pil.py:250\u001b[39m, in \u001b[36mresize\u001b[39m\u001b[34m(img, size, interpolation)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) == \u001b[32m2\u001b[39m):\n\u001b[32m    248\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/sg2121/fyp/aimusicdetector/venvNew/lib/python3.12/site-packages/PIL/Image.py:2316\u001b[39m, in \u001b[36mImage.resize\u001b[39m\u001b[34m(self, size, resample, box, reducing_gap)\u001b[39m\n\u001b[32m   2304\u001b[39m         \u001b[38;5;28mself\u001b[39m = (\n\u001b[32m   2305\u001b[39m             \u001b[38;5;28mself\u001b[39m.reduce(factor, box=reduce_box)\n\u001b[32m   2306\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.reduce)\n\u001b[32m   2307\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m Image.reduce(\u001b[38;5;28mself\u001b[39m, factor, box=reduce_box)\n\u001b[32m   2308\u001b[39m         )\n\u001b[32m   2309\u001b[39m         box = (\n\u001b[32m   2310\u001b[39m             (box[\u001b[32m0\u001b[39m] - reduce_box[\u001b[32m0\u001b[39m]) / factor_x,\n\u001b[32m   2311\u001b[39m             (box[\u001b[32m1\u001b[39m] - reduce_box[\u001b[32m1\u001b[39m]) / factor_y,\n\u001b[32m   2312\u001b[39m             (box[\u001b[32m2\u001b[39m] - reduce_box[\u001b[32m0\u001b[39m]) / factor_x,\n\u001b[32m   2313\u001b[39m             (box[\u001b[32m3\u001b[39m] - reduce_box[\u001b[32m1\u001b[39m]) / factor_y,\n\u001b[32m   2314\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m2316\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "results = []\n",
    "\n",
    "for batch in tqdm(test_loader):\n",
    "    inputs, labels, filenames = batch  # Unpack the filename from the dataset\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "        for i in range(len(filenames)):\n",
    "            results.append({\n",
    "                \"filename\": filenames[i],  # Use filename directly from dataset\n",
    "                \"prob_ai\": probs[i][0].item(),\n",
    "                \"prob_human\": probs[i][1].item(),\n",
    "                \"true_label\": labels[i].item(),\n",
    "                \"pred_label\": torch.argmax(probs[i]).item()\n",
    "            })\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"plp_test_large_predictions.csv\", index=False)\n",
    "\n",
    "# Preview results\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c0573b-f029-41a8-956e-3542648e72b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'cur_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fab43cab-32a6-4f62-b748-2c6c3651ce78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Evaluation at 2025-05-28 00:57:52 =====\n",
      "\n",
      "Hyperparameters:\n",
      "batch_size: 32\n",
      "learning_rate: 0.001\n",
      "epochs: 5\n",
      "optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 1e-07\n",
      ")\n",
      "model: ResNet\n",
      "\n",
      "Accuracy per class:\n",
      "ai: 0.4538\n",
      "human: 0.9509\n",
      "\n",
      "Precision, Recall, F1:\n",
      "ai → Precision: 0.7353, Recall: 0.4538, F1: 0.5612\n",
      "human → Precision: 0.8529, Recall: 0.9509, F1: 0.8992\n",
      "\n",
      "False Positive Rate:\n",
      "ai: 0.0491\n",
      "human: 0.5462\n",
      "\n",
      "Overall Accuracy: 0.8361\n",
      "Average Inference Time per Sample: 0.267669 seconds\n",
      "========================================\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8361025641025641"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparams = {\n",
    "    \"batch_size\": batch_size,\n",
    "    \"learning_rate\": lr,\n",
    "    \"epochs\": num_epochs,\n",
    "    \"optimizer\": optimizer,\n",
    "    \"model\": model.__class__.__name__,\n",
    "}\n",
    "\n",
    "evaluate_model(model, test_loader, device, hyperparams=hyperparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63811d05-382c-47a0-884e-54d45252bc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPERPARANM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ed1c5e6-f878-46b7-ab31-2d0fa6405b4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Trial 1/10: LR=0.0001, WD=1e-08, Epochs=10 ===\n",
      "Epoch [1/10], train loss: 0.5028, train acc: 0.7453, val loss: 0.4157, val acc: 0.8004\n",
      "Epoch [2/10], train loss: 0.3842, train acc: 0.8239, val loss: 0.3754, val acc: 0.8299\n",
      "Epoch [3/10], train loss: 0.3391, train acc: 0.8492, val loss: 0.3602, val acc: 0.8344\n",
      "Epoch [4/10], train loss: 0.3021, train acc: 0.8678, val loss: 0.3448, val acc: 0.8416\n",
      "Epoch [5/10], train loss: 0.2621, train acc: 0.8891, val loss: 0.3620, val acc: 0.8425\n",
      "Epoch [6/10], train loss: 0.2333, train acc: 0.9015, val loss: 0.3576, val acc: 0.8416\n",
      "Epoch [7/10], train loss: 0.1956, train acc: 0.9220, val loss: 0.3600, val acc: 0.8470\n",
      "Epoch [8/10], train loss: 0.1630, train acc: 0.9381, val loss: 0.3674, val acc: 0.8422\n",
      "Epoch [9/10], train loss: 0.1328, train acc: 0.9506, val loss: 0.4027, val acc: 0.8335\n",
      "Epoch [10/10], train loss: 0.1041, train acc: 0.9655, val loss: 0.4312, val acc: 0.8353\n",
      "===== Evaluation at 2025-05-12 23:16:39 =====\n",
      "\n",
      "Hyperparameters:\n",
      "batch_size: 32\n",
      "learning_rate: 0.0001\n",
      "epochs: 10\n",
      "optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 1e-08\n",
      ")\n",
      "model: ResNet\n",
      "\n",
      "Accuracy per class:\n",
      "ai: 0.8242\n",
      "human: 0.8549\n",
      "\n",
      "Precision, Recall, F1:\n",
      "ai → Precision: 0.9095, Recall: 0.8242, F1: 0.8648\n",
      "human → Precision: 0.7332, Recall: 0.8549, F1: 0.7894\n",
      "\n",
      "False Positive Rate:\n",
      "ai: 0.1451\n",
      "human: 0.1758\n",
      "\n",
      "Overall Accuracy: 0.8353\n",
      "Average Inference Time per Sample: 0.117643 seconds\n",
      "========================================\n",
      "\n",
      "\n",
      "\n",
      "=== Trial 2/10: LR=0.001, WD=1e-06, Epochs=10 ===\n",
      "Epoch [1/10], train loss: 0.4118, train acc: 0.8053, val loss: 0.4118, val acc: 0.8067\n",
      "Epoch [2/10], train loss: 0.3062, train acc: 0.8633, val loss: 0.3334, val acc: 0.8576\n",
      "Epoch [3/10], train loss: 0.2388, train acc: 0.8970, val loss: 0.3169, val acc: 0.8693\n",
      "Epoch [4/10], train loss: 0.1862, train acc: 0.9227, val loss: 0.3511, val acc: 0.8555\n",
      "Epoch [5/10], train loss: 0.1450, train acc: 0.9405, val loss: 0.3432, val acc: 0.8717\n",
      "Epoch [6/10], train loss: 0.0920, train acc: 0.9642, val loss: 0.3916, val acc: 0.8687\n",
      "Epoch [7/10], train loss: 0.0771, train acc: 0.9728, val loss: 0.4258, val acc: 0.8678\n",
      "Epoch [8/10], train loss: 0.0797, train acc: 0.9699, val loss: 0.4415, val acc: 0.8714\n",
      "Epoch [9/10], train loss: 0.0478, train acc: 0.9826, val loss: 0.4594, val acc: 0.8729\n",
      "Epoch [10/10], train loss: 0.0523, train acc: 0.9792, val loss: 0.5135, val acc: 0.8618\n",
      "===== Evaluation at 2025-05-13 05:00:48 =====\n",
      "\n",
      "Hyperparameters:\n",
      "batch_size: 32\n",
      "learning_rate: 0.001\n",
      "epochs: 10\n",
      "optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 1e-06\n",
      ")\n",
      "model: ResNet\n",
      "\n",
      "Accuracy per class:\n",
      "ai: 0.8869\n",
      "human: 0.8173\n",
      "\n",
      "Precision, Recall, F1:\n",
      "ai → Precision: 0.8958, Recall: 0.8869, F1: 0.8913\n",
      "human → Precision: 0.8033, Recall: 0.8173, F1: 0.8103\n",
      "\n",
      "False Positive Rate:\n",
      "ai: 0.1827\n",
      "human: 0.1131\n",
      "\n",
      "Overall Accuracy: 0.8618\n",
      "Average Inference Time per Sample: 0.093313 seconds\n",
      "========================================\n",
      "\n",
      "\n",
      "\n",
      "=== Trial 3/10: LR=0.0005, WD=1e-06, Epochs=20 ===\n",
      "Epoch [1/20], train loss: 0.4251, train acc: 0.7998, val loss: 0.3729, val acc: 0.8320\n",
      "Epoch [2/20], train loss: 0.3178, train acc: 0.8604, val loss: 0.3539, val acc: 0.8368\n",
      "Epoch [3/20], train loss: 0.2522, train acc: 0.8937, val loss: 0.3193, val acc: 0.8630\n",
      "Epoch [4/20], train loss: 0.1893, train acc: 0.9212, val loss: 0.3734, val acc: 0.8455\n",
      "Epoch [5/20], train loss: 0.1328, train acc: 0.9470, val loss: 0.3865, val acc: 0.8549\n",
      "Epoch [6/20], train loss: 0.0934, train acc: 0.9649, val loss: 0.4040, val acc: 0.8549\n",
      "Epoch [7/20], train loss: 0.0756, train acc: 0.9706, val loss: 0.4577, val acc: 0.8678\n",
      "Epoch [8/20], train loss: 0.0551, train acc: 0.9807, val loss: 0.4714, val acc: 0.8452\n",
      "Epoch [9/20], train loss: 0.0449, train acc: 0.9837, val loss: 0.5086, val acc: 0.8525\n",
      "Epoch [10/20], train loss: 0.0399, train acc: 0.9863, val loss: 0.5021, val acc: 0.8585\n",
      "Epoch [11/20], train loss: 0.0286, train acc: 0.9899, val loss: 0.5233, val acc: 0.8657\n",
      "Epoch [12/20], train loss: 0.0300, train acc: 0.9897, val loss: 0.5938, val acc: 0.8467\n",
      "Epoch [13/20], train loss: 0.0264, train acc: 0.9901, val loss: 0.5503, val acc: 0.8627\n",
      "Epoch [14/20], train loss: 0.0224, train acc: 0.9919, val loss: 0.5840, val acc: 0.8663\n",
      "Epoch [15/20], train loss: 0.0181, train acc: 0.9934, val loss: 0.5557, val acc: 0.8666\n",
      "Epoch [16/20], train loss: 0.0189, train acc: 0.9930, val loss: 0.5492, val acc: 0.8675\n",
      "Epoch [17/20], train loss: 0.0092, train acc: 0.9975, val loss: 0.9360, val acc: 0.8046\n",
      "Epoch [18/20], train loss: 0.0191, train acc: 0.9934, val loss: 0.5795, val acc: 0.8747\n",
      "Epoch [19/20], train loss: 0.0166, train acc: 0.9948, val loss: 0.5701, val acc: 0.8777\n",
      "Epoch [20/20], train loss: 0.0108, train acc: 0.9967, val loss: 0.5875, val acc: 0.8666\n",
      "===== Evaluation at 2025-05-13 14:57:37 =====\n",
      "\n",
      "Hyperparameters:\n",
      "batch_size: 32\n",
      "learning_rate: 0.0005\n",
      "epochs: 20\n",
      "optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0005\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 1e-06\n",
      ")\n",
      "model: ResNet\n",
      "\n",
      "Accuracy per class:\n",
      "ai: 0.8902\n",
      "human: 0.8249\n",
      "\n",
      "Precision, Recall, F1:\n",
      "ai → Precision: 0.9000, Recall: 0.8902, F1: 0.8950\n",
      "human → Precision: 0.8093, Recall: 0.8249, F1: 0.8170\n",
      "\n",
      "False Positive Rate:\n",
      "ai: 0.1751\n",
      "human: 0.1098\n",
      "\n",
      "Overall Accuracy: 0.8666\n",
      "Average Inference Time per Sample: 0.092838 seconds\n",
      "========================================\n",
      "\n",
      "\n",
      "\n",
      "=== Trial 4/10: LR=0.0001, WD=1e-07, Epochs=10 ===\n",
      "Epoch [1/10], train loss: 0.4945, train acc: 0.7531, val loss: 0.4079, val acc: 0.8121\n",
      "Epoch [2/10], train loss: 0.3870, train acc: 0.8228, val loss: 0.3771, val acc: 0.8320\n",
      "Epoch [3/10], train loss: 0.3436, train acc: 0.8465, val loss: 0.3518, val acc: 0.8413\n",
      "Epoch [4/10], train loss: 0.3021, train acc: 0.8690, val loss: 0.3540, val acc: 0.8425\n",
      "Epoch [5/10], train loss: 0.2747, train acc: 0.8815, val loss: 0.3399, val acc: 0.8522\n",
      "Epoch [6/10], train loss: 0.2384, train acc: 0.9033, val loss: 0.3302, val acc: 0.8573\n",
      "Epoch [7/10], train loss: 0.2033, train acc: 0.9170, val loss: 0.3396, val acc: 0.8543\n",
      "Epoch [8/10], train loss: 0.1702, train acc: 0.9351, val loss: 0.3468, val acc: 0.8552\n",
      "Epoch [9/10], train loss: 0.1382, train acc: 0.9487, val loss: 0.3418, val acc: 0.8609\n",
      "Epoch [10/10], train loss: 0.1113, train acc: 0.9619, val loss: 0.3681, val acc: 0.8597\n",
      "===== Evaluation at 2025-05-13 20:04:19 =====\n",
      "\n",
      "Hyperparameters:\n",
      "batch_size: 32\n",
      "learning_rate: 0.0001\n",
      "epochs: 10\n",
      "optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 1e-07\n",
      ")\n",
      "model: ResNet\n",
      "\n",
      "Accuracy per class:\n",
      "ai: 0.8968\n",
      "human: 0.7940\n",
      "\n",
      "Precision, Recall, F1:\n",
      "ai → Precision: 0.8851, Recall: 0.8968, F1: 0.8909\n",
      "human → Precision: 0.8130, Recall: 0.7940, F1: 0.8034\n",
      "\n",
      "False Positive Rate:\n",
      "ai: 0.2060\n",
      "human: 0.1032\n",
      "\n",
      "Overall Accuracy: 0.8597\n",
      "Average Inference Time per Sample: 0.096026 seconds\n",
      "========================================\n",
      "\n",
      "\n",
      "\n",
      "=== Trial 5/10: LR=0.001, WD=1e-07, Epochs=10 ===\n",
      "Epoch [1/10], train loss: 0.4184, train acc: 0.8008, val loss: 0.3611, val acc: 0.8398\n",
      "Epoch [2/10], train loss: 0.3079, train acc: 0.8633, val loss: 0.3211, val acc: 0.8525\n",
      "Epoch [3/10], train loss: 0.2385, train acc: 0.8983, val loss: 0.3449, val acc: 0.8455\n",
      "Epoch [4/10], train loss: 0.1829, train acc: 0.9252, val loss: 0.3453, val acc: 0.8765\n",
      "Epoch [5/10], train loss: 0.1253, train acc: 0.9508, val loss: 0.3585, val acc: 0.8645\n",
      "Epoch [6/10], train loss: 0.1040, train acc: 0.9593, val loss: 0.4101, val acc: 0.8612\n",
      "Epoch [7/10], train loss: 0.0832, train acc: 0.9674, val loss: 0.5245, val acc: 0.8597\n",
      "Epoch [8/10], train loss: 0.0651, train acc: 0.9769, val loss: 0.4430, val acc: 0.8645\n",
      "Epoch [9/10], train loss: 0.0421, train acc: 0.9854, val loss: 0.5195, val acc: 0.8594\n",
      "Epoch [10/10], train loss: 0.0389, train acc: 0.9866, val loss: 0.4727, val acc: 0.8672\n",
      "===== Evaluation at 2025-05-14 01:58:27 =====\n",
      "\n",
      "Hyperparameters:\n",
      "batch_size: 32\n",
      "learning_rate: 0.001\n",
      "epochs: 10\n",
      "optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 1e-07\n",
      ")\n",
      "model: ResNet\n",
      "\n",
      "Accuracy per class:\n",
      "ai: 0.8893\n",
      "human: 0.8282\n",
      "\n",
      "Precision, Recall, F1:\n",
      "ai → Precision: 0.9016, Recall: 0.8893, F1: 0.8954\n",
      "human → Precision: 0.8086, Recall: 0.8282, F1: 0.8183\n",
      "\n",
      "False Positive Rate:\n",
      "ai: 0.1718\n",
      "human: 0.1107\n",
      "\n",
      "Overall Accuracy: 0.8672\n",
      "Average Inference Time per Sample: 0.111101 seconds\n",
      "========================================\n",
      "\n",
      "\n",
      "\n",
      "=== Trial 6/10: LR=0.0005, WD=1e-07, Epochs=10 ===\n",
      "Epoch [1/10], train loss: 0.4315, train acc: 0.7951, val loss: 0.4277, val acc: 0.7977\n",
      "Epoch [2/10], train loss: 0.3253, train acc: 0.8557, val loss: 0.3497, val acc: 0.8491\n",
      "Epoch [3/10], train loss: 0.2529, train acc: 0.8922, val loss: 0.3345, val acc: 0.8558\n",
      "Epoch [4/10], train loss: 0.1941, train acc: 0.9201, val loss: 0.3682, val acc: 0.8594\n",
      "Epoch [5/10], train loss: 0.1356, train acc: 0.9464, val loss: 0.3625, val acc: 0.8729\n",
      "Epoch [6/10], train loss: 0.0971, train acc: 0.9638, val loss: 0.4000, val acc: 0.8615\n",
      "Epoch [7/10], train loss: 0.0687, train acc: 0.9754, val loss: 0.4994, val acc: 0.8428\n",
      "Epoch [8/10], train loss: 0.0621, train acc: 0.9763, val loss: 0.4999, val acc: 0.8446\n",
      "Epoch [9/10], train loss: 0.0404, train acc: 0.9864, val loss: 0.5565, val acc: 0.8540\n",
      "Epoch [10/10], train loss: 0.0412, train acc: 0.9853, val loss: 0.5412, val acc: 0.8419\n",
      "===== Evaluation at 2025-05-14 07:53:31 =====\n",
      "\n",
      "Hyperparameters:\n",
      "batch_size: 32\n",
      "learning_rate: 0.0005\n",
      "epochs: 10\n",
      "optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0005\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 1e-07\n",
      ")\n",
      "model: ResNet\n",
      "\n",
      "Accuracy per class:\n",
      "ai: 0.8261\n",
      "human: 0.8699\n",
      "\n",
      "Precision, Recall, F1:\n",
      "ai → Precision: 0.9183, Recall: 0.8261, F1: 0.8698\n",
      "human → Precision: 0.7387, Recall: 0.8699, F1: 0.7989\n",
      "\n",
      "False Positive Rate:\n",
      "ai: 0.1301\n",
      "human: 0.1739\n",
      "\n",
      "Overall Accuracy: 0.8419\n",
      "Average Inference Time per Sample: 0.093652 seconds\n",
      "========================================\n",
      "\n",
      "\n",
      "\n",
      "=== Trial 7/10: LR=0.001, WD=1e-07, Epochs=5 ===\n",
      "Epoch [1/5], train loss: 0.4122, train acc: 0.8059, val loss: 0.3454, val acc: 0.8437\n",
      "Epoch [2/5], train loss: 0.3025, train acc: 0.8653, val loss: 0.3039, val acc: 0.8633\n",
      "Epoch [3/5], train loss: 0.2359, train acc: 0.8992, val loss: 0.3177, val acc: 0.8612\n",
      "Epoch [4/5], train loss: 0.1737, train acc: 0.9284, val loss: 0.3479, val acc: 0.8600\n",
      "Epoch [5/5], train loss: 0.1256, train acc: 0.9481, val loss: 0.4406, val acc: 0.8672\n",
      "===== Evaluation at 2025-05-14 12:31:24 =====\n",
      "\n",
      "Hyperparameters:\n",
      "batch_size: 32\n",
      "learning_rate: 0.001\n",
      "epochs: 5\n",
      "optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 1e-07\n",
      ")\n",
      "model: ResNet\n",
      "\n",
      "Accuracy per class:\n",
      "ai: 0.8596\n",
      "human: 0.8807\n",
      "\n",
      "Precision, Recall, F1:\n",
      "ai → Precision: 0.9273, Recall: 0.8596, F1: 0.8921\n",
      "human → Precision: 0.7799, Recall: 0.8807, F1: 0.8273\n",
      "\n",
      "False Positive Rate:\n",
      "ai: 0.1193\n",
      "human: 0.1404\n",
      "\n",
      "Overall Accuracy: 0.8672\n",
      "Average Inference Time per Sample: 0.270754 seconds\n",
      "========================================\n",
      "\n",
      "\n",
      "\n",
      "=== Trial 8/10: LR=0.0005, WD=1e-06, Epochs=5 ===\n",
      "Epoch [1/5], train loss: 0.4293, train acc: 0.7969, val loss: 0.3819, val acc: 0.8187\n",
      "Epoch [2/5], train loss: 0.3175, train acc: 0.8581, val loss: 0.3398, val acc: 0.8549\n",
      "Epoch [3/5], train loss: 0.2558, train acc: 0.8898, val loss: 0.3394, val acc: 0.8497\n",
      "Epoch [4/5], train loss: 0.1903, train acc: 0.9219, val loss: 0.3686, val acc: 0.8567\n",
      "Epoch [5/5], train loss: 0.1403, train acc: 0.9445, val loss: 0.3773, val acc: 0.8534\n",
      "===== Evaluation at 2025-05-14 19:34:12 =====\n",
      "\n",
      "Hyperparameters:\n",
      "batch_size: 32\n",
      "learning_rate: 0.0005\n",
      "epochs: 5\n",
      "optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0005\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 1e-06\n",
      ")\n",
      "model: ResNet\n",
      "\n",
      "Accuracy per class:\n",
      "ai: 0.8544\n",
      "human: 0.8515\n",
      "\n",
      "Precision, Recall, F1:\n",
      "ai → Precision: 0.9106, Recall: 0.8544, F1: 0.8816\n",
      "human → Precision: 0.7677, Recall: 0.8515, F1: 0.8074\n",
      "\n",
      "False Positive Rate:\n",
      "ai: 0.1485\n",
      "human: 0.1456\n",
      "\n",
      "Overall Accuracy: 0.8534\n",
      "Average Inference Time per Sample: 0.404492 seconds\n",
      "========================================\n",
      "\n",
      "\n",
      "\n",
      "=== Trial 9/10: LR=0.0005, WD=1e-07, Epochs=5 ===\n",
      "Epoch [1/5], train loss: 0.4247, train acc: 0.7939, val loss: 0.3639, val acc: 0.8407\n",
      "Epoch [2/5], train loss: 0.3192, train acc: 0.8554, val loss: 0.3302, val acc: 0.8543\n",
      "Epoch [3/5], train loss: 0.2506, train acc: 0.8920, val loss: 0.4098, val acc: 0.8130\n",
      "Epoch [4/5], train loss: 0.1881, train acc: 0.9234, val loss: 0.3546, val acc: 0.8519\n",
      "Epoch [5/5], train loss: 0.1276, train acc: 0.9501, val loss: 0.4044, val acc: 0.8555\n",
      "===== Evaluation at 2025-05-15 01:24:17 =====\n",
      "\n",
      "Hyperparameters:\n",
      "batch_size: 32\n",
      "learning_rate: 0.0005\n",
      "epochs: 5\n",
      "optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0005\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 1e-07\n",
      ")\n",
      "model: ResNet\n",
      "\n",
      "Accuracy per class:\n",
      "ai: 0.9076\n",
      "human: 0.7631\n",
      "\n",
      "Precision, Recall, F1:\n",
      "ai → Precision: 0.8715, Recall: 0.9076, F1: 0.8892\n",
      "human → Precision: 0.8236, Recall: 0.7631, F1: 0.7922\n",
      "\n",
      "False Positive Rate:\n",
      "ai: 0.2369\n",
      "human: 0.0924\n",
      "\n",
      "Overall Accuracy: 0.8555\n",
      "Average Inference Time per Sample: 0.121608 seconds\n",
      "========================================\n",
      "\n",
      "\n",
      "\n",
      "=== Trial 10/10: LR=0.001, WD=1e-06, Epochs=5 ===\n",
      "Epoch [1/5], train loss: 0.4163, train acc: 0.8070, val loss: 0.3502, val acc: 0.8452\n",
      "Epoch [2/5], train loss: 0.3091, train acc: 0.8648, val loss: 0.3146, val acc: 0.8633\n",
      "Epoch [3/5], train loss: 0.2385, train acc: 0.8978, val loss: 0.3306, val acc: 0.8543\n",
      "Epoch [4/5], train loss: 0.1781, train acc: 0.9290, val loss: 0.3895, val acc: 0.8537\n",
      "Epoch [5/5], train loss: 0.1281, train acc: 0.9491, val loss: 0.3676, val acc: 0.8570\n",
      "===== Evaluation at 2025-05-15 04:19:51 =====\n",
      "\n",
      "Hyperparameters:\n",
      "batch_size: 32\n",
      "learning_rate: 0.001\n",
      "epochs: 5\n",
      "optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 1e-06\n",
      ")\n",
      "model: ResNet\n",
      "\n",
      "Accuracy per class:\n",
      "ai: 0.8709\n",
      "human: 0.8324\n",
      "\n",
      "Precision, Recall, F1:\n",
      "ai → Precision: 0.9019, Recall: 0.8709, F1: 0.8861\n",
      "human → Precision: 0.7846, Recall: 0.8324, F1: 0.8078\n",
      "\n",
      "False Positive Rate:\n",
      "ai: 0.1676\n",
      "human: 0.1291\n",
      "\n",
      "Overall Accuracy: 0.8570\n",
      "Average Inference Time per Sample: 0.101230 seconds\n",
      "========================================\n",
      "\n",
      "\n",
      "\n",
      "Best validation accuracy: 0.8672086720867209\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameter search space\n",
    "learning_rates = [1e-3, 1e-4, 5e-4]\n",
    "weight_decays = [1e-6, 1e-7, 1e-8]\n",
    "epochs_list = [5, 10, 20] \n",
    "\n",
    "# Generate hyperparameter combinations and randomly select 10\n",
    "param_combinations = list(product(learning_rates, weight_decays, epochs_list))\n",
    "random.shuffle(param_combinations)\n",
    "hyperparam_trials = param_combinations[:10]\n",
    "\n",
    "# Run randomized search\n",
    "best_model = None\n",
    "best_acc = 0\n",
    "\n",
    "for i, (lr, wd, epochs) in enumerate(hyperparam_trials):\n",
    "    print(f\"\\n=== Trial {i+1}/10: LR={lr}, WD={wd}, Epochs={epochs} ===\")\n",
    "\n",
    "    model = get_model()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Use an optimizer like Adam or SGD (adjust based on your requirement)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd)\n",
    "    \n",
    "    # Train the model (assuming you have a function to handle training)\n",
    "    train(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
    "\n",
    "    hyperparams = {\n",
    "        \"batch_size\": batch_size,\n",
    "        \"learning_rate\": lr,\n",
    "        \"epochs\": epochs,\n",
    "        \"optimizer\": optimizer,\n",
    "        \"model\": model.__class__.__name__,\n",
    "    }\n",
    "    \n",
    "    val_acc = evaluate_model(model, val_loader, device, hyperparams=hyperparams)\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        best_model = model\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "    \n",
    "print(\"\\nBest validation accuracy:\", best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d9ec0365-7592-472b-a320-65ce25b593a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 153/153 [06:07<00:00,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          filename   prob_ai  prob_human  true_label  pred_label\n",
      "0  H20441N_plp.png  0.000301    0.999699           1           1\n",
      "1    H441N_plp.png  0.000463    0.999537           1           1\n",
      "2  H16310N_plp.png  0.000207    0.999793           1           1\n",
      "3   H4960N_plp.png  0.014200    0.985800           1           1\n",
      "4   H9761N_plp.png  0.620863    0.379137           1           0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.load_state_dict(torch.load(\"best_model.pt\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "results = []\n",
    "\n",
    "for batch in tqdm(test_loader):\n",
    "    inputs, labels, filenames = batch  # Unpack the filename from the dataset\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "        for i in range(len(filenames)):\n",
    "            results.append({\n",
    "                \"filename\": filenames[i],  # Use filename directly from dataset\n",
    "                \"prob_ai\": probs[i][0].item(),\n",
    "                \"prob_human\": probs[i][1].item(),\n",
    "                \"true_label\": labels[i].item(),\n",
    "                \"pred_label\": torch.argmax(probs[i]).item()\n",
    "            })\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"best_plp_ai_aug_test_predictions.csv\", index=False)\n",
    "\n",
    "# Preview results\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0a526208-c0ed-49c4-a5bf-de25c69d3f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 4348/4875 (89.19%)\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV\n",
    "df = pd.read_csv(\"best_plp_ai_aug_test_predictions.csv\")  # Replace with your actual file path\n",
    "\n",
    "# Total number of rows (i.e. total number of samples/files)\n",
    "total = len(df)\n",
    "\n",
    "# Count where prediction is correct\n",
    "correct = (df[\"true_label\"] == df[\"pred_label\"]).sum()\n",
    "\n",
    "print(f\"Correct predictions: {correct}/{total} ({correct/total:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b55ead-7b75-4552-8109-8572032cae75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
