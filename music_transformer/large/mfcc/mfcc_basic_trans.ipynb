{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecfb6aab-283a-4780-a21b-6bc52d095b81",
   "metadata": {},
   "source": [
    "Input: Mel Spectrogram (1, 128, 256)\n",
    "‚Üì\n",
    "Conv Block √ó4: (Conv ‚Üí BN + ReLU ‚Üí MaxPool)\n",
    "‚Üì\n",
    "Flatten + Positional Encoding\n",
    "‚Üì\n",
    "Transformer Encoder Layer √ó2\n",
    "‚Üì\n",
    "Avg Pool ‚Üí BN ‚Üí FC ‚Üí Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "747cbd70-1d18-4cb0-805b-7a0cb9831de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO\n"
     ]
    }
   ],
   "source": [
    "print(\"HELLO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2c20bc2-9fed-4ec7-a9ea-85d434c351e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import csv\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "import time\n",
    "\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7da72ef-9ed2-4429-83d0-414bc54f5198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5a4ae91-0920-4978-9a2c-5f553ec2f6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFCCDataset(Dataset):\n",
    "    def __init__(self, samples, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            samples (List[Tuple[str, int]]): List of (tensor_path, label) pairs.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.samples = samples\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tensor_path, label = self.samples[idx]\n",
    "        mel_tensor = torch.load(tensor_path)  # Expected shape: (1, 128, 256)\n",
    "\n",
    "        if self.transform:\n",
    "            mel_tensor = self.transform(mel_tensor)\n",
    "\n",
    "        filename = os.path.basename(tensor_path)\n",
    "        return mel_tensor, label, filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dde1f672-0ffa-4533-956c-452a65ae00b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(batch_size=32):\n",
    "    # Helper function to read file paths from a text file\n",
    "    def read_file_paths(file_name):\n",
    "        with open(file_name, 'r') as f:\n",
    "            return [line.strip() for line in f.readlines()]\n",
    "        \n",
    "    ai_segments_path = \"/vol/bitbucket/sg2121/fypdataset/dataset_large2/normal_data/ai_segments\"\n",
    "    human_segments_path = \"/vol/bitbucket/sg2121/fypdataset/dataset_large2/normal_data/human\"\n",
    "    ai_tensor_path = \"/vol/bitbucket/sg2121/fypdataset/dataset_large2/tensors/ai_segments/mfcc_tensor\"\n",
    "    human_tensor_path = \"/vol/bitbucket/sg2121/fypdataset/dataset_large2/tensors/human/mfcc_tensor\"\n",
    "\n",
    "    ai_aug_segments_path = \"/vol/bitbucket/sg2121/fypdataset/dataset_large2/normal_data/augmented_ai\"\n",
    "    ai_aug_tensor_path = \"/vol/bitbucket/sg2121/fypdataset/dataset_large2/tensors/augmented_ai/mfcc_tensor\"\n",
    "\n",
    "    # List to store the results\n",
    "    ai_aug_test_files = []\n",
    "    \n",
    "    # Loop through files in the directory\n",
    "    for filename in os.listdir(ai_aug_tensor_path):\n",
    "        if filename.endswith(\".pt\"):\n",
    "            full_path = os.path.join(ai_aug_tensor_path, filename)\n",
    "            ai_aug_test_files.append((full_path, 1))\n",
    "\n",
    "    train_files = read_file_paths('/vol/bitbucket/sg2121/fyp/aimusicdetector/train_test_split/bitbucket/augmented/train_files_w_aug.txt')\n",
    "    val_files = read_file_paths('/vol/bitbucket/sg2121/fyp/aimusicdetector/train_test_split/bitbucket/augmented/val_files_w_aug.txt')\n",
    "    test_files = read_file_paths('/vol/bitbucket/sg2121/fyp/aimusicdetector/train_test_split/bitbucket/augmented/test_files_w_aug.txt')\n",
    "\n",
    "    # Function to convert segment file path to lyric file path\n",
    "    def convert_to_tensor_path(file_path, is_ai):\n",
    "        if is_ai:\n",
    "            if file_path.startswith(ai_segments_path):\n",
    "                base_tensor_path = ai_tensor_path\n",
    "            elif file_path.startswith(ai_aug_segments_path):\n",
    "                base_tensor_path = ai_aug_tensor_path\n",
    "            else:\n",
    "                return\n",
    "        else:\n",
    "            if file_path.startswith(human_segments_path):\n",
    "                base_tensor_path = human_tensor_path\n",
    "            else:\n",
    "                return\n",
    "\n",
    "        # Convert filename to mfcc filename\n",
    "        file_name = os.path.basename(file_path).replace('.mp3', '.pt')\n",
    "        return os.path.join(base_tensor_path, file_name)\n",
    "\n",
    "    # Process the file lists and create tuples of (lyric_path, label)\n",
    "    def process_file_paths(file_paths, is_ai):\n",
    "        return [(convert_to_tensor_path(file_path, is_ai), 0 if is_ai else 1) for file_path in file_paths]\n",
    "    \n",
    "    # Convert all file paths from the train, validation, and test sets\n",
    "    ai_train_files = process_file_paths(train_files, is_ai=True)\n",
    "    human_train_files = process_file_paths(train_files, is_ai=False)\n",
    "    \n",
    "    ai_val_files = process_file_paths(val_files, is_ai=True)\n",
    "    human_val_files = process_file_paths(val_files, is_ai=False)\n",
    "    \n",
    "    ai_test_files = process_file_paths(test_files, is_ai=True)\n",
    "    human_test_files = process_file_paths(test_files, is_ai=False)\n",
    "    \n",
    "    def clean(paths):\n",
    "        return [(p, l) for p, l in paths if p is not None]\n",
    "    \n",
    "    train_files_combined = clean(ai_train_files) + clean(human_train_files)\n",
    "    val_files_combined = clean(ai_val_files) + clean(human_val_files)\n",
    "    test_files_combined = clean(ai_test_files) + clean(human_test_files)\n",
    "    \n",
    "    # Shuffle the data if needed\n",
    "    random.shuffle(train_files_combined)\n",
    "    random.shuffle(val_files_combined)\n",
    "    random.shuffle(test_files_combined)\n",
    "    random.shuffle(ai_aug_test_files)\n",
    "\n",
    "    # Example of how you might check the splits\n",
    "    print(f\"Training set size: {len(train_files_combined)}\")\n",
    "    print(f\"Validation set size: {len(val_files_combined)}\")\n",
    "    print(f\"Test set size: {len(test_files_combined)}\")\n",
    "    print(f\"AI Aug Test set size: {len(ai_aug_test_files)}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = MFCCDataset(train_files_combined)\n",
    "    val_dataset = MFCCDataset(val_files_combined)\n",
    "    test_dataset = MFCCDataset(test_files_combined)\n",
    "    ai_aug_test_dataset = MFCCDataset(ai_aug_test_files)\n",
    "    \n",
    "    return {\n",
    "        \"train\": DataLoader(train_dataset, batch_size=batch_size, shuffle=True),\n",
    "        \"val\": DataLoader(val_dataset, batch_size=batch_size),\n",
    "        \"test\": DataLoader(test_dataset, batch_size=batch_size),\n",
    "        \"ai_aug_test\": DataLoader(ai_aug_test_dataset, batch_size=batch_size)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b6fdcc9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class ConvTransformerClassifier(nn.Module):\n",
    "    def __init__(self, n_classes, d_model=128, nhead=4, num_layers=2, input_shape=(1, 128, 256)):\n",
    "        super(ConvTransformerClassifier, self).__init__()\n",
    "\n",
    "        # --- CNN Feature Extractor ---\n",
    "        conv_layers = []\n",
    "        in_channels = input_shape[0]\n",
    "        for _ in range(4):\n",
    "            conv_layers += [\n",
    "                nn.Conv2d(in_channels, d_model, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(d_model),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2)\n",
    "            ]\n",
    "            in_channels = d_model\n",
    "\n",
    "        self.cnn = nn.Sequential(*conv_layers)\n",
    "\n",
    "        # --- Flatten + Positional Encoding ---\n",
    "        self.flatten = nn.Flatten(2)  # flatten spatial dims into a sequence\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "\n",
    "        # --- Transformer Encoder ---\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # --- Classification Head ---\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.bn = nn.BatchNorm1d(d_model)\n",
    "        self.fc = nn.Linear(d_model, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 1, 128, 256)\n",
    "        x = self.cnn(x)  # (B, C, H', W') => e.g. (B, 128, 8, 16)\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        x = self.flatten(x)            # (B, C, H*W)  ‚Üí sequence\n",
    "        x = x.permute(0, 2, 1)         # (B, seq_len, C)\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        x = self.transformer(x)        # (B, seq_len, C)\n",
    "        x = x.permute(0, 2, 1)         # (B, C, seq_len)\n",
    "        x = self.avgpool(x).squeeze(-1)  # (B, C)\n",
    "\n",
    "        x = self.bn(x)\n",
    "        x = self.fc(x)                 # (B, n_classes)\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=1024):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)  # (max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)  # (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, seq_len, d_model)\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d18332d1-5109-4700-87d3-33e7e7d5edba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, device=\"cuda\", epochs=10, lr=1e-3, weight_decay=0.0, clip_grad=True, patience=5):\n",
    "    set_seed()\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "\n",
    "    best_model = None\n",
    "    best_val_acc = 0.0\n",
    "    best_epoch = 0\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        model.train()\n",
    "        train_loss, train_correct = 0.0, 0\n",
    "\n",
    "        for X, y, filenames in dataloaders[\"train\"]:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            if clip_grad:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * X.size(0)\n",
    "            train_correct += (outputs.argmax(1) == y).sum().item()\n",
    "\n",
    "        train_acc = train_correct / len(dataloaders[\"train\"].dataset)\n",
    "        train_loss /= len(dataloaders[\"train\"].dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_correct = 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for X, y, filenames in dataloaders[\"val\"]:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                outputs = model(X)\n",
    "                loss = criterion(outputs, y)\n",
    "                val_loss += loss.item() * X.size(0)\n",
    "                val_correct += (outputs.argmax(1) == y).sum().item()\n",
    "\n",
    "        val_acc = val_correct / len(dataloaders[\"val\"].dataset)\n",
    "        val_loss /= len(dataloaders[\"val\"].dataset)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Accuracy: {train_acc:.4f}\")\n",
    "        print(f\"Val   Loss: {val_loss:.4f} | Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            best_epoch = epoch\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_model)\n",
    "    print(f\"Best validation accuracy: {best_val_acc:.4f} at epoch {best_epoch+1}\")\n",
    "    return model, best_val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fadc06d2-90a6-418f-8ce9-b4030f052bd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 37297\n",
      "Validation set size: 7991\n",
      "Test set size: 7995\n",
      "AI Aug Test set size: 14149\n"
     ]
    }
   ],
   "source": [
    "# Suppose `model` is your CNN+Transformer hybrid\n",
    "model = ConvTransformerClassifier(n_classes=2, d_model=128, nhead=4, num_layers=2)\n",
    "dataloaders = get_dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3cd1203e-29f9-4656-a92b-3096bd2812df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/15\n",
      "Train Loss: 0.2427 | Accuracy: 0.9035\n",
      "Val   Loss: 0.1947 | Accuracy: 0.9215\n",
      "\n",
      "Epoch 2/15\n",
      "Train Loss: 0.1774 | Accuracy: 0.9304\n",
      "Val   Loss: 0.1612 | Accuracy: 0.9377\n",
      "\n",
      "Epoch 3/15\n",
      "Train Loss: 0.1490 | Accuracy: 0.9443\n",
      "Val   Loss: 0.1502 | Accuracy: 0.9477\n",
      "\n",
      "Epoch 4/15\n",
      "Train Loss: 0.1266 | Accuracy: 0.9535\n",
      "Val   Loss: 0.1329 | Accuracy: 0.9484\n",
      "\n",
      "Epoch 5/15\n",
      "Train Loss: 0.1135 | Accuracy: 0.9587\n",
      "Val   Loss: 0.1723 | Accuracy: 0.9429\n",
      "\n",
      "Epoch 6/15\n",
      "Train Loss: 0.0992 | Accuracy: 0.9654\n",
      "Val   Loss: 0.1534 | Accuracy: 0.9419\n",
      "\n",
      "Epoch 7/15\n",
      "Train Loss: 0.0847 | Accuracy: 0.9705\n",
      "Val   Loss: 0.1547 | Accuracy: 0.9514\n",
      "\n",
      "Epoch 8/15\n",
      "Train Loss: 0.0592 | Accuracy: 0.9810\n",
      "Val   Loss: 0.1694 | Accuracy: 0.9585\n",
      "\n",
      "Epoch 9/15\n",
      "Train Loss: 0.0472 | Accuracy: 0.9855\n",
      "Val   Loss: 0.1905 | Accuracy: 0.9567\n",
      "\n",
      "Epoch 10/15\n",
      "Train Loss: 0.0423 | Accuracy: 0.9875\n",
      "Val   Loss: 0.2059 | Accuracy: 0.9592\n",
      "\n",
      "Epoch 11/15\n",
      "Train Loss: 0.0255 | Accuracy: 0.9931\n",
      "Val   Loss: 0.2422 | Accuracy: 0.9558\n",
      "\n",
      "Epoch 12/15\n",
      "Train Loss: 0.0218 | Accuracy: 0.9944\n",
      "Val   Loss: 0.2423 | Accuracy: 0.9557\n",
      "\n",
      "Epoch 13/15\n",
      "Train Loss: 0.0176 | Accuracy: 0.9955\n",
      "Val   Loss: 0.2565 | Accuracy: 0.9585\n",
      "\n",
      "Epoch 14/15\n",
      "Train Loss: 0.0096 | Accuracy: 0.9974\n",
      "Val   Loss: 0.2835 | Accuracy: 0.9600\n",
      "\n",
      "Epoch 15/15\n",
      "Train Loss: 0.0097 | Accuracy: 0.9975\n",
      "Val   Loss: 0.2887 | Accuracy: 0.9583\n",
      "Best validation accuracy: 0.9600 at epoch 14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(ConvTransformerClassifier(\n",
       "   (cnn): Sequential(\n",
       "     (0): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): ReLU()\n",
       "     (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "     (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (6): ReLU()\n",
       "     (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "     (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (10): ReLU()\n",
       "     (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "     (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (14): ReLU()\n",
       "     (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "   )\n",
       "   (flatten): Flatten(start_dim=2, end_dim=-1)\n",
       "   (positional_encoding): PositionalEncoding()\n",
       "   (transformer): TransformerEncoder(\n",
       "     (layers): ModuleList(\n",
       "       (0-1): 2 x TransformerEncoderLayer(\n",
       "         (self_attn): MultiheadAttention(\n",
       "           (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "         )\n",
       "         (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "         (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "         (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "         (dropout1): Dropout(p=0.1, inplace=False)\n",
       "         (dropout2): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "   (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (fc): Linear(in_features=128, out_features=2, bias=True)\n",
       " ),\n",
       " 0.9599549493179828)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(model, dataloaders, device=\"cuda\", epochs=15, lr=0.0001, weight_decay=0.0, clip_grad=True, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d915127a-2f96-442b-8cec-66fceac68383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device=\"cuda\", csv_path=\"predictions.csv\"):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    test_loss, test_correct = 0.0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "    total_inference_time = 0.0\n",
    "    csv_rows = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y, filenames in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            start_time = time.time()\n",
    "            outputs = model(X)\n",
    "            end_time = time.time()\n",
    "\n",
    "            inference_time = end_time - start_time\n",
    "            total_inference_time += inference_time\n",
    "\n",
    "            loss = criterion(outputs, y)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            preds = outputs.argmax(1)\n",
    "\n",
    "            test_loss += loss.item() * X.size(0)\n",
    "            test_correct += (preds == y).sum().item()\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "            for i in range(X.size(0)):\n",
    "                prob_ai = probs[i][1].item()  # class 1 = AI\n",
    "                prob_human = probs[i][0].item()  # class 0 = Human\n",
    "                csv_rows.append([\n",
    "                    filenames[i],\n",
    "                    f\"{prob_ai:.6f}\",\n",
    "                    f\"{prob_human:.6f}\",\n",
    "                    y[i].item(),\n",
    "                    preds[i].item()\n",
    "                ])\n",
    "\n",
    "    with open(csv_path, mode='w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"filename\", \"prob_ai\", \"prob_human\", \"true_label\", \"pred_label\"])\n",
    "        writer.writerows(csv_rows)\n",
    "\n",
    "    test_acc = test_correct / len(dataloader.dataset)\n",
    "    test_loss /= len(dataloader.dataset)\n",
    "    avg_inference_time = total_inference_time / len(dataloader.dataset)\n",
    "\n",
    "    # Metrics\n",
    "    precision = precision_score(all_labels, all_preds, average='binary')\n",
    "    recall = recall_score(all_labels, all_preds, average='binary')\n",
    "    f1 = f1_score(all_labels, all_preds, average='binary')\n",
    "\n",
    "    # False Positive Rate (FPR = FP / (FP + TN))\n",
    "    tn, fp, fn, tp = confusion_matrix(all_labels, all_preds).ravel()\n",
    "    fpr = fp / (fp + tn + 1e-10)  # avoid division by zero\n",
    "\n",
    "    print(f\"Test  Loss: {test_loss:.4f} | Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f} | Recall: {recall:.4f} | F1 Score: {f1:.4f}\")\n",
    "    print(f\"False Positive Rate: {fpr:.4f}\")\n",
    "    print(f\"Avg Inference Time per Sample: {avg_inference_time * 1000:.2f} ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dba349e1-aee1-41b3-bab7-94655864a20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test  Loss: 0.2597 | Accuracy: 0.9627\n",
      "Precision: 0.9528 | Recall: 0.9685 | F1 Score: 0.9606\n",
      "False Positive Rate: 0.0424\n",
      "Avg Inference Time per Sample: 0.06 ms\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, dataloaders[\"test\"], device=\"cuda\", csv_path=\"tensors_test_large_with_aug_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9d6066a2-cc47-4930-8b65-bf6832e509fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvTransformerClassifier(\n",
       "  (cnn): Sequential(\n",
       "    (0): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (14): ReLU()\n",
       "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (flatten): Flatten(start_dim=2, end_dim=-1)\n",
       "  (positional_encoding): PositionalEncoding()\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "  (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model weights\n",
    "torch.save(model.state_dict(), 'model_weights.pt')\n",
    "\n",
    "# Load model weights (later)\n",
    "model = ConvTransformerClassifier(n_classes=2, d_model=128, nhead=4, num_layers=2)\n",
    "model.load_state_dict(torch.load('model_weights.pt'))\n",
    "model.eval()  # Set to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c0f29bd6-a391-4fb3-8acc-a12b0c495ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 7697/7995 (96.27%)\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV\n",
    "df = pd.read_csv(\"tensors_test_large_with_aug_predictions.csv\")  # Replace with your actual file path\n",
    "\n",
    "# Total number of rows (i.e. total number of samples/files)\n",
    "total = len(df)\n",
    "\n",
    "# Count where prediction is correct\n",
    "correct = (df[\"true_label\"] == df[\"pred_label\"]).sum()\n",
    "\n",
    "print(f\"Correct predictions: {correct}/{total} ({correct/total:.2%})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a8d461-4f14-4c3f-ae65-6ebf1b865e84",
   "metadata": {},
   "source": [
    "HYPERPARAM SEARCH BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "765a5b0a-ff3e-47fc-ab9c-70a4f3b046a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your search space\n",
    "param_grid = {\n",
    "    \"lr\": [1e-4, 5e-4, 1e-3],\n",
    "    \"weight_decay\": [0.0, 1e-5],\n",
    "    \"clip_grad\": [True, False],\n",
    "    \"patience\": [3, 5],\n",
    "    \"epochs\": [10, 15, 20]  # Can be tuned or fixed\n",
    "}\n",
    "\n",
    "search_space = list(product(*param_grid.values()))\n",
    "random.shuffle(search_space)\n",
    "\n",
    "# Tracking best result\n",
    "best_val_acc = float(\"-inf\")\n",
    "best_params = None\n",
    "best_model_path = \"best_model.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8553bdd6-a4d1-45d7-83c0-a80244d7124d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Trial 1/10 with params: {'lr': 0.001, 'weight_decay': 0.0, 'clip_grad': True, 'patience': 3, 'epochs': 15}\n",
      "\n",
      "Epoch 1/15\n",
      "Train Loss: 0.3274 | Accuracy: 0.8728\n",
      "Val   Loss: 0.2277 | Accuracy: 0.9132\n",
      "\n",
      "Epoch 2/15\n",
      "Train Loss: 0.2365 | Accuracy: 0.9104\n",
      "Val   Loss: 0.2846 | Accuracy: 0.8739\n",
      "\n",
      "Epoch 3/15\n",
      "Train Loss: 0.1951 | Accuracy: 0.9269\n",
      "Val   Loss: 0.2854 | Accuracy: 0.8912\n",
      "\n",
      "Epoch 4/15\n",
      "Train Loss: 0.1702 | Accuracy: 0.9375\n",
      "Val   Loss: 0.1492 | Accuracy: 0.9437\n",
      "\n",
      "Epoch 5/15\n",
      "Train Loss: 0.1469 | Accuracy: 0.9460\n",
      "Val   Loss: 0.1792 | Accuracy: 0.9329\n",
      "\n",
      "Epoch 6/15\n",
      "Train Loss: 0.1279 | Accuracy: 0.9535\n",
      "Val   Loss: 0.3174 | Accuracy: 0.9010\n",
      "\n",
      "Epoch 7/15\n",
      "Train Loss: 0.1096 | Accuracy: 0.9611\n",
      "Val   Loss: 0.1387 | Accuracy: 0.9509\n",
      "\n",
      "Epoch 8/15\n",
      "Train Loss: 0.0960 | Accuracy: 0.9656\n",
      "Val   Loss: 0.4905 | Accuracy: 0.8056\n",
      "\n",
      "Epoch 9/15\n",
      "Train Loss: 0.0817 | Accuracy: 0.9704\n",
      "Val   Loss: 0.2571 | Accuracy: 0.9242\n",
      "\n",
      "Epoch 10/15\n",
      "Train Loss: 0.0676 | Accuracy: 0.9754\n",
      "Val   Loss: 0.2234 | Accuracy: 0.9407\n",
      "Early stopping triggered.\n",
      "Best validation accuracy: 0.9509 at epoch 7\n",
      "Validation Accuracy: 0.9509\n",
      "‚úÖ New best model saved!\n",
      "\n",
      "üîç Trial 2/10 with params: {'lr': 0.0001, 'weight_decay': 1e-05, 'clip_grad': False, 'patience': 3, 'epochs': 10}\n",
      "\n",
      "Epoch 1/10\n",
      "Train Loss: 0.3318 | Accuracy: 0.8826\n",
      "Val   Loss: 0.2086 | Accuracy: 0.9265\n",
      "\n",
      "Epoch 2/10\n",
      "Train Loss: 0.2246 | Accuracy: 0.9180\n",
      "Val   Loss: 0.2168 | Accuracy: 0.9117\n",
      "\n",
      "Epoch 3/10\n",
      "Train Loss: 0.1772 | Accuracy: 0.9370\n",
      "Val   Loss: 0.1599 | Accuracy: 0.9433\n",
      "\n",
      "Epoch 4/10\n",
      "Train Loss: 0.1558 | Accuracy: 0.9463\n",
      "Val   Loss: 0.1717 | Accuracy: 0.9425\n",
      "\n",
      "Epoch 5/10\n",
      "Train Loss: 0.1281 | Accuracy: 0.9542\n",
      "Val   Loss: 0.1431 | Accuracy: 0.9487\n",
      "\n",
      "Epoch 6/10\n",
      "Train Loss: 0.1070 | Accuracy: 0.9629\n",
      "Val   Loss: 0.2212 | Accuracy: 0.9308\n",
      "\n",
      "Epoch 7/10\n",
      "Train Loss: 0.0904 | Accuracy: 0.9680\n",
      "Val   Loss: 0.1429 | Accuracy: 0.9481\n",
      "\n",
      "Epoch 8/10\n",
      "Train Loss: 0.0807 | Accuracy: 0.9724\n",
      "Val   Loss: 0.1211 | Accuracy: 0.9612\n",
      "\n",
      "Epoch 9/10\n",
      "Train Loss: 0.0695 | Accuracy: 0.9758\n",
      "Val   Loss: 0.1732 | Accuracy: 0.9462\n",
      "\n",
      "Epoch 10/10\n",
      "Train Loss: 0.0555 | Accuracy: 0.9799\n",
      "Val   Loss: 0.1640 | Accuracy: 0.9487\n",
      "Best validation accuracy: 0.9612 at epoch 8\n",
      "Validation Accuracy: 0.9612\n",
      "‚úÖ New best model saved!\n",
      "\n",
      "üîç Trial 3/10 with params: {'lr': 0.0005, 'weight_decay': 0.0, 'clip_grad': False, 'patience': 3, 'epochs': 10}\n",
      "\n",
      "Epoch 1/10\n",
      "Train Loss: 0.3168 | Accuracy: 0.8839\n",
      "Val   Loss: 0.2230 | Accuracy: 0.9148\n",
      "\n",
      "Epoch 2/10\n",
      "Train Loss: 0.2172 | Accuracy: 0.9195\n",
      "Val   Loss: 0.2028 | Accuracy: 0.9279\n",
      "\n",
      "Epoch 3/10\n",
      "Train Loss: 0.1759 | Accuracy: 0.9354\n",
      "Val   Loss: 0.1558 | Accuracy: 0.9429\n",
      "\n",
      "Epoch 4/10\n",
      "Train Loss: 0.1536 | Accuracy: 0.9457\n",
      "Val   Loss: 0.3022 | Accuracy: 0.8945\n",
      "\n",
      "Epoch 5/10\n",
      "Train Loss: 0.1270 | Accuracy: 0.9547\n",
      "Val   Loss: 0.1313 | Accuracy: 0.9522\n",
      "\n",
      "Epoch 6/10\n",
      "Train Loss: 0.1106 | Accuracy: 0.9593\n",
      "Val   Loss: 0.1976 | Accuracy: 0.9440\n",
      "\n",
      "Epoch 7/10\n",
      "Train Loss: 0.0945 | Accuracy: 0.9666\n",
      "Val   Loss: 0.1610 | Accuracy: 0.9401\n",
      "\n",
      "Epoch 8/10\n",
      "Train Loss: 0.0793 | Accuracy: 0.9733\n",
      "Val   Loss: 0.1422 | Accuracy: 0.9493\n",
      "Early stopping triggered.\n",
      "Best validation accuracy: 0.9522 at epoch 5\n",
      "Validation Accuracy: 0.9522\n",
      "\n",
      "üîç Trial 4/10 with params: {'lr': 0.0005, 'weight_decay': 1e-05, 'clip_grad': False, 'patience': 5, 'epochs': 15}\n",
      "\n",
      "Epoch 1/15\n",
      "Train Loss: 0.3296 | Accuracy: 0.8787\n",
      "Val   Loss: 0.2516 | Accuracy: 0.9023\n",
      "\n",
      "Epoch 2/15\n",
      "Train Loss: 0.2223 | Accuracy: 0.9162\n",
      "Val   Loss: 0.2537 | Accuracy: 0.9023\n",
      "\n",
      "Epoch 3/15\n",
      "Train Loss: 0.1821 | Accuracy: 0.9326\n",
      "Val   Loss: 0.1964 | Accuracy: 0.9251\n",
      "\n",
      "Epoch 4/15\n",
      "Train Loss: 0.1584 | Accuracy: 0.9442\n",
      "Val   Loss: 0.3275 | Accuracy: 0.8803\n",
      "\n",
      "Epoch 5/15\n",
      "Train Loss: 0.1367 | Accuracy: 0.9505\n",
      "Val   Loss: 0.1579 | Accuracy: 0.9476\n",
      "\n",
      "Epoch 6/15\n",
      "Train Loss: 0.1193 | Accuracy: 0.9560\n",
      "Val   Loss: 0.1820 | Accuracy: 0.9454\n",
      "\n",
      "Epoch 7/15\n",
      "Train Loss: 0.1026 | Accuracy: 0.9631\n",
      "Val   Loss: 0.1728 | Accuracy: 0.9376\n",
      "\n",
      "Epoch 8/15\n",
      "Train Loss: 0.0890 | Accuracy: 0.9680\n",
      "Val   Loss: 0.2022 | Accuracy: 0.9433\n",
      "\n",
      "Epoch 9/15\n",
      "Train Loss: 0.0512 | Accuracy: 0.9820\n",
      "Val   Loss: 0.1682 | Accuracy: 0.9542\n",
      "\n",
      "Epoch 10/15\n",
      "Train Loss: 0.0366 | Accuracy: 0.9870\n",
      "Val   Loss: 0.2614 | Accuracy: 0.9435\n",
      "\n",
      "Epoch 11/15\n",
      "Train Loss: 0.0336 | Accuracy: 0.9893\n",
      "Val   Loss: 0.2553 | Accuracy: 0.9487\n",
      "\n",
      "Epoch 12/15\n",
      "Train Loss: 0.0179 | Accuracy: 0.9949\n",
      "Val   Loss: 0.2565 | Accuracy: 0.9530\n",
      "\n",
      "Epoch 13/15\n",
      "Train Loss: 0.0153 | Accuracy: 0.9957\n",
      "Val   Loss: 0.2050 | Accuracy: 0.9594\n",
      "\n",
      "Epoch 14/15\n",
      "Train Loss: 0.0088 | Accuracy: 0.9973\n",
      "Val   Loss: 0.2319 | Accuracy: 0.9594\n",
      "\n",
      "Epoch 15/15\n",
      "Train Loss: 0.0040 | Accuracy: 0.9989\n",
      "Val   Loss: 0.2345 | Accuracy: 0.9635\n",
      "Best validation accuracy: 0.9635 at epoch 15\n",
      "Validation Accuracy: 0.9635\n",
      "‚úÖ New best model saved!\n",
      "\n",
      "üîç Trial 5/10 with params: {'lr': 0.0001, 'weight_decay': 0.0, 'clip_grad': False, 'patience': 3, 'epochs': 15}\n",
      "\n",
      "Epoch 1/15\n",
      "Train Loss: 0.3494 | Accuracy: 0.8728\n",
      "Val   Loss: 0.2222 | Accuracy: 0.9216\n",
      "\n",
      "Epoch 2/15\n",
      "Train Loss: 0.2369 | Accuracy: 0.9156\n",
      "Val   Loss: 0.2639 | Accuracy: 0.8906\n",
      "\n",
      "Epoch 3/15\n",
      "Train Loss: 0.1968 | Accuracy: 0.9327\n",
      "Val   Loss: 0.1707 | Accuracy: 0.9370\n",
      "\n",
      "Epoch 4/15\n",
      "Train Loss: 0.1651 | Accuracy: 0.9425\n",
      "Val   Loss: 0.1644 | Accuracy: 0.9421\n",
      "\n",
      "Epoch 5/15\n",
      "Train Loss: 0.1442 | Accuracy: 0.9500\n",
      "Val   Loss: 0.1704 | Accuracy: 0.9409\n",
      "\n",
      "Epoch 6/15\n",
      "Train Loss: 0.1244 | Accuracy: 0.9550\n",
      "Val   Loss: 0.2185 | Accuracy: 0.9265\n",
      "\n",
      "Epoch 7/15\n",
      "Train Loss: 0.1145 | Accuracy: 0.9606\n",
      "Val   Loss: 0.2743 | Accuracy: 0.8990\n",
      "Early stopping triggered.\n",
      "Best validation accuracy: 0.9421 at epoch 4\n",
      "Validation Accuracy: 0.9421\n",
      "\n",
      "üîç Trial 6/10 with params: {'lr': 0.0001, 'weight_decay': 1e-05, 'clip_grad': False, 'patience': 3, 'epochs': 20}\n",
      "\n",
      "Epoch 1/20\n",
      "Train Loss: 0.3411 | Accuracy: 0.8765\n",
      "Val   Loss: 1.0412 | Accuracy: 0.7717\n",
      "\n",
      "Epoch 2/20\n",
      "Train Loss: 0.2354 | Accuracy: 0.9141\n",
      "Val   Loss: 0.2576 | Accuracy: 0.8939\n",
      "\n",
      "Epoch 3/20\n",
      "Train Loss: 0.1899 | Accuracy: 0.9317\n",
      "Val   Loss: 0.1754 | Accuracy: 0.9351\n",
      "\n",
      "Epoch 4/20\n",
      "Train Loss: 0.1640 | Accuracy: 0.9429\n",
      "Val   Loss: 0.2227 | Accuracy: 0.9189\n",
      "\n",
      "Epoch 5/20\n",
      "Train Loss: 0.1411 | Accuracy: 0.9504\n",
      "Val   Loss: 0.1416 | Accuracy: 0.9524\n",
      "\n",
      "Epoch 6/20\n",
      "Train Loss: 0.1180 | Accuracy: 0.9587\n",
      "Val   Loss: 0.1481 | Accuracy: 0.9479\n",
      "\n",
      "Epoch 7/20\n",
      "Train Loss: 0.1030 | Accuracy: 0.9642\n",
      "Val   Loss: 0.1424 | Accuracy: 0.9481\n",
      "\n",
      "Epoch 8/20\n",
      "Train Loss: 0.0884 | Accuracy: 0.9681\n",
      "Val   Loss: 0.2618 | Accuracy: 0.9099\n",
      "Early stopping triggered.\n",
      "Best validation accuracy: 0.9524 at epoch 5\n",
      "Validation Accuracy: 0.9524\n",
      "\n",
      "üîç Trial 7/10 with params: {'lr': 0.0001, 'weight_decay': 1e-05, 'clip_grad': True, 'patience': 5, 'epochs': 10}\n",
      "\n",
      "Epoch 1/10\n",
      "Train Loss: 0.3447 | Accuracy: 0.8788\n",
      "Val   Loss: 0.1990 | Accuracy: 0.9249\n",
      "\n",
      "Epoch 2/10\n",
      "Train Loss: 0.2456 | Accuracy: 0.9178\n",
      "Val   Loss: 0.2314 | Accuracy: 0.9148\n",
      "\n",
      "Epoch 3/10\n",
      "Train Loss: 0.1853 | Accuracy: 0.9365\n",
      "Val   Loss: 0.1613 | Accuracy: 0.9487\n",
      "\n",
      "Epoch 4/10\n",
      "Train Loss: 0.1679 | Accuracy: 0.9470\n",
      "Val   Loss: 0.1929 | Accuracy: 0.9378\n",
      "\n",
      "Epoch 5/10\n",
      "Train Loss: 0.1383 | Accuracy: 0.9555\n",
      "Val   Loss: 0.2465 | Accuracy: 0.9347\n",
      "\n",
      "Epoch 6/10\n",
      "Train Loss: 0.1159 | Accuracy: 0.9611\n",
      "Val   Loss: 0.1606 | Accuracy: 0.9513\n",
      "\n",
      "Epoch 7/10\n",
      "Train Loss: 0.1003 | Accuracy: 0.9689\n",
      "Val   Loss: 0.4043 | Accuracy: 0.9095\n",
      "\n",
      "Epoch 8/10\n",
      "Train Loss: 0.0899 | Accuracy: 0.9737\n",
      "Val   Loss: 0.1901 | Accuracy: 0.9522\n",
      "\n",
      "Epoch 9/10\n",
      "Train Loss: 0.0804 | Accuracy: 0.9782\n",
      "Val   Loss: 0.2545 | Accuracy: 0.9540\n",
      "\n",
      "Epoch 10/10\n",
      "Train Loss: 0.0385 | Accuracy: 0.9897\n",
      "Val   Loss: 0.2452 | Accuracy: 0.9557\n",
      "Best validation accuracy: 0.9557 at epoch 10\n",
      "Validation Accuracy: 0.9557\n",
      "\n",
      "üîç Trial 8/10 with params: {'lr': 0.0005, 'weight_decay': 0.0, 'clip_grad': False, 'patience': 5, 'epochs': 10}\n",
      "\n",
      "Epoch 1/10\n",
      "Train Loss: 0.3201 | Accuracy: 0.8822\n",
      "Val   Loss: 0.2288 | Accuracy: 0.9146\n",
      "\n",
      "Epoch 2/10\n",
      "Train Loss: 0.2223 | Accuracy: 0.9167\n",
      "Val   Loss: 0.2014 | Accuracy: 0.9249\n",
      "\n",
      "Epoch 3/10\n",
      "Train Loss: 0.1809 | Accuracy: 0.9346\n",
      "Val   Loss: 0.1582 | Accuracy: 0.9413\n",
      "\n",
      "Epoch 4/10\n",
      "Train Loss: 0.1571 | Accuracy: 0.9448\n",
      "Val   Loss: 0.3880 | Accuracy: 0.8565\n",
      "\n",
      "Epoch 5/10\n",
      "Train Loss: 0.1350 | Accuracy: 0.9506\n",
      "Val   Loss: 0.1568 | Accuracy: 0.9466\n",
      "\n",
      "Epoch 6/10\n",
      "Train Loss: 0.1141 | Accuracy: 0.9587\n",
      "Val   Loss: 0.1809 | Accuracy: 0.9409\n",
      "\n",
      "Epoch 7/10\n",
      "Train Loss: 0.1011 | Accuracy: 0.9643\n",
      "Val   Loss: 0.2124 | Accuracy: 0.9181\n",
      "\n",
      "Epoch 8/10\n",
      "Train Loss: 0.0876 | Accuracy: 0.9684\n",
      "Val   Loss: 0.1289 | Accuracy: 0.9544\n",
      "\n",
      "Epoch 9/10\n",
      "Train Loss: 0.0747 | Accuracy: 0.9726\n",
      "Val   Loss: 0.1753 | Accuracy: 0.9503\n",
      "\n",
      "Epoch 10/10\n",
      "Train Loss: 0.0630 | Accuracy: 0.9773\n",
      "Val   Loss: 0.1593 | Accuracy: 0.9456\n",
      "Best validation accuracy: 0.9544 at epoch 8\n",
      "Validation Accuracy: 0.9544\n",
      "\n",
      "üîç Trial 9/10 with params: {'lr': 0.001, 'weight_decay': 0.0, 'clip_grad': True, 'patience': 5, 'epochs': 15}\n",
      "\n",
      "Epoch 1/15\n",
      "Train Loss: 0.3165 | Accuracy: 0.8770\n",
      "Val   Loss: 0.2129 | Accuracy: 0.9166\n",
      "\n",
      "Epoch 2/15\n",
      "Train Loss: 0.2246 | Accuracy: 0.9163\n",
      "Val   Loss: 0.3178 | Accuracy: 0.8682\n",
      "\n",
      "Epoch 3/15\n",
      "Train Loss: 0.1827 | Accuracy: 0.9330\n",
      "Val   Loss: 0.2272 | Accuracy: 0.9288\n",
      "\n",
      "Epoch 4/15\n",
      "Train Loss: 0.1620 | Accuracy: 0.9402\n",
      "Val   Loss: 0.2502 | Accuracy: 0.9043\n",
      "\n",
      "Epoch 5/15\n",
      "Train Loss: 0.1168 | Accuracy: 0.9576\n",
      "Val   Loss: 0.1517 | Accuracy: 0.9456\n",
      "\n",
      "Epoch 6/15\n",
      "Train Loss: 0.0987 | Accuracy: 0.9640\n",
      "Val   Loss: 0.1538 | Accuracy: 0.9509\n",
      "\n",
      "Epoch 7/15\n",
      "Train Loss: 0.0831 | Accuracy: 0.9701\n",
      "Val   Loss: 0.1649 | Accuracy: 0.9481\n",
      "\n",
      "Epoch 8/15\n",
      "Train Loss: 0.0719 | Accuracy: 0.9752\n",
      "Val   Loss: 0.1622 | Accuracy: 0.9579\n",
      "\n",
      "Epoch 9/15\n",
      "Train Loss: 0.0349 | Accuracy: 0.9891\n",
      "Val   Loss: 0.1975 | Accuracy: 0.9565\n",
      "\n",
      "Epoch 10/15\n",
      "Train Loss: 0.0264 | Accuracy: 0.9927\n",
      "Val   Loss: 0.2330 | Accuracy: 0.9513\n",
      "\n",
      "Epoch 11/15\n",
      "Train Loss: 0.0264 | Accuracy: 0.9932\n",
      "Val   Loss: 0.2537 | Accuracy: 0.9561\n",
      "\n",
      "Epoch 12/15\n",
      "Train Loss: 0.0140 | Accuracy: 0.9967\n",
      "Val   Loss: 0.3163 | Accuracy: 0.9571\n",
      "\n",
      "Epoch 13/15\n",
      "Train Loss: 0.0068 | Accuracy: 0.9984\n",
      "Val   Loss: 0.3458 | Accuracy: 0.9559\n",
      "Early stopping triggered.\n",
      "Best validation accuracy: 0.9579 at epoch 8\n",
      "Validation Accuracy: 0.9579\n",
      "\n",
      "üîç Trial 10/10 with params: {'lr': 0.0001, 'weight_decay': 0.0, 'clip_grad': True, 'patience': 5, 'epochs': 15}\n",
      "\n",
      "Epoch 1/15\n",
      "Train Loss: 0.3583 | Accuracy: 0.8775\n",
      "Val   Loss: 0.4629 | Accuracy: 0.8655\n",
      "\n",
      "Epoch 2/15\n",
      "Train Loss: 0.2391 | Accuracy: 0.9171\n",
      "Val   Loss: 0.1992 | Accuracy: 0.9206\n",
      "\n",
      "Epoch 3/15\n",
      "Train Loss: 0.1824 | Accuracy: 0.9393\n",
      "Val   Loss: 0.2113 | Accuracy: 0.9394\n",
      "\n",
      "Epoch 4/15\n",
      "Train Loss: 0.1629 | Accuracy: 0.9478\n",
      "Val   Loss: 0.1857 | Accuracy: 0.9374\n",
      "\n",
      "Epoch 5/15\n",
      "Train Loss: 0.1391 | Accuracy: 0.9565\n",
      "Val   Loss: 0.1626 | Accuracy: 0.9524\n",
      "\n",
      "Epoch 6/15\n",
      "Train Loss: 0.1143 | Accuracy: 0.9614\n",
      "Val   Loss: 0.2155 | Accuracy: 0.9382\n",
      "\n",
      "Epoch 7/15\n",
      "Train Loss: 0.1070 | Accuracy: 0.9681\n",
      "Val   Loss: 0.1940 | Accuracy: 0.9485\n",
      "\n",
      "Epoch 8/15\n",
      "Train Loss: 0.0943 | Accuracy: 0.9725\n",
      "Val   Loss: 0.1400 | Accuracy: 0.9630\n",
      "\n",
      "Epoch 9/15\n",
      "Train Loss: 0.0777 | Accuracy: 0.9774\n",
      "Val   Loss: 0.2694 | Accuracy: 0.9448\n",
      "\n",
      "Epoch 10/15\n",
      "Train Loss: 0.0647 | Accuracy: 0.9819\n",
      "Val   Loss: 0.2594 | Accuracy: 0.9479\n",
      "\n",
      "Epoch 11/15\n",
      "Train Loss: 0.0685 | Accuracy: 0.9827\n",
      "Val   Loss: 0.1969 | Accuracy: 0.9600\n",
      "\n",
      "Epoch 12/15\n",
      "Train Loss: 0.0349 | Accuracy: 0.9924\n",
      "Val   Loss: 0.2249 | Accuracy: 0.9635\n",
      "\n",
      "Epoch 13/15\n",
      "Train Loss: 0.0219 | Accuracy: 0.9951\n",
      "Val   Loss: 0.3015 | Accuracy: 0.9563\n",
      "\n",
      "Epoch 14/15\n",
      "Train Loss: 0.0168 | Accuracy: 0.9960\n",
      "Val   Loss: 0.3478 | Accuracy: 0.9600\n",
      "\n",
      "Epoch 15/15\n",
      "Train Loss: 0.0069 | Accuracy: 0.9986\n",
      "Val   Loss: 0.3002 | Accuracy: 0.9655\n",
      "Best validation accuracy: 0.9655 at epoch 15\n",
      "Validation Accuracy: 0.9655\n",
      "‚úÖ New best model saved!\n",
      "\n",
      "üèÜ Best Hyperparameters:\n",
      "{'lr': 0.0001, 'weight_decay': 0.0, 'clip_grad': True, 'patience': 5, 'epochs': 15}\n",
      "Best Validation Accuracy: 0.9655\n",
      "Model saved to: best_model.pt\n"
     ]
    }
   ],
   "source": [
    "# Start search\n",
    "max_trials = min(10, len(search_space))\n",
    "for i, values in enumerate(search_space[:max_trials]):\n",
    "    params = dict(zip(param_grid.keys(), values))\n",
    "    print(f\"\\nüîç Trial {i + 1}/{max_trials} with params: {params}\")\n",
    "\n",
    "    model = ConvTransformerClassifier(n_classes=2, d_model=128, nhead=4, num_layers=2) \n",
    "    trained_model, val_acc = train_model(model, dataloaders, device=\"cuda\", **params)\n",
    "\n",
    "    print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_params = params\n",
    "\n",
    "        # Save the best model\n",
    "        torch.save(trained_model.state_dict(), best_model_path)\n",
    "        print(\"‚úÖ New best model saved!\")\n",
    "\n",
    "print(\"\\nüèÜ Best Hyperparameters:\")\n",
    "print(best_params)\n",
    "print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")\n",
    "print(f\"Model saved to: {best_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4a597d5-fbb7-4312-9f26-d3c7af31451d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test  Loss: 0.3241 | Accuracy: 0.9621\n",
      "Precision: 0.9677 | Recall: 0.9835 | F1 Score: 0.9755\n",
      "False Positive Rate: 0.1092\n",
      "Avg Inference Time per Sample: 0.05 ms\n"
     ]
    }
   ],
   "source": [
    "model = ConvTransformerClassifier(n_classes=2, d_model=128, nhead=4, num_layers=2)\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "device=\"cuda\"\n",
    "model.to(device)\n",
    "model.eval()  # Set to evaluation mode\n",
    "evaluate_model(model, dataloaders[\"test\"], device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9bb6e3c5-9c6f-4cd3-a3e3-f73536c20e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 22736\n",
      "Validation set size: 4871\n",
      "Test set size: 4875\n",
      "AI Aug Test set size: 14149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 443/443 [04:12<00:00,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         filename   prob_ai  prob_human  true_label  \\\n",
      "0  S5093RN_segment_1_stretched.pt  0.000019    0.999982           1   \n",
      "1   U844RN_segment_1_stretched.pt  0.000019    0.999981           1   \n",
      "2    S1384RN_segment_2_shifted.pt  0.000019    0.999981           1   \n",
      "3   S334RN_segment_2_stretched.pt  0.000023    0.999977           1   \n",
      "4  S1253RN_segment_2_stretched.pt  0.000019    0.999981           1   \n",
      "\n",
      "   pred_label  \n",
      "0           1  \n",
      "1           1  \n",
      "2           1  \n",
      "3           1  \n",
      "4           1  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataloaders = get_dataloaders()\n",
    "model = ConvTransformerClassifier(n_classes=2, d_model=128, nhead=4, num_layers=2)\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "device=\"cuda\"\n",
    "model.to(device)\n",
    "model.eval()  # Set to evaluation mode\n",
    "results = []\n",
    "\n",
    "for batch in tqdm(dataloaders[\"ai_aug_test\"]):\n",
    "    mel_tensors, labels, filenames = batch  # Unpack the filename from the dataset\n",
    "    mel_tensors = mel_tensors.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(mel_tensors)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "        for i in range(1):\n",
    "            results.append({\n",
    "                \"filename\": filenames[i],  # Use filename directly from dataset\n",
    "                \"prob_ai\": probs[i][0].item(),\n",
    "                \"prob_human\": probs[i][1].item(),\n",
    "                \"true_label\": labels[i].item(),\n",
    "                \"pred_label\": torch.argmax(probs[i]).item()\n",
    "            })\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"best_mfcc_ai_aug_test_predictions.csv\", index=False)\n",
    "\n",
    "# Preview results\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f11a7fd8-9bd4-42f8-ba26-c7cfa10cde15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 443/443 (100.00%)\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV\n",
    "df = pd.read_csv(\"best_mfcc_ai_aug_test_predictions.csv\")  # Replace with your actual file path\n",
    "\n",
    "# Total number of rows (i.e. total number of samples/files)\n",
    "total = len(df)\n",
    "\n",
    "# Count where prediction is correct\n",
    "correct = (df[\"true_label\"] == df[\"pred_label\"]).sum()\n",
    "\n",
    "print(f\"Correct predictions: {correct}/{total} ({correct/total:.2%})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56916d51-78b0-442d-834e-17c45df4544c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
