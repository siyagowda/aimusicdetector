{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecfb6aab-283a-4780-a21b-6bc52d095b81",
   "metadata": {},
   "source": [
    "Input: Mel Spectrogram (1, 128, 256)\n",
    "↓\n",
    "Conv Block ×4: (Conv → BN + ReLU → MaxPool)\n",
    "↓\n",
    "Flatten + Positional Encoding\n",
    "↓\n",
    "Transformer Encoder Layer ×2\n",
    "↓\n",
    "Avg Pool → BN → FC → Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2c20bc2-9fed-4ec7-a9ea-85d434c351e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7da72ef-9ed2-4429-83d0-414bc54f5198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5a4ae91-0920-4978-9a2c-5f553ec2f6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelSpectrogramDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.samples = []\n",
    "        self.transform = transform\n",
    "        self.label_map = {}\n",
    "\n",
    "        for label_idx, label in enumerate(sorted(os.listdir(root_dir))):\n",
    "            label_path = os.path.join(root_dir, label, \"mel_spec_tensor\")\n",
    "            if not os.path.isdir(label_path):\n",
    "                continue\n",
    "\n",
    "            self.label_map[label] = label_idx\n",
    "            for file in os.listdir(label_path):\n",
    "                if file.endswith(\".pt\"):\n",
    "                    self.samples.append({\n",
    "                        \"path\": os.path.join(label_path, file),\n",
    "                        \"label\": label_idx\n",
    "                    })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        mel_tensor = torch.load(sample[\"path\"])  # shape: (1, 128, 256)\n",
    "        label = sample[\"label\"]\n",
    "\n",
    "        if self.transform:\n",
    "            mel_tensor = self.transform(mel_tensor)\n",
    "\n",
    "        return mel_tensor, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dde1f672-0ffa-4533-956c-452a65ae00b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(root_dir, batch_size=32, val_frac=0.1, test_frac=0.1):\n",
    "    dataset = MelSpectrogramDataset(root_dir)\n",
    "    total_size = len(dataset)\n",
    "    test_size = int(total_size * test_frac)\n",
    "    val_size = int(total_size * val_frac)\n",
    "    train_size = total_size - val_size - test_size\n",
    "\n",
    "    train_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size])\n",
    "    \n",
    "    return {\n",
    "        \"train\": DataLoader(train_set, batch_size=batch_size, shuffle=True),\n",
    "        \"val\": DataLoader(val_set, batch_size=batch_size),\n",
    "        \"test\": DataLoader(test_set, batch_size=batch_size),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b6fdcc9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class ConvTransformerClassifier(nn.Module):\n",
    "    def __init__(self, n_classes, d_model=128, nhead=4, num_layers=2, input_shape=(1, 128, 256)):\n",
    "        super(ConvTransformerClassifier, self).__init__()\n",
    "\n",
    "        # --- CNN Feature Extractor ---\n",
    "        conv_layers = []\n",
    "        in_channels = input_shape[0]\n",
    "        for _ in range(4):\n",
    "            conv_layers += [\n",
    "                nn.Conv2d(in_channels, d_model, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(d_model),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2)\n",
    "            ]\n",
    "            in_channels = d_model\n",
    "\n",
    "        self.cnn = nn.Sequential(*conv_layers)\n",
    "\n",
    "        # --- Flatten + Positional Encoding ---\n",
    "        self.flatten = nn.Flatten(2)  # flatten spatial dims into a sequence\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "\n",
    "        # --- Transformer Encoder ---\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # --- Classification Head ---\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.bn = nn.BatchNorm1d(d_model)\n",
    "        self.fc = nn.Linear(d_model, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 1, 128, 256)\n",
    "        x = self.cnn(x)  # (B, C, H', W') => e.g. (B, 128, 8, 16)\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        x = self.flatten(x)            # (B, C, H*W)  → sequence\n",
    "        x = x.permute(0, 2, 1)         # (B, seq_len, C)\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        x = self.transformer(x)        # (B, seq_len, C)\n",
    "        x = x.permute(0, 2, 1)         # (B, C, seq_len)\n",
    "        x = self.avgpool(x).squeeze(-1)  # (B, C)\n",
    "\n",
    "        x = self.bn(x)\n",
    "        x = self.fc(x)                 # (B, n_classes)\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=1024):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)  # (max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)  # (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, seq_len, d_model)\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d18332d1-5109-4700-87d3-33e7e7d5edba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, device=\"cuda\", epochs=10, lr=1e-3, weight_decay=0.0, clip_grad=True, patience=5):\n",
    "    set_seed()\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "\n",
    "    best_model = None\n",
    "    best_val_acc = 0.0\n",
    "    best_epoch = 0\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        model.train()\n",
    "        train_loss, train_correct = 0.0, 0\n",
    "\n",
    "        for X, y in dataloaders[\"train\"]:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            if clip_grad:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * X.size(0)\n",
    "            train_correct += (outputs.argmax(1) == y).sum().item()\n",
    "\n",
    "        train_acc = train_correct / len(dataloaders[\"train\"].dataset)\n",
    "        train_loss /= len(dataloaders[\"train\"].dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_correct = 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in dataloaders[\"val\"]:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                outputs = model(X)\n",
    "                loss = criterion(outputs, y)\n",
    "                val_loss += loss.item() * X.size(0)\n",
    "                val_correct += (outputs.argmax(1) == y).sum().item()\n",
    "\n",
    "        val_acc = val_correct / len(dataloaders[\"val\"].dataset)\n",
    "        val_loss /= len(dataloaders[\"val\"].dataset)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Accuracy: {train_acc:.4f}\")\n",
    "        print(f\"Val   Loss: {val_loss:.4f} | Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            best_epoch = epoch\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_model)\n",
    "    print(f\"Best validation accuracy: {best_val_acc:.4f} at epoch {best_epoch+1}\")\n",
    "    return model, best_val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fadc06d2-90a6-418f-8ce9-b4030f052bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n",
      "Train Loss: 0.2750 | Accuracy: 0.8887\n",
      "Val   Loss: 0.9806 | Accuracy: 0.7051\n",
      "\n",
      "Epoch 2/10\n",
      "Train Loss: 0.1792 | Accuracy: 0.9300\n",
      "Val   Loss: 0.9177 | Accuracy: 0.7450\n",
      "\n",
      "Epoch 3/10\n",
      "Train Loss: 0.1461 | Accuracy: 0.9447\n",
      "Val   Loss: 0.1470 | Accuracy: 0.9420\n",
      "\n",
      "Epoch 4/10\n",
      "Train Loss: 0.1260 | Accuracy: 0.9516\n",
      "Val   Loss: 0.3866 | Accuracy: 0.8684\n",
      "\n",
      "Epoch 5/10\n",
      "Train Loss: 0.1095 | Accuracy: 0.9587\n",
      "Val   Loss: 0.9521 | Accuracy: 0.7613\n",
      "\n",
      "Epoch 6/10\n",
      "Train Loss: 0.0906 | Accuracy: 0.9656\n",
      "Val   Loss: 0.0861 | Accuracy: 0.9686\n",
      "\n",
      "Epoch 7/10\n",
      "Train Loss: 0.0799 | Accuracy: 0.9704\n",
      "Val   Loss: 0.1168 | Accuracy: 0.9591\n",
      "\n",
      "Epoch 8/10\n",
      "Train Loss: 0.0685 | Accuracy: 0.9746\n",
      "Val   Loss: 0.1211 | Accuracy: 0.9665\n",
      "\n",
      "Epoch 9/10\n",
      "Train Loss: 0.0564 | Accuracy: 0.9808\n",
      "Val   Loss: 0.0885 | Accuracy: 0.9732\n",
      "\n",
      "Epoch 10/10\n",
      "Train Loss: 0.0281 | Accuracy: 0.9903\n",
      "Val   Loss: 0.0955 | Accuracy: 0.9747\n",
      "Best validation accuracy: 0.9747 at epoch 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(ConvTransformerClassifier(\n",
       "   (cnn): Sequential(\n",
       "     (0): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): ReLU()\n",
       "     (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "     (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (6): ReLU()\n",
       "     (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "     (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (10): ReLU()\n",
       "     (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "     (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (14): ReLU()\n",
       "     (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "   )\n",
       "   (flatten): Flatten(start_dim=2, end_dim=-1)\n",
       "   (positional_encoding): PositionalEncoding()\n",
       "   (transformer): TransformerEncoder(\n",
       "     (layers): ModuleList(\n",
       "       (0-1): 2 x TransformerEncoderLayer(\n",
       "         (self_attn): MultiheadAttention(\n",
       "           (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "         )\n",
       "         (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "         (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "         (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "         (dropout1): Dropout(p=0.1, inplace=False)\n",
       "         (dropout2): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "   (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (fc): Linear(in_features=128, out_features=2, bias=True)\n",
       " ),\n",
       " 0.9746997188857653)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Suppose `model` is your CNN+Transformer hybrid\n",
    "model = ConvTransformerClassifier(n_classes=2, d_model=128, nhead=4, num_layers=2)\n",
    "dataloaders = get_dataloaders(\"/vol/bitbucket/sg2121/fypdataset/dataset_large2/tensors\", batch_size=32)\n",
    "train_model(model, dataloaders, device=\"cuda\", epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d915127a-2f96-442b-8cec-66fceac68383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    test_loss, test_correct = 0.0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "    total_inference_time = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            start_time = time.time()\n",
    "            outputs = model(X)\n",
    "            end_time = time.time()\n",
    "\n",
    "            inference_time = end_time - start_time\n",
    "            total_inference_time += inference_time\n",
    "\n",
    "            loss = criterion(outputs, y)\n",
    "            preds = outputs.argmax(1)\n",
    "\n",
    "            test_loss += loss.item() * X.size(0)\n",
    "            test_correct += (preds == y).sum().item()\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    test_acc = test_correct / len(dataloader.dataset)\n",
    "    test_loss /= len(dataloader.dataset)\n",
    "    avg_inference_time = total_inference_time / len(dataloader.dataset)\n",
    "\n",
    "    # Metrics\n",
    "    precision = precision_score(all_labels, all_preds, average='binary')\n",
    "    recall = recall_score(all_labels, all_preds, average='binary')\n",
    "    f1 = f1_score(all_labels, all_preds, average='binary')\n",
    "\n",
    "    # False Positive Rate (FPR = FP / (FP + TN))\n",
    "    tn, fp, fn, tp = confusion_matrix(all_labels, all_preds).ravel()\n",
    "    fpr = fp / (fp + tn + 1e-10)  # avoid division by zero\n",
    "\n",
    "    print(f\"Test  Loss: {test_loss:.4f} | Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f} | Recall: {recall:.4f} | F1 Score: {f1:.4f}\")\n",
    "    print(f\"False Positive Rate: {fpr:.4f}\")\n",
    "    print(f\"Avg Inference Time per Sample: {avg_inference_time * 1000:.2f} ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dba349e1-aee1-41b3-bab7-94655864a20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test  Loss: 0.1122 | Accuracy: 0.9696\n",
      "Precision: 0.9811 | Recall: 0.9713 | F1 Score: 0.9762\n",
      "False Positive Rate: 0.0335\n",
      "Avg Inference Time per Sample: 0.11 ms\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, dataloaders[\"test\"], device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6066a2-cc47-4930-8b65-bf6832e509fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model weights\n",
    "torch.save(model.state_dict(), 'model_weights.pth')\n",
    "\n",
    "# Load model weights (later)\n",
    "model = ConvTransformerClassifier(n_classes=2, d_model=128, nhead=4, num_layers=2)\n",
    "model.load_state_dict(torch.load('model_weights.pth'))\n",
    "model.eval()  # Set to evaluation mode\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
