{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecfb6aab-283a-4780-a21b-6bc52d095b81",
   "metadata": {},
   "source": [
    "Input: Mel Spectrogram (1, 128, 256)\n",
    "â†“\n",
    "Conv Block Ã—4: (Conv â†’ BN + ReLU â†’ MaxPool)\n",
    "â†“\n",
    "Flatten + Positional Encoding\n",
    "â†“\n",
    "Transformer Encoder Layer Ã—2\n",
    "â†“\n",
    "Avg Pool â†’ BN â†’ FC â†’ Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "747cbd70-1d18-4cb0-805b-7a0cb9831de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO\n"
     ]
    }
   ],
   "source": [
    "print(\"HELLO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d2c20bc2-9fed-4ec7-a9ea-85d434c351e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import csv\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "import time\n",
    "\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7da72ef-9ed2-4429-83d0-414bc54f5198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5a4ae91-0920-4978-9a2c-5f553ec2f6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelSpectrogramDataset(Dataset):\n",
    "    def __init__(self, samples, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            samples (List[Tuple[str, int]]): List of (tensor_path, label) pairs.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.samples = samples\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tensor_path, label = self.samples[idx]\n",
    "        mel_tensor = torch.load(tensor_path)  # Expected shape: (1, 128, 256)\n",
    "\n",
    "        if self.transform:\n",
    "            mel_tensor = self.transform(mel_tensor)\n",
    "\n",
    "        filename = os.path.basename(tensor_path)\n",
    "        return mel_tensor, label, filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dde1f672-0ffa-4533-956c-452a65ae00b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(batch_size=32):\n",
    "    # Helper function to read file paths from a text file\n",
    "    def read_file_paths(file_name):\n",
    "        with open(file_name, 'r') as f:\n",
    "            return [line.strip() for line in f.readlines()]\n",
    "        \n",
    "    ai_segments_path = \"/vol/bitbucket/sg2121/fypdataset/dataset_large2/normal_data/ai_segments\"\n",
    "    human_segments_path = \"/vol/bitbucket/sg2121/fypdataset/dataset_large2/normal_data/human\"\n",
    "    ai_tensor_path = \"/vol/bitbucket/sg2121/fypdataset/dataset_large2/tensors/ai_segments/mel_spec_tensor\"\n",
    "    human_tensor_path = \"/vol/bitbucket/sg2121/fypdataset/dataset_large2/tensors/human/mel_spec_tensor\"\n",
    "\n",
    "    ai_aug_segments_path = \"/vol/bitbucket/sg2121/fypdataset/dataset_large2/normal_data/augmented_ai\"\n",
    "    ai_aug_tensor_path = \"/vol/bitbucket/sg2121/fypdataset/dataset_large2/tensors/augmented_ai/mel_spec_tensor\"\n",
    "\n",
    "    # List to store the results\n",
    "    ai_aug_test_files = []\n",
    "    \n",
    "    # Loop through files in the directory\n",
    "    for filename in os.listdir(ai_aug_tensor_path):\n",
    "        if filename.endswith(\".pt\"):\n",
    "            full_path = os.path.join(ai_aug_tensor_path, filename)\n",
    "            ai_aug_test_files.append((full_path, 1))\n",
    "\n",
    "    train_files = read_file_paths('/vol/bitbucket/sg2121/fyp/aimusicdetector/train_test_split/bitbucket/augmented/train_files_w_aug.txt')\n",
    "    val_files = read_file_paths('/vol/bitbucket/sg2121/fyp/aimusicdetector/train_test_split/bitbucket/augmented/val_files_w_aug.txt')\n",
    "    test_files = read_file_paths('/vol/bitbucket/sg2121/fyp/aimusicdetector/train_test_split/bitbucket/augmented/test_files_w_aug.txt')\n",
    "\n",
    "    train_files = read_file_paths('/vol/bitbucket/sg2121/fyp/aimusicdetector/train_test_split/bitbucket/train_files_large.txt')\n",
    "    val_files = read_file_paths('/vol/bitbucket/sg2121/fyp/aimusicdetector/train_test_split/bitbucket/val_files_large.txt')\n",
    "    test_files = read_file_paths('/vol/bitbucket/sg2121/fyp/aimusicdetector/train_test_split/bitbucket/test_files_large.txt')\n",
    "\n",
    "    # Function to convert segment file path to lyric file path\n",
    "    def convert_to_tensor_path(file_path, is_ai):\n",
    "        if is_ai:\n",
    "            if file_path.startswith(ai_segments_path):\n",
    "                base_tensor_path = ai_tensor_path\n",
    "            elif file_path.startswith(ai_aug_segments_path):\n",
    "                base_tensor_path = ai_aug_tensor_path\n",
    "            else:\n",
    "                return\n",
    "        else:\n",
    "            if file_path.startswith(human_segments_path):\n",
    "                base_tensor_path = human_tensor_path\n",
    "            else:\n",
    "                return\n",
    "\n",
    "        # Convert filename to tensor filename\n",
    "        file_name = os.path.basename(file_path).replace('.mp3', '.pt')\n",
    "        return os.path.join(base_tensor_path, file_name)\n",
    "\n",
    "    # Process the file lists and create tuples of (lyric_path, label)\n",
    "    def process_file_paths(file_paths, is_ai):\n",
    "        return [(convert_to_tensor_path(file_path, is_ai), 0 if is_ai else 1) for file_path in file_paths]\n",
    "    \n",
    "    # Convert all file paths from the train, validation, and test sets\n",
    "    ai_train_files = process_file_paths(train_files, is_ai=True)\n",
    "    human_train_files = process_file_paths(train_files, is_ai=False)\n",
    "    \n",
    "    ai_val_files = process_file_paths(val_files, is_ai=True)\n",
    "    human_val_files = process_file_paths(val_files, is_ai=False)\n",
    "    \n",
    "    ai_test_files = process_file_paths(test_files, is_ai=True)\n",
    "    human_test_files = process_file_paths(test_files, is_ai=False)\n",
    "    \n",
    "    def clean(paths):\n",
    "        return [(p, l) for p, l in paths if p is not None]\n",
    "    \n",
    "    train_files_combined = clean(ai_train_files) + clean(human_train_files)\n",
    "    val_files_combined = clean(ai_val_files) + clean(human_val_files)\n",
    "    test_files_combined = clean(ai_test_files) + clean(human_test_files)\n",
    "    \n",
    "    # Shuffle the data\n",
    "    random.shuffle(train_files_combined)\n",
    "    random.shuffle(val_files_combined)\n",
    "    random.shuffle(test_files_combined)\n",
    "    random.shuffle(ai_aug_test_files)\n",
    "\n",
    "    # Check the splits\n",
    "    print(f\"Training set size: {len(train_files_combined)}\")\n",
    "    print(f\"Validation set size: {len(val_files_combined)}\")\n",
    "    print(f\"Test set size: {len(test_files_combined)}\")\n",
    "    print(f\"AI Aug Test set size: {len(ai_aug_test_files)}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = MelSpectrogramDataset(train_files_combined)\n",
    "    val_dataset = MelSpectrogramDataset(val_files_combined)\n",
    "    test_dataset = MelSpectrogramDataset(test_files_combined)\n",
    "    ai_aug_test_dataset = MelSpectrogramDataset(ai_aug_test_files)\n",
    "    \n",
    "    return {\n",
    "        \"train\": DataLoader(train_dataset, batch_size=batch_size, shuffle=True),\n",
    "        \"val\": DataLoader(val_dataset, batch_size=batch_size),\n",
    "        \"test\": DataLoader(test_dataset, batch_size=batch_size),\n",
    "        \"ai_aug_test\": DataLoader(ai_aug_test_dataset, batch_size=batch_size)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b6fdcc9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class ConvTransformerClassifier(nn.Module):\n",
    "    def __init__(self, n_classes, d_model=128, nhead=4, num_layers=2, input_shape=(1, 128, 256)):\n",
    "        super(ConvTransformerClassifier, self).__init__()\n",
    "\n",
    "        # --- CNN Feature Extractor ---\n",
    "        conv_layers = []\n",
    "        in_channels = input_shape[0]\n",
    "        for _ in range(4):\n",
    "            conv_layers += [\n",
    "                nn.Conv2d(in_channels, d_model, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(d_model),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2)\n",
    "            ]\n",
    "            in_channels = d_model\n",
    "\n",
    "        self.cnn = nn.Sequential(*conv_layers)\n",
    "\n",
    "        # --- Flatten + Positional Encoding ---\n",
    "        self.flatten = nn.Flatten(2)  # flatten spatial dims into a sequence\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "\n",
    "        # --- Transformer Encoder ---\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # --- Classification Head ---\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.bn = nn.BatchNorm1d(d_model)\n",
    "        self.fc = nn.Linear(d_model, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 1, 128, 256)\n",
    "        x = self.cnn(x)  # (B, C, H', W') => (B, 128, 8, 16)\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        x = self.flatten(x)            # (B, C, H*W)  â†’ sequence\n",
    "        x = x.permute(0, 2, 1)         # (B, seq_len, C)\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        x = self.transformer(x)        # (B, seq_len, C)\n",
    "        x = x.permute(0, 2, 1)         # (B, C, seq_len)\n",
    "        x = self.avgpool(x).squeeze(-1)  # (B, C)\n",
    "\n",
    "        x = self.bn(x)\n",
    "        x = self.fc(x)                 # (B, n_classes)\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=1024):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)  # (max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)  # (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, seq_len, d_model)\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d18332d1-5109-4700-87d3-33e7e7d5edba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, device=\"cuda\", epochs=10, lr=1e-3, weight_decay=0.0, clip_grad=True, patience=5):\n",
    "    set_seed()\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "\n",
    "    best_model = None\n",
    "    best_val_acc = 0.0\n",
    "    best_epoch = 0\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        model.train()\n",
    "        train_loss, train_correct = 0.0, 0\n",
    "\n",
    "        for X, y, filenames in dataloaders[\"train\"]:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            if clip_grad:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * X.size(0)\n",
    "            train_correct += (outputs.argmax(1) == y).sum().item()\n",
    "\n",
    "        train_acc = train_correct / len(dataloaders[\"train\"].dataset)\n",
    "        train_loss /= len(dataloaders[\"train\"].dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_correct = 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for X, y, filenames in dataloaders[\"val\"]:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                outputs = model(X)\n",
    "                loss = criterion(outputs, y)\n",
    "                val_loss += loss.item() * X.size(0)\n",
    "                val_correct += (outputs.argmax(1) == y).sum().item()\n",
    "\n",
    "        val_acc = val_correct / len(dataloaders[\"val\"].dataset)\n",
    "        val_loss /= len(dataloaders[\"val\"].dataset)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Accuracy: {train_acc:.4f}\")\n",
    "        print(f\"Val   Loss: {val_loss:.4f} | Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            best_epoch = epoch\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_model)\n",
    "    print(f\"Best validation accuracy: {best_val_acc:.4f} at epoch {best_epoch+1}\")\n",
    "    return model, best_val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fadc06d2-90a6-418f-8ce9-b4030f052bd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 22736\n",
      "Validation set size: 4871\n",
      "Test set size: 4875\n",
      "AI Aug Test set size: 14149\n"
     ]
    }
   ],
   "source": [
    "model = ConvTransformerClassifier(n_classes=2, d_model=128, nhead=4, num_layers=2)\n",
    "dataloaders = get_dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3cd1203e-29f9-4656-a92b-3096bd2812df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n",
      "Train Loss: 0.2920 | Accuracy: 0.8865\n",
      "Val   Loss: 0.1580 | Accuracy: 0.9392\n",
      "\n",
      "Epoch 2/10\n",
      "Train Loss: 0.1743 | Accuracy: 0.9344\n",
      "Val   Loss: 0.3302 | Accuracy: 0.9056\n",
      "\n",
      "Epoch 3/10\n",
      "Train Loss: 0.1484 | Accuracy: 0.9447\n",
      "Val   Loss: 0.3335 | Accuracy: 0.9121\n",
      "\n",
      "Epoch 4/10\n",
      "Train Loss: 0.1201 | Accuracy: 0.9558\n",
      "Val   Loss: 0.3241 | Accuracy: 0.9183\n",
      "\n",
      "Epoch 5/10\n",
      "Train Loss: 0.0846 | Accuracy: 0.9692\n",
      "Val   Loss: 0.1832 | Accuracy: 0.9427\n",
      "\n",
      "Epoch 6/10\n",
      "Train Loss: 0.0666 | Accuracy: 0.9759\n",
      "Val   Loss: 0.0921 | Accuracy: 0.9711\n",
      "\n",
      "Epoch 7/10\n",
      "Train Loss: 0.0589 | Accuracy: 0.9810\n",
      "Val   Loss: 0.2084 | Accuracy: 0.9368\n",
      "\n",
      "Epoch 8/10\n",
      "Train Loss: 0.0466 | Accuracy: 0.9847\n",
      "Val   Loss: 0.3047 | Accuracy: 0.9300\n",
      "\n",
      "Epoch 9/10\n",
      "Train Loss: 0.0407 | Accuracy: 0.9872\n",
      "Val   Loss: 0.1666 | Accuracy: 0.9630\n",
      "\n",
      "Epoch 10/10\n",
      "Train Loss: 0.0189 | Accuracy: 0.9949\n",
      "Val   Loss: 0.1736 | Accuracy: 0.9633\n",
      "Best validation accuracy: 0.9711 at epoch 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(ConvTransformerClassifier(\n",
       "   (cnn): Sequential(\n",
       "     (0): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): ReLU()\n",
       "     (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "     (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (6): ReLU()\n",
       "     (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "     (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (10): ReLU()\n",
       "     (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "     (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (14): ReLU()\n",
       "     (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "   )\n",
       "   (flatten): Flatten(start_dim=2, end_dim=-1)\n",
       "   (positional_encoding): PositionalEncoding()\n",
       "   (transformer): TransformerEncoder(\n",
       "     (layers): ModuleList(\n",
       "       (0-1): 2 x TransformerEncoderLayer(\n",
       "         (self_attn): MultiheadAttention(\n",
       "           (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "         )\n",
       "         (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "         (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "         (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "         (dropout1): Dropout(p=0.1, inplace=False)\n",
       "         (dropout2): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "   (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (fc): Linear(in_features=128, out_features=2, bias=True)\n",
       " ),\n",
       " 0.9710531718332991)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(model, dataloaders, device=\"cuda\", epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d915127a-2f96-442b-8cec-66fceac68383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device=\"cuda\", csv_path=\"predictions.csv\"):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    test_loss, test_correct = 0.0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "    total_inference_time = 0.0\n",
    "    csv_rows = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y, filenames in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            start_time = time.time()\n",
    "            outputs = model(X)\n",
    "            end_time = time.time()\n",
    "\n",
    "            inference_time = end_time - start_time\n",
    "            total_inference_time += inference_time\n",
    "\n",
    "            loss = criterion(outputs, y)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            preds = outputs.argmax(1)\n",
    "\n",
    "            test_loss += loss.item() * X.size(0)\n",
    "            test_correct += (preds == y).sum().item()\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "            for i in range(X.size(0)):\n",
    "                prob_ai = probs[i][1].item()  # class 1 = AI\n",
    "                prob_human = probs[i][0].item()  # class 0 = Human\n",
    "                csv_rows.append([\n",
    "                    filenames[i],\n",
    "                    f\"{prob_ai:.6f}\",\n",
    "                    f\"{prob_human:.6f}\",\n",
    "                    y[i].item(),\n",
    "                    preds[i].item()\n",
    "                ])\n",
    "\n",
    "    with open(csv_path, mode='w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"filename\", \"prob_ai\", \"prob_human\", \"true_label\", \"pred_label\"])\n",
    "        writer.writerows(csv_rows)\n",
    "\n",
    "    test_acc = test_correct / len(dataloader.dataset)\n",
    "    test_loss /= len(dataloader.dataset)\n",
    "    avg_inference_time = total_inference_time / len(dataloader.dataset)\n",
    "\n",
    "    # Metrics\n",
    "    precision = precision_score(all_labels, all_preds, average='binary')\n",
    "    recall = recall_score(all_labels, all_preds, average='binary')\n",
    "    f1 = f1_score(all_labels, all_preds, average='binary')\n",
    "\n",
    "    # False Positive Rate (FPR = FP / (FP + TN))\n",
    "    tn, fp, fn, tp = confusion_matrix(all_labels, all_preds).ravel()\n",
    "    fpr = fp / (fp + tn + 1e-10)  # avoid division by zero\n",
    "\n",
    "    print(f\"Test  Loss: {test_loss:.4f} | Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f} | Recall: {recall:.4f} | F1 Score: {f1:.4f}\")\n",
    "    print(f\"False Positive Rate: {fpr:.4f}\")\n",
    "    print(f\"Avg Inference Time per Sample: {avg_inference_time * 1000:.2f} ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dba349e1-aee1-41b3-bab7-94655864a20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test  Loss: 0.0900 | Accuracy: 0.9725\n",
      "Precision: 0.9788 | Recall: 0.9856 | F1 Score: 0.9822\n",
      "False Positive Rate: 0.0710\n",
      "Avg Inference Time per Sample: 0.05 ms\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, dataloaders[\"test\"], device=\"cuda\", csv_path=\"tensors_test_large_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9d6066a2-cc47-4930-8b65-bf6832e509fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvTransformerClassifier(\n",
       "  (cnn): Sequential(\n",
       "    (0): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (14): ReLU()\n",
       "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (flatten): Flatten(start_dim=2, end_dim=-1)\n",
       "  (positional_encoding): PositionalEncoding()\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "  (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model weights\n",
    "torch.save(model.state_dict(), 'model_weights.pt')\n",
    "\n",
    "# Load model weights (later)\n",
    "model = ConvTransformerClassifier(n_classes=2, d_model=128, nhead=4, num_layers=2)\n",
    "model.load_state_dict(torch.load('model_weights.pt'))\n",
    "model.eval()  # Set to evaluation mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a8d461-4f14-4c3f-ae65-6ebf1b865e84",
   "metadata": {},
   "source": [
    "HYPERPARAM SEARCH BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "765a5b0a-ff3e-47fc-ab9c-70a4f3b046a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your search space\n",
    "param_grid = {\n",
    "    \"lr\": [1e-4, 5e-4, 1e-3],\n",
    "    \"weight_decay\": [0.0, 1e-5],\n",
    "    \"clip_grad\": [True, False],\n",
    "    \"patience\": [3, 5],\n",
    "    \"epochs\": [10, 15, 20]  \n",
    "}\n",
    "\n",
    "search_space = list(product(*param_grid.values()))\n",
    "random.shuffle(search_space)\n",
    "\n",
    "# Tracking best result\n",
    "best_val_acc = float(\"-inf\")\n",
    "best_params = None\n",
    "best_model_path = \"best_model.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8553bdd6-a4d1-45d7-83c0-a80244d7124d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Trial 1/10 with params: {'lr': 0.0005, 'weight_decay': 1e-05, 'clip_grad': True, 'patience': 5, 'epochs': 10}\n",
      "\n",
      "Epoch 1/10\n",
      "Train Loss: 0.3016 | Accuracy: 0.8922\n",
      "Val   Loss: 0.2605 | Accuracy: 0.9060\n",
      "\n",
      "Epoch 2/10\n",
      "Train Loss: 0.1828 | Accuracy: 0.9363\n",
      "Val   Loss: 0.2012 | Accuracy: 0.9306\n",
      "\n",
      "Epoch 3/10\n",
      "Train Loss: 0.1339 | Accuracy: 0.9507\n",
      "Val   Loss: 0.1246 | Accuracy: 0.9577\n",
      "\n",
      "Epoch 4/10\n",
      "Train Loss: 0.1102 | Accuracy: 0.9605\n",
      "Val   Loss: 0.2039 | Accuracy: 0.9396\n",
      "\n",
      "Epoch 5/10\n",
      "Train Loss: 0.0957 | Accuracy: 0.9681\n",
      "Val   Loss: 0.3255 | Accuracy: 0.9076\n",
      "\n",
      "Epoch 6/10\n",
      "Train Loss: 0.0815 | Accuracy: 0.9718\n",
      "Val   Loss: 0.0948 | Accuracy: 0.9684\n",
      "\n",
      "Epoch 7/10\n",
      "Train Loss: 0.0693 | Accuracy: 0.9770\n",
      "Val   Loss: 1.1867 | Accuracy: 0.8241\n",
      "\n",
      "Epoch 8/10\n",
      "Train Loss: 0.0612 | Accuracy: 0.9794\n",
      "Val   Loss: 0.1040 | Accuracy: 0.9684\n",
      "\n",
      "Epoch 9/10\n",
      "Train Loss: 0.0560 | Accuracy: 0.9814\n",
      "Val   Loss: 0.1274 | Accuracy: 0.9661\n",
      "\n",
      "Epoch 10/10\n",
      "Train Loss: 0.0307 | Accuracy: 0.9908\n",
      "Val   Loss: 0.2202 | Accuracy: 0.9509\n",
      "Best validation accuracy: 0.9684 at epoch 6\n",
      "Validation Accuracy: 0.9684\n",
      "âœ… New best model saved!\n",
      "\n",
      "ðŸ” Trial 2/10 with params: {'lr': 0.001, 'weight_decay': 1e-05, 'clip_grad': True, 'patience': 5, 'epochs': 10}\n",
      "\n",
      "Epoch 1/10\n",
      "Train Loss: 0.2788 | Accuracy: 0.8906\n",
      "Val   Loss: 0.4240 | Accuracy: 0.8438\n",
      "\n",
      "Epoch 2/10\n",
      "Train Loss: 0.1804 | Accuracy: 0.9327\n",
      "Val   Loss: 0.1664 | Accuracy: 0.9425\n",
      "\n",
      "Epoch 3/10\n",
      "Train Loss: 0.1509 | Accuracy: 0.9432\n",
      "Val   Loss: 0.1460 | Accuracy: 0.9524\n",
      "\n",
      "Epoch 4/10\n",
      "Train Loss: 0.1284 | Accuracy: 0.9527\n",
      "Val   Loss: 0.1434 | Accuracy: 0.9483\n",
      "\n",
      "Epoch 5/10\n",
      "Train Loss: 0.1162 | Accuracy: 0.9568\n",
      "Val   Loss: 0.1430 | Accuracy: 0.9491\n",
      "\n",
      "Epoch 6/10\n",
      "Train Loss: 0.0997 | Accuracy: 0.9638\n",
      "Val   Loss: 0.1217 | Accuracy: 0.9591\n",
      "\n",
      "Epoch 7/10\n",
      "Train Loss: 0.0824 | Accuracy: 0.9708\n",
      "Val   Loss: 1.1581 | Accuracy: 0.7953\n",
      "\n",
      "Epoch 8/10\n",
      "Train Loss: 0.0768 | Accuracy: 0.9733\n",
      "Val   Loss: 0.4980 | Accuracy: 0.8524\n",
      "\n",
      "Epoch 9/10\n",
      "Train Loss: 0.0666 | Accuracy: 0.9771\n",
      "Val   Loss: 0.0909 | Accuracy: 0.9711\n",
      "\n",
      "Epoch 10/10\n",
      "Train Loss: 0.0608 | Accuracy: 0.9785\n",
      "Val   Loss: 0.1210 | Accuracy: 0.9581\n",
      "Best validation accuracy: 0.9711 at epoch 9\n",
      "Validation Accuracy: 0.9711\n",
      "âœ… New best model saved!\n",
      "\n",
      "ðŸ” Trial 3/10 with params: {'lr': 0.0001, 'weight_decay': 1e-05, 'clip_grad': True, 'patience': 5, 'epochs': 10}\n",
      "\n",
      "Epoch 1/10\n",
      "Train Loss: 0.2886 | Accuracy: 0.8972\n",
      "Val   Loss: 0.2723 | Accuracy: 0.8969\n",
      "\n",
      "Epoch 2/10\n",
      "Train Loss: 0.1853 | Accuracy: 0.9389\n",
      "Val   Loss: 0.2339 | Accuracy: 0.9179\n",
      "\n",
      "Epoch 3/10\n",
      "Train Loss: 0.1434 | Accuracy: 0.9524\n",
      "Val   Loss: 0.2184 | Accuracy: 0.9353\n",
      "\n",
      "Epoch 4/10\n",
      "Train Loss: 0.1219 | Accuracy: 0.9615\n",
      "Val   Loss: 0.1497 | Accuracy: 0.9515\n",
      "\n",
      "Epoch 5/10\n",
      "Train Loss: 0.1072 | Accuracy: 0.9661\n",
      "Val   Loss: 0.1377 | Accuracy: 0.9596\n",
      "\n",
      "Epoch 6/10\n",
      "Train Loss: 0.0895 | Accuracy: 0.9740\n",
      "Val   Loss: 0.1277 | Accuracy: 0.9657\n",
      "\n",
      "Epoch 7/10\n",
      "Train Loss: 0.0724 | Accuracy: 0.9778\n",
      "Val   Loss: 0.2745 | Accuracy: 0.9329\n",
      "\n",
      "Epoch 8/10\n",
      "Train Loss: 0.0631 | Accuracy: 0.9814\n",
      "Val   Loss: 0.1765 | Accuracy: 0.9684\n",
      "\n",
      "Epoch 9/10\n",
      "Train Loss: 0.0562 | Accuracy: 0.9854\n",
      "Val   Loss: 0.1326 | Accuracy: 0.9754\n",
      "\n",
      "Epoch 10/10\n",
      "Train Loss: 0.0255 | Accuracy: 0.9936\n",
      "Val   Loss: 0.2577 | Accuracy: 0.9552\n",
      "Best validation accuracy: 0.9754 at epoch 9\n",
      "Validation Accuracy: 0.9754\n",
      "âœ… New best model saved!\n",
      "\n",
      "ðŸ” Trial 4/10 with params: {'lr': 0.0001, 'weight_decay': 0.0, 'clip_grad': True, 'patience': 5, 'epochs': 20}\n",
      "\n",
      "Epoch 1/20\n",
      "Train Loss: 0.2886 | Accuracy: 0.8972\n",
      "Val   Loss: 0.2723 | Accuracy: 0.8969\n",
      "\n",
      "Epoch 2/20\n",
      "Train Loss: 0.1860 | Accuracy: 0.9397\n",
      "Val   Loss: 0.3000 | Accuracy: 0.8949\n",
      "\n",
      "Epoch 3/20\n",
      "Train Loss: 0.1442 | Accuracy: 0.9532\n",
      "Val   Loss: 0.1339 | Accuracy: 0.9575\n",
      "\n",
      "Epoch 4/20\n",
      "Train Loss: 0.1265 | Accuracy: 0.9603\n",
      "Val   Loss: 0.1261 | Accuracy: 0.9591\n",
      "\n",
      "Epoch 5/20\n",
      "Train Loss: 0.1054 | Accuracy: 0.9671\n",
      "Val   Loss: 0.1416 | Accuracy: 0.9550\n",
      "\n",
      "Epoch 6/20\n",
      "Train Loss: 0.0876 | Accuracy: 0.9748\n",
      "Val   Loss: 0.1130 | Accuracy: 0.9694\n",
      "\n",
      "Epoch 7/20\n",
      "Train Loss: 0.0730 | Accuracy: 0.9793\n",
      "Val   Loss: 0.3915 | Accuracy: 0.9136\n",
      "\n",
      "Epoch 8/20\n",
      "Train Loss: 0.0624 | Accuracy: 0.9817\n",
      "Val   Loss: 0.1937 | Accuracy: 0.9655\n",
      "\n",
      "Epoch 9/20\n",
      "Train Loss: 0.0641 | Accuracy: 0.9837\n",
      "Val   Loss: 0.2305 | Accuracy: 0.9591\n",
      "\n",
      "Epoch 10/20\n",
      "Train Loss: 0.0264 | Accuracy: 0.9933\n",
      "Val   Loss: 0.1787 | Accuracy: 0.9696\n",
      "\n",
      "Epoch 11/20\n",
      "Train Loss: 0.0130 | Accuracy: 0.9969\n",
      "Val   Loss: 0.1893 | Accuracy: 0.9735\n",
      "\n",
      "Epoch 12/20\n",
      "Train Loss: 0.0132 | Accuracy: 0.9964\n",
      "Val   Loss: 0.2630 | Accuracy: 0.9647\n",
      "\n",
      "Epoch 13/20\n",
      "Train Loss: 0.0060 | Accuracy: 0.9985\n",
      "Val   Loss: 0.1938 | Accuracy: 0.9747\n",
      "\n",
      "Epoch 14/20\n",
      "Train Loss: 0.0040 | Accuracy: 0.9990\n",
      "Val   Loss: 0.2089 | Accuracy: 0.9735\n",
      "\n",
      "Epoch 15/20\n",
      "Train Loss: 0.0049 | Accuracy: 0.9989\n",
      "Val   Loss: 0.2375 | Accuracy: 0.9717\n",
      "\n",
      "Epoch 16/20\n",
      "Train Loss: 0.0025 | Accuracy: 0.9993\n",
      "Val   Loss: 0.2440 | Accuracy: 0.9731\n",
      "\n",
      "Epoch 17/20\n",
      "Train Loss: 0.0018 | Accuracy: 0.9995\n",
      "Val   Loss: 0.2424 | Accuracy: 0.9725\n",
      "\n",
      "Epoch 18/20\n",
      "Train Loss: 0.0012 | Accuracy: 0.9997\n",
      "Val   Loss: 0.2453 | Accuracy: 0.9723\n",
      "Early stopping triggered.\n",
      "Best validation accuracy: 0.9747 at epoch 13\n",
      "Validation Accuracy: 0.9747\n",
      "\n",
      "ðŸ” Trial 5/10 with params: {'lr': 0.001, 'weight_decay': 0.0, 'clip_grad': True, 'patience': 3, 'epochs': 15}\n",
      "\n",
      "Epoch 1/15\n",
      "Train Loss: 0.2876 | Accuracy: 0.8875\n",
      "Val   Loss: 0.5216 | Accuracy: 0.8306\n",
      "\n",
      "Epoch 2/15\n",
      "Train Loss: 0.1788 | Accuracy: 0.9340\n",
      "Val   Loss: 0.3929 | Accuracy: 0.8762\n",
      "\n",
      "Epoch 3/15\n",
      "Train Loss: 0.1468 | Accuracy: 0.9451\n",
      "Val   Loss: 0.1675 | Accuracy: 0.9384\n",
      "\n",
      "Epoch 4/15\n",
      "Train Loss: 0.1269 | Accuracy: 0.9542\n",
      "Val   Loss: 0.1168 | Accuracy: 0.9594\n",
      "\n",
      "Epoch 5/15\n",
      "Train Loss: 0.1130 | Accuracy: 0.9580\n",
      "Val   Loss: 0.1155 | Accuracy: 0.9610\n",
      "\n",
      "Epoch 6/15\n",
      "Train Loss: 0.0960 | Accuracy: 0.9666\n",
      "Val   Loss: 0.1222 | Accuracy: 0.9569\n",
      "\n",
      "Epoch 7/15\n",
      "Train Loss: 0.0872 | Accuracy: 0.9676\n",
      "Val   Loss: 0.2943 | Accuracy: 0.9136\n",
      "\n",
      "Epoch 8/15\n",
      "Train Loss: 0.0746 | Accuracy: 0.9732\n",
      "Val   Loss: 0.5632 | Accuracy: 0.8204\n",
      "Early stopping triggered.\n",
      "Best validation accuracy: 0.9610 at epoch 5\n",
      "Validation Accuracy: 0.9610\n",
      "\n",
      "ðŸ” Trial 6/10 with params: {'lr': 0.001, 'weight_decay': 0.0, 'clip_grad': True, 'patience': 5, 'epochs': 10}\n",
      "\n",
      "Epoch 1/10\n",
      "Train Loss: 0.2948 | Accuracy: 0.8834\n",
      "Val   Loss: 0.2746 | Accuracy: 0.8914\n",
      "\n",
      "Epoch 2/10\n",
      "Train Loss: 0.1832 | Accuracy: 0.9334\n",
      "Val   Loss: 0.6971 | Accuracy: 0.8087\n",
      "\n",
      "Epoch 3/10\n",
      "Train Loss: 0.1524 | Accuracy: 0.9423\n",
      "Val   Loss: 0.1806 | Accuracy: 0.9353\n",
      "\n",
      "Epoch 4/10\n",
      "Train Loss: 0.1300 | Accuracy: 0.9515\n",
      "Val   Loss: 0.1104 | Accuracy: 0.9614\n",
      "\n",
      "Epoch 5/10\n",
      "Train Loss: 0.1150 | Accuracy: 0.9600\n",
      "Val   Loss: 0.2656 | Accuracy: 0.9132\n",
      "\n",
      "Epoch 6/10\n",
      "Train Loss: 0.1008 | Accuracy: 0.9635\n",
      "Val   Loss: 0.3042 | Accuracy: 0.9091\n",
      "\n",
      "Epoch 7/10\n",
      "Train Loss: 0.0890 | Accuracy: 0.9686\n",
      "Val   Loss: 0.1902 | Accuracy: 0.9427\n",
      "\n",
      "Epoch 8/10\n",
      "Train Loss: 0.0550 | Accuracy: 0.9800\n",
      "Val   Loss: 0.1626 | Accuracy: 0.9483\n",
      "\n",
      "Epoch 9/10\n",
      "Train Loss: 0.0434 | Accuracy: 0.9858\n",
      "Val   Loss: 0.1324 | Accuracy: 0.9645\n",
      "\n",
      "Epoch 10/10\n",
      "Train Loss: 0.0347 | Accuracy: 0.9899\n",
      "Val   Loss: 0.2849 | Accuracy: 0.9380\n",
      "Best validation accuracy: 0.9645 at epoch 9\n",
      "Validation Accuracy: 0.9645\n",
      "\n",
      "ðŸ” Trial 7/10 with params: {'lr': 0.0001, 'weight_decay': 0.0, 'clip_grad': False, 'patience': 5, 'epochs': 15}\n",
      "\n",
      "Epoch 1/15\n",
      "Train Loss: 0.3003 | Accuracy: 0.8901\n",
      "Val   Loss: 0.2504 | Accuracy: 0.9010\n",
      "\n",
      "Epoch 2/15\n",
      "Train Loss: 0.1857 | Accuracy: 0.9358\n",
      "Val   Loss: 0.1377 | Accuracy: 0.9495\n",
      "\n",
      "Epoch 3/15\n",
      "Train Loss: 0.1383 | Accuracy: 0.9516\n",
      "Val   Loss: 0.2640 | Accuracy: 0.9183\n",
      "\n",
      "Epoch 4/15\n",
      "Train Loss: 0.1196 | Accuracy: 0.9587\n",
      "Val   Loss: 0.2448 | Accuracy: 0.9187\n",
      "\n",
      "Epoch 5/15\n",
      "Train Loss: 0.1094 | Accuracy: 0.9612\n",
      "Val   Loss: 0.1186 | Accuracy: 0.9581\n",
      "\n",
      "Epoch 6/15\n",
      "Train Loss: 0.0916 | Accuracy: 0.9685\n",
      "Val   Loss: 0.2564 | Accuracy: 0.9152\n",
      "\n",
      "Epoch 7/15\n",
      "Train Loss: 0.0765 | Accuracy: 0.9738\n",
      "Val   Loss: 0.1817 | Accuracy: 0.9460\n",
      "\n",
      "Epoch 8/15\n",
      "Train Loss: 0.0667 | Accuracy: 0.9772\n",
      "Val   Loss: 0.1108 | Accuracy: 0.9663\n",
      "\n",
      "Epoch 9/15\n",
      "Train Loss: 0.0633 | Accuracy: 0.9789\n",
      "Val   Loss: 0.1609 | Accuracy: 0.9532\n",
      "\n",
      "Epoch 10/15\n",
      "Train Loss: 0.0529 | Accuracy: 0.9826\n",
      "Val   Loss: 0.2220 | Accuracy: 0.9333\n",
      "\n",
      "Epoch 11/15\n",
      "Train Loss: 0.0429 | Accuracy: 0.9857\n",
      "Val   Loss: 0.1279 | Accuracy: 0.9669\n",
      "\n",
      "Epoch 12/15\n",
      "Train Loss: 0.0206 | Accuracy: 0.9932\n",
      "Val   Loss: 0.1947 | Accuracy: 0.9532\n",
      "\n",
      "Epoch 13/15\n",
      "Train Loss: 0.0150 | Accuracy: 0.9954\n",
      "Val   Loss: 0.1292 | Accuracy: 0.9686\n",
      "\n",
      "Epoch 14/15\n",
      "Train Loss: 0.0143 | Accuracy: 0.9953\n",
      "Val   Loss: 0.1213 | Accuracy: 0.9688\n",
      "\n",
      "Epoch 15/15\n",
      "Train Loss: 0.0062 | Accuracy: 0.9983\n",
      "Val   Loss: 0.1119 | Accuracy: 0.9729\n",
      "Best validation accuracy: 0.9729 at epoch 15\n",
      "Validation Accuracy: 0.9729\n",
      "\n",
      "ðŸ” Trial 8/10 with params: {'lr': 0.001, 'weight_decay': 1e-05, 'clip_grad': False, 'patience': 5, 'epochs': 10}\n",
      "\n",
      "Epoch 1/10\n",
      "Train Loss: 0.2960 | Accuracy: 0.8822\n",
      "Val   Loss: 0.3958 | Accuracy: 0.8485\n",
      "\n",
      "Epoch 2/10\n",
      "Train Loss: 0.1798 | Accuracy: 0.9333\n",
      "Val   Loss: 0.8989 | Accuracy: 0.7988\n",
      "\n",
      "Epoch 3/10\n",
      "Train Loss: 0.1512 | Accuracy: 0.9433\n",
      "Val   Loss: 0.1577 | Accuracy: 0.9485\n",
      "\n",
      "Epoch 4/10\n",
      "Train Loss: 0.1298 | Accuracy: 0.9521\n",
      "Val   Loss: 0.7621 | Accuracy: 0.8029\n",
      "\n",
      "Epoch 5/10\n",
      "Train Loss: 0.1173 | Accuracy: 0.9570\n",
      "Val   Loss: 0.2239 | Accuracy: 0.9339\n",
      "\n",
      "Epoch 6/10\n",
      "Train Loss: 0.1034 | Accuracy: 0.9614\n",
      "Val   Loss: 0.5424 | Accuracy: 0.8530\n",
      "\n",
      "Epoch 7/10\n",
      "Train Loss: 0.0657 | Accuracy: 0.9755\n",
      "Val   Loss: 0.1165 | Accuracy: 0.9663\n",
      "\n",
      "Epoch 8/10\n",
      "Train Loss: 0.0538 | Accuracy: 0.9816\n",
      "Val   Loss: 0.1419 | Accuracy: 0.9540\n",
      "\n",
      "Epoch 9/10\n",
      "Train Loss: 0.0461 | Accuracy: 0.9837\n",
      "Val   Loss: 0.1078 | Accuracy: 0.9669\n",
      "\n",
      "Epoch 10/10\n",
      "Train Loss: 0.0407 | Accuracy: 0.9873\n",
      "Val   Loss: 0.2846 | Accuracy: 0.9132\n",
      "Best validation accuracy: 0.9669 at epoch 9\n",
      "Validation Accuracy: 0.9669\n",
      "\n",
      "ðŸ” Trial 9/10 with params: {'lr': 0.0005, 'weight_decay': 1e-05, 'clip_grad': True, 'patience': 3, 'epochs': 10}\n",
      "\n",
      "Epoch 1/10\n",
      "Train Loss: 0.3021 | Accuracy: 0.8881\n",
      "Val   Loss: 0.2062 | Accuracy: 0.9187\n",
      "\n",
      "Epoch 2/10\n",
      "Train Loss: 0.1748 | Accuracy: 0.9372\n",
      "Val   Loss: 0.3731 | Accuracy: 0.8943\n",
      "\n",
      "Epoch 3/10\n",
      "Train Loss: 0.1398 | Accuracy: 0.9496\n",
      "Val   Loss: 0.1241 | Accuracy: 0.9567\n",
      "\n",
      "Epoch 4/10\n",
      "Train Loss: 0.1169 | Accuracy: 0.9581\n",
      "Val   Loss: 0.1277 | Accuracy: 0.9565\n",
      "\n",
      "Epoch 5/10\n",
      "Train Loss: 0.1028 | Accuracy: 0.9639\n",
      "Val   Loss: 0.2756 | Accuracy: 0.9158\n",
      "\n",
      "Epoch 6/10\n",
      "Train Loss: 0.0868 | Accuracy: 0.9705\n",
      "Val   Loss: 0.3474 | Accuracy: 0.9166\n",
      "Early stopping triggered.\n",
      "Best validation accuracy: 0.9567 at epoch 3\n",
      "Validation Accuracy: 0.9567\n",
      "\n",
      "ðŸ” Trial 10/10 with params: {'lr': 0.0005, 'weight_decay': 1e-05, 'clip_grad': True, 'patience': 5, 'epochs': 15}\n",
      "\n",
      "Epoch 1/15\n",
      "Train Loss: 0.2943 | Accuracy: 0.8915\n",
      "Val   Loss: 0.2370 | Accuracy: 0.9029\n",
      "\n",
      "Epoch 2/15\n",
      "Train Loss: 0.1707 | Accuracy: 0.9383\n",
      "Val   Loss: 0.5804 | Accuracy: 0.8237\n",
      "\n",
      "Epoch 3/15\n",
      "Train Loss: 0.1306 | Accuracy: 0.9530\n",
      "Val   Loss: 0.1755 | Accuracy: 0.9532\n",
      "\n",
      "Epoch 4/15\n",
      "Train Loss: 0.1162 | Accuracy: 0.9590\n",
      "Val   Loss: 0.2161 | Accuracy: 0.9333\n",
      "\n",
      "Epoch 5/15\n",
      "Train Loss: 0.1028 | Accuracy: 0.9634\n",
      "Val   Loss: 0.3057 | Accuracy: 0.9080\n",
      "\n",
      "Epoch 6/15\n",
      "Train Loss: 0.0873 | Accuracy: 0.9704\n",
      "Val   Loss: 0.3377 | Accuracy: 0.9019\n",
      "\n",
      "Epoch 7/15\n",
      "Train Loss: 0.0494 | Accuracy: 0.9843\n",
      "Val   Loss: 0.3086 | Accuracy: 0.9300\n",
      "\n",
      "Epoch 8/15\n",
      "Train Loss: 0.0397 | Accuracy: 0.9872\n",
      "Val   Loss: 0.1512 | Accuracy: 0.9661\n",
      "\n",
      "Epoch 9/15\n",
      "Train Loss: 0.0329 | Accuracy: 0.9898\n",
      "Val   Loss: 0.1268 | Accuracy: 0.9696\n",
      "\n",
      "Epoch 10/15\n",
      "Train Loss: 0.0319 | Accuracy: 0.9914\n",
      "Val   Loss: 0.1927 | Accuracy: 0.9604\n",
      "\n",
      "Epoch 11/15\n",
      "Train Loss: 0.0252 | Accuracy: 0.9927\n",
      "Val   Loss: 0.1968 | Accuracy: 0.9700\n",
      "\n",
      "Epoch 12/15\n",
      "Train Loss: 0.0210 | Accuracy: 0.9941\n",
      "Val   Loss: 0.1843 | Accuracy: 0.9667\n",
      "\n",
      "Epoch 13/15\n",
      "Train Loss: 0.0089 | Accuracy: 0.9978\n",
      "Val   Loss: 0.1735 | Accuracy: 0.9735\n",
      "\n",
      "Epoch 14/15\n",
      "Train Loss: 0.0074 | Accuracy: 0.9982\n",
      "Val   Loss: 0.1890 | Accuracy: 0.9758\n",
      "\n",
      "Epoch 15/15\n",
      "Train Loss: 0.0076 | Accuracy: 0.9984\n",
      "Val   Loss: 0.1738 | Accuracy: 0.9752\n",
      "Best validation accuracy: 0.9758 at epoch 14\n",
      "Validation Accuracy: 0.9758\n",
      "âœ… New best model saved!\n",
      "\n",
      "ðŸ† Best Hyperparameters:\n",
      "{'lr': 0.0005, 'weight_decay': 1e-05, 'clip_grad': True, 'patience': 5, 'epochs': 15}\n",
      "Best Validation Accuracy: 0.9758\n",
      "Model saved to: best_model.pt\n"
     ]
    }
   ],
   "source": [
    "# Start search\n",
    "max_trials = min(10, len(search_space))\n",
    "for i, values in enumerate(search_space[:max_trials]):\n",
    "    params = dict(zip(param_grid.keys(), values))\n",
    "    print(f\"\\nTrial {i + 1}/{max_trials} with params: {params}\")\n",
    "\n",
    "    model = ConvTransformerClassifier(n_classes=2, d_model=128, nhead=4, num_layers=2) \n",
    "    trained_model, val_acc = train_model(model, dataloaders, device=\"cuda\", **params)\n",
    "\n",
    "    print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_params = params\n",
    "\n",
    "        # Save the best model\n",
    "        torch.save(trained_model.state_dict(), best_model_path)\n",
    "        print(\"New best model saved!\")\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "print(best_params)\n",
    "print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")\n",
    "print(f\"Model saved to: {best_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4a597d5-fbb7-4312-9f26-d3c7af31451d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test  Loss: 0.1772 | Accuracy: 0.9770\n",
      "Precision: 0.9815 | Recall: 0.9888 | F1 Score: 0.9851\n",
      "False Positive Rate: 0.0622\n",
      "Avg Inference Time per Sample: 0.05 ms\n"
     ]
    }
   ],
   "source": [
    "model = ConvTransformerClassifier(n_classes=2, d_model=128, nhead=4, num_layers=2)\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "device=\"cuda\"\n",
    "model.to(device)\n",
    "model.eval()  # Set to evaluation mode\n",
    "evaluate_model(model, dataloaders[\"test\"], device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a430025-efd1-4ec1-a280-19b5ff75959d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO\n"
     ]
    }
   ],
   "source": [
    "print(\"HELLO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9bb6e3c5-9c6f-4cd3-a3e3-f73536c20e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 443/443 [00:28<00:00, 15.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         filename   prob_ai  prob_human  true_label  \\\n",
      "0   S622RN_segment_2_stretched.pt  0.000007    0.999993           1   \n",
      "1  S3363RN_segment_2_stretched.pt  0.000010    0.999990           1   \n",
      "2    S2378RN_segment_1_shifted.pt  0.000009    0.999992           1   \n",
      "3  U1291RN_segment_2_stretched.pt  0.000007    0.999993           1   \n",
      "4  S4761RN_segment_1_stretched.pt  0.000007    0.999993           1   \n",
      "\n",
      "   pred_label  \n",
      "0           1  \n",
      "1           1  \n",
      "2           1  \n",
      "3           1  \n",
      "4           1  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = ConvTransformerClassifier(n_classes=2, d_model=128, nhead=4, num_layers=2)\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "device=\"cuda\"\n",
    "model.to(device)\n",
    "model.eval()  # Set to evaluation mode\n",
    "results = []\n",
    "\n",
    "for batch in tqdm(dataloaders[\"ai_aug_test\"]):\n",
    "    mel_tensors, labels, filenames = batch \n",
    "    mel_tensors = mel_tensors.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(mel_tensors)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "        for i in range(1):\n",
    "            results.append({\n",
    "                \"filename\": filenames[i],  \n",
    "                \"prob_ai\": probs[i][0].item(),\n",
    "                \"prob_human\": probs[i][1].item(),\n",
    "                \"true_label\": labels[i].item(),\n",
    "                \"pred_label\": torch.argmax(probs[i]).item()\n",
    "            })\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"best_mel-spec_ai_aug_test_predictions.csv\", index=False)\n",
    "\n",
    "# Preview results\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f11a7fd8-9bd4-42f8-ba26-c7cfa10cde15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 443/443 (100.00%)\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV\n",
    "df = pd.read_csv(\"best_mel-spec_ai_aug_test_predictions.csv\")  \n",
    "\n",
    "total = len(df)\n",
    "\n",
    "# Count where prediction is correct\n",
    "correct = (df[\"true_label\"] == df[\"pred_label\"]).sum()\n",
    "\n",
    "print(f\"Correct predictions: {correct}/{total} ({correct/total:.2%})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b07e2c8-41e6-4cc0-9c21-6289526b679f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
