{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecfb6aab-283a-4780-a21b-6bc52d095b81",
   "metadata": {},
   "source": [
    "Input: Mel Spectrogram (1, 128, 256)\n",
    "â†“\n",
    "Conv Block Ã—4: (Conv â†’ BN + ReLU â†’ MaxPool)\n",
    "â†“\n",
    "Flatten + Positional Encoding\n",
    "â†“\n",
    "Transformer Encoder Layer Ã—2\n",
    "â†“\n",
    "Avg Pool â†’ BN â†’ FC â†’ Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "747cbd70-1d18-4cb0-805b-7a0cb9831de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO\n"
     ]
    }
   ],
   "source": [
    "print(\"HELLO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2c20bc2-9fed-4ec7-a9ea-85d434c351e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "import time\n",
    "\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7da72ef-9ed2-4429-83d0-414bc54f5198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5a4ae91-0920-4978-9a2c-5f553ec2f6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelSpectrogramDataset(Dataset):\n",
    "    def __init__(self, samples, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            samples (List[Tuple[str, int]]): List of (tensor_path, label) pairs.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.samples = samples\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tensor_path, label = self.samples[idx]\n",
    "        mel_tensor = torch.load(tensor_path)  # Expected shape: (1, 128, 256)\n",
    "\n",
    "        if self.transform:\n",
    "            mel_tensor = self.transform(mel_tensor)\n",
    "\n",
    "        return mel_tensor, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dde1f672-0ffa-4533-956c-452a65ae00b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(root_dir, batch_size=32, val_frac=0.1, test_frac=0.1):\n",
    "    # Helper function to read file paths from a text file\n",
    "    def read_file_paths(file_name):\n",
    "        with open(file_name, 'r') as f:\n",
    "            return [line.strip() for line in f.readlines()]\n",
    "        \n",
    "    ai_segments_path = \"/vol/bitbucket/sg2121/fypdataset/dataset_large2/normal_data/ai_segments\"\n",
    "    human_segments_path = \"/vol/bitbucket/sg2121/fypdataset/dataset_large2/normal_data/human\"\n",
    "    ai_tensor_path = \"/vol/bitbucket/sg2121/fypdataset/dataset_large2/tensors/ai_segments/mel_spec_tensor\"\n",
    "    human_tensor_path = \"/vol/bitbucket/sg2121/fypdataset/dataset_large2/tensors/human/mel_spec_tensor\"\n",
    "\n",
    "    train_files = read_file_paths('/vol/bitbucket/sg2121/fyp/aimusicdetector/train_test_split/bitbucket/train_files_large.txt')\n",
    "    val_files = read_file_paths('/vol/bitbucket/sg2121/fyp/aimusicdetector/train_test_split/bitbucket/val_files_large.txt')\n",
    "    test_files = read_file_paths('/vol/bitbucket/sg2121/fyp/aimusicdetector/train_test_split/bitbucket/test_files_large.txt')\n",
    "\n",
    "    # Function to convert segment file path to lyric file path\n",
    "    def convert_to_tensor_path(file_path, is_ai):\n",
    "        if is_ai:\n",
    "            if file_path.startswith(ai_segments_path):\n",
    "                base_tensor_path = ai_tensor_path\n",
    "            else:\n",
    "                return\n",
    "        else:\n",
    "            if file_path.startswith(human_segments_path):\n",
    "                base_tensor_path = human_tensor_path\n",
    "            else:\n",
    "                return\n",
    "\n",
    "        # Convert filename to mfcc filename\n",
    "        file_name = os.path.basename(file_path).replace('.mp3', '.pt')\n",
    "        return os.path.join(base_tensor_path, file_name)\n",
    "\n",
    "    # Process the file lists and create tuples of (lyric_path, label)\n",
    "    def process_file_paths(file_paths, is_ai):\n",
    "        return [(convert_to_tensor_path(file_path, is_ai), 0 if is_ai else 1) for file_path in file_paths]\n",
    "    \n",
    "    # Convert all file paths from the train, validation, and test sets\n",
    "    ai_train_files = process_file_paths(train_files, is_ai=True)\n",
    "    human_train_files = process_file_paths(train_files, is_ai=False)\n",
    "    \n",
    "    ai_val_files = process_file_paths(val_files, is_ai=True)\n",
    "    human_val_files = process_file_paths(val_files, is_ai=False)\n",
    "    \n",
    "    ai_test_files = process_file_paths(test_files, is_ai=True)\n",
    "    human_test_files = process_file_paths(test_files, is_ai=False)\n",
    "    \n",
    "    def clean(paths):\n",
    "        return [(p, l) for p, l in paths if p is not None]\n",
    "    \n",
    "    train_files_combined = clean(ai_train_files) + clean(human_train_files)\n",
    "    val_files_combined = clean(ai_val_files) + clean(human_val_files)\n",
    "    test_files_combined = clean(ai_test_files) + clean(human_test_files)\n",
    "    \n",
    "    # Shuffle the data if needed\n",
    "    random.shuffle(train_files_combined)\n",
    "    random.shuffle(val_files_combined)\n",
    "    random.shuffle(test_files_combined)\n",
    "\n",
    "    # Example of how you might check the splits\n",
    "    print(f\"Training set size: {len(train_files_combined)}\")\n",
    "    print(f\"Validation set size: {len(val_files_combined)}\")\n",
    "    print(f\"Test set size: {len(test_files_combined)}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = MelSpectrogramDataset(train_files_combined)\n",
    "    val_dataset = MelSpectrogramDataset(val_files_combined)\n",
    "    test_dataset = MelSpectrogramDataset(test_files_combined)\n",
    "    \n",
    "    return {\n",
    "        \"train\": DataLoader(train_dataset, batch_size=batch_size, shuffle=True),\n",
    "        \"val\": DataLoader(val_dataset, batch_size=batch_size),\n",
    "        \"test\": DataLoader(test_dataset, batch_size=batch_size),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b6fdcc9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class ConvTransformerClassifier(nn.Module):\n",
    "    def __init__(self, n_classes, d_model=128, nhead=4, num_layers=2, input_shape=(1, 128, 256)):\n",
    "        super(ConvTransformerClassifier, self).__init__()\n",
    "\n",
    "        # --- CNN Feature Extractor ---\n",
    "        conv_layers = []\n",
    "        in_channels = input_shape[0]\n",
    "        for _ in range(4):\n",
    "            conv_layers += [\n",
    "                nn.Conv2d(in_channels, d_model, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(d_model),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2)\n",
    "            ]\n",
    "            in_channels = d_model\n",
    "\n",
    "        self.cnn = nn.Sequential(*conv_layers)\n",
    "\n",
    "        # --- Flatten + Positional Encoding ---\n",
    "        self.flatten = nn.Flatten(2)  # flatten spatial dims into a sequence\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "\n",
    "        # --- Transformer Encoder ---\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # --- Classification Head ---\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.bn = nn.BatchNorm1d(d_model)\n",
    "        self.fc = nn.Linear(d_model, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 1, 128, 256)\n",
    "        x = self.cnn(x)  # (B, C, H', W') => e.g. (B, 128, 8, 16)\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        x = self.flatten(x)            # (B, C, H*W)  â†’ sequence\n",
    "        x = x.permute(0, 2, 1)         # (B, seq_len, C)\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        x = self.transformer(x)        # (B, seq_len, C)\n",
    "        x = x.permute(0, 2, 1)         # (B, C, seq_len)\n",
    "        x = self.avgpool(x).squeeze(-1)  # (B, C)\n",
    "\n",
    "        x = self.bn(x)\n",
    "        x = self.fc(x)                 # (B, n_classes)\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=1024):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)  # (max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)  # (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, seq_len, d_model)\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d18332d1-5109-4700-87d3-33e7e7d5edba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, device=\"cuda\", epochs=10, lr=1e-3, weight_decay=0.0, clip_grad=True, patience=5):\n",
    "    set_seed()\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "\n",
    "    best_model = None\n",
    "    best_val_acc = 0.0\n",
    "    best_epoch = 0\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        model.train()\n",
    "        train_loss, train_correct = 0.0, 0\n",
    "\n",
    "        for X, y in dataloaders[\"train\"]:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            if clip_grad:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * X.size(0)\n",
    "            train_correct += (outputs.argmax(1) == y).sum().item()\n",
    "\n",
    "        train_acc = train_correct / len(dataloaders[\"train\"].dataset)\n",
    "        train_loss /= len(dataloaders[\"train\"].dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_correct = 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in dataloaders[\"val\"]:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                outputs = model(X)\n",
    "                loss = criterion(outputs, y)\n",
    "                val_loss += loss.item() * X.size(0)\n",
    "                val_correct += (outputs.argmax(1) == y).sum().item()\n",
    "\n",
    "        val_acc = val_correct / len(dataloaders[\"val\"].dataset)\n",
    "        val_loss /= len(dataloaders[\"val\"].dataset)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Accuracy: {train_acc:.4f}\")\n",
    "        print(f\"Val   Loss: {val_loss:.4f} | Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            best_epoch = epoch\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_model)\n",
    "    print(f\"Best validation accuracy: {best_val_acc:.4f} at epoch {best_epoch+1}\")\n",
    "    return model, best_val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fadc06d2-90a6-418f-8ce9-b4030f052bd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 22736\n",
      "Validation set size: 4871\n",
      "Test set size: 4875\n"
     ]
    }
   ],
   "source": [
    "# Suppose `model` is your CNN+Transformer hybrid\n",
    "model = ConvTransformerClassifier(n_classes=2, d_model=128, nhead=4, num_layers=2)\n",
    "dataloaders = get_dataloaders(\"/vol/bitbucket/sg2121/fypdataset/dataset_large2/tensors\", batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd1203e-29f9-4656-a92b-3096bd2812df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, dataloaders, device=\"cuda\", epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d915127a-2f96-442b-8cec-66fceac68383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    test_loss, test_correct = 0.0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "    total_inference_time = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            start_time = time.time()\n",
    "            outputs = model(X)\n",
    "            end_time = time.time()\n",
    "\n",
    "            inference_time = end_time - start_time\n",
    "            total_inference_time += inference_time\n",
    "\n",
    "            loss = criterion(outputs, y)\n",
    "            preds = outputs.argmax(1)\n",
    "\n",
    "            test_loss += loss.item() * X.size(0)\n",
    "            test_correct += (preds == y).sum().item()\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    test_acc = test_correct / len(dataloader.dataset)\n",
    "    test_loss /= len(dataloader.dataset)\n",
    "    avg_inference_time = total_inference_time / len(dataloader.dataset)\n",
    "\n",
    "    # Metrics\n",
    "    precision = precision_score(all_labels, all_preds, average='binary')\n",
    "    recall = recall_score(all_labels, all_preds, average='binary')\n",
    "    f1 = f1_score(all_labels, all_preds, average='binary')\n",
    "\n",
    "    # False Positive Rate (FPR = FP / (FP + TN))\n",
    "    tn, fp, fn, tp = confusion_matrix(all_labels, all_preds).ravel()\n",
    "    fpr = fp / (fp + tn + 1e-10)  # avoid division by zero\n",
    "\n",
    "    print(f\"Test  Loss: {test_loss:.4f} | Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f} | Recall: {recall:.4f} | F1 Score: {f1:.4f}\")\n",
    "    print(f\"False Positive Rate: {fpr:.4f}\")\n",
    "    print(f\"Avg Inference Time per Sample: {avg_inference_time * 1000:.2f} ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dba349e1-aee1-41b3-bab7-94655864a20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test  Loss: 0.0836 | Accuracy: 0.9705\n",
      "Precision: 0.9727 | Recall: 0.9893 | F1 Score: 0.9810\n",
      "False Positive Rate: 0.0924\n",
      "Avg Inference Time per Sample: 0.13 ms\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, dataloaders[\"test\"], device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9d6066a2-cc47-4930-8b65-bf6832e509fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvTransformerClassifier(\n",
       "  (cnn): Sequential(\n",
       "    (0): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (14): ReLU()\n",
       "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (flatten): Flatten(start_dim=2, end_dim=-1)\n",
       "  (positional_encoding): PositionalEncoding()\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "  (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model weights\n",
    "torch.save(model.state_dict(), 'model_weights.pt')\n",
    "\n",
    "# Load model weights (later)\n",
    "model = ConvTransformerClassifier(n_classes=2, d_model=128, nhead=4, num_layers=2)\n",
    "model.load_state_dict(torch.load('model_weights.pt'))\n",
    "model.eval()  # Set to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "765a5b0a-ff3e-47fc-ab9c-70a4f3b046a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your search space\n",
    "param_grid = {\n",
    "    \"lr\": [1e-4, 5e-4, 1e-3],\n",
    "    \"weight_decay\": [0.0, 1e-5],\n",
    "    \"clip_grad\": [True, False],\n",
    "    \"patience\": [3, 5],\n",
    "    \"epochs\": [10, 15, 20]  # Can be tuned or fixed\n",
    "}\n",
    "\n",
    "search_space = list(product(*param_grid.values()))\n",
    "random.shuffle(search_space)\n",
    "\n",
    "# Tracking best result\n",
    "best_val_acc = float(\"-inf\")\n",
    "best_params = None\n",
    "best_model_path = \"best_model.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8553bdd6-a4d1-45d7-83c0-a80244d7124d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Trial 1/10 with params: {'lr': 0.0005, 'weight_decay': 1e-05, 'clip_grad': True, 'patience': 5, 'epochs': 10}\n",
      "\n",
      "Epoch 1/10\n",
      "Train Loss: 0.3016 | Accuracy: 0.8922\n",
      "Val   Loss: 0.2605 | Accuracy: 0.9060\n",
      "\n",
      "Epoch 2/10\n",
      "Train Loss: 0.1828 | Accuracy: 0.9363\n",
      "Val   Loss: 0.2012 | Accuracy: 0.9306\n",
      "\n",
      "Epoch 3/10\n",
      "Train Loss: 0.1339 | Accuracy: 0.9507\n",
      "Val   Loss: 0.1246 | Accuracy: 0.9577\n",
      "\n",
      "Epoch 4/10\n",
      "Train Loss: 0.1102 | Accuracy: 0.9605\n",
      "Val   Loss: 0.2039 | Accuracy: 0.9396\n",
      "\n",
      "Epoch 5/10\n",
      "Train Loss: 0.0957 | Accuracy: 0.9681\n",
      "Val   Loss: 0.3255 | Accuracy: 0.9076\n",
      "\n",
      "Epoch 6/10\n",
      "Train Loss: 0.0815 | Accuracy: 0.9718\n",
      "Val   Loss: 0.0948 | Accuracy: 0.9684\n",
      "\n",
      "Epoch 7/10\n",
      "Train Loss: 0.0693 | Accuracy: 0.9770\n",
      "Val   Loss: 1.1867 | Accuracy: 0.8241\n",
      "\n",
      "Epoch 8/10\n",
      "Train Loss: 0.0612 | Accuracy: 0.9794\n",
      "Val   Loss: 0.1040 | Accuracy: 0.9684\n",
      "\n",
      "Epoch 9/10\n",
      "Train Loss: 0.0560 | Accuracy: 0.9814\n",
      "Val   Loss: 0.1274 | Accuracy: 0.9661\n",
      "\n",
      "Epoch 10/10\n",
      "Train Loss: 0.0307 | Accuracy: 0.9908\n",
      "Val   Loss: 0.2202 | Accuracy: 0.9509\n",
      "Best validation accuracy: 0.9684 at epoch 6\n",
      "Validation Accuracy: 0.9684\n",
      "âœ… New best model saved!\n",
      "\n",
      "ðŸ” Trial 2/10 with params: {'lr': 0.001, 'weight_decay': 1e-05, 'clip_grad': True, 'patience': 5, 'epochs': 10}\n",
      "\n",
      "Epoch 1/10\n",
      "Train Loss: 0.2788 | Accuracy: 0.8906\n",
      "Val   Loss: 0.4240 | Accuracy: 0.8438\n",
      "\n",
      "Epoch 2/10\n",
      "Train Loss: 0.1804 | Accuracy: 0.9327\n",
      "Val   Loss: 0.1664 | Accuracy: 0.9425\n",
      "\n",
      "Epoch 3/10\n",
      "Train Loss: 0.1509 | Accuracy: 0.9432\n",
      "Val   Loss: 0.1460 | Accuracy: 0.9524\n",
      "\n",
      "Epoch 4/10\n",
      "Train Loss: 0.1284 | Accuracy: 0.9527\n",
      "Val   Loss: 0.1434 | Accuracy: 0.9483\n",
      "\n",
      "Epoch 5/10\n",
      "Train Loss: 0.1162 | Accuracy: 0.9568\n",
      "Val   Loss: 0.1430 | Accuracy: 0.9491\n",
      "\n",
      "Epoch 6/10\n",
      "Train Loss: 0.0997 | Accuracy: 0.9638\n",
      "Val   Loss: 0.1217 | Accuracy: 0.9591\n",
      "\n",
      "Epoch 7/10\n",
      "Train Loss: 0.0824 | Accuracy: 0.9708\n",
      "Val   Loss: 1.1581 | Accuracy: 0.7953\n",
      "\n",
      "Epoch 8/10\n",
      "Train Loss: 0.0768 | Accuracy: 0.9733\n",
      "Val   Loss: 0.4980 | Accuracy: 0.8524\n",
      "\n",
      "Epoch 9/10\n",
      "Train Loss: 0.0666 | Accuracy: 0.9771\n",
      "Val   Loss: 0.0909 | Accuracy: 0.9711\n",
      "\n",
      "Epoch 10/10\n",
      "Train Loss: 0.0608 | Accuracy: 0.9785\n",
      "Val   Loss: 0.1210 | Accuracy: 0.9581\n",
      "Best validation accuracy: 0.9711 at epoch 9\n",
      "Validation Accuracy: 0.9711\n",
      "âœ… New best model saved!\n",
      "\n",
      "ðŸ” Trial 3/10 with params: {'lr': 0.0001, 'weight_decay': 1e-05, 'clip_grad': True, 'patience': 5, 'epochs': 10}\n",
      "\n",
      "Epoch 1/10\n",
      "Train Loss: 0.2886 | Accuracy: 0.8972\n",
      "Val   Loss: 0.2723 | Accuracy: 0.8969\n",
      "\n",
      "Epoch 2/10\n",
      "Train Loss: 0.1853 | Accuracy: 0.9389\n",
      "Val   Loss: 0.2339 | Accuracy: 0.9179\n",
      "\n",
      "Epoch 3/10\n",
      "Train Loss: 0.1434 | Accuracy: 0.9524\n",
      "Val   Loss: 0.2184 | Accuracy: 0.9353\n",
      "\n",
      "Epoch 4/10\n",
      "Train Loss: 0.1219 | Accuracy: 0.9615\n",
      "Val   Loss: 0.1497 | Accuracy: 0.9515\n",
      "\n",
      "Epoch 5/10\n",
      "Train Loss: 0.1072 | Accuracy: 0.9661\n",
      "Val   Loss: 0.1377 | Accuracy: 0.9596\n",
      "\n",
      "Epoch 6/10\n",
      "Train Loss: 0.0895 | Accuracy: 0.9740\n",
      "Val   Loss: 0.1277 | Accuracy: 0.9657\n",
      "\n",
      "Epoch 7/10\n",
      "Train Loss: 0.0724 | Accuracy: 0.9778\n",
      "Val   Loss: 0.2745 | Accuracy: 0.9329\n",
      "\n",
      "Epoch 8/10\n",
      "Train Loss: 0.0631 | Accuracy: 0.9814\n",
      "Val   Loss: 0.1765 | Accuracy: 0.9684\n",
      "\n",
      "Epoch 9/10\n",
      "Train Loss: 0.0562 | Accuracy: 0.9854\n",
      "Val   Loss: 0.1326 | Accuracy: 0.9754\n",
      "\n",
      "Epoch 10/10\n",
      "Train Loss: 0.0255 | Accuracy: 0.9936\n",
      "Val   Loss: 0.2577 | Accuracy: 0.9552\n",
      "Best validation accuracy: 0.9754 at epoch 9\n",
      "Validation Accuracy: 0.9754\n",
      "âœ… New best model saved!\n",
      "\n",
      "ðŸ” Trial 4/10 with params: {'lr': 0.0001, 'weight_decay': 0.0, 'clip_grad': True, 'patience': 5, 'epochs': 20}\n",
      "\n",
      "Epoch 1/20\n",
      "Train Loss: 0.2886 | Accuracy: 0.8972\n",
      "Val   Loss: 0.2723 | Accuracy: 0.8969\n",
      "\n",
      "Epoch 2/20\n",
      "Train Loss: 0.1860 | Accuracy: 0.9397\n",
      "Val   Loss: 0.3000 | Accuracy: 0.8949\n",
      "\n",
      "Epoch 3/20\n",
      "Train Loss: 0.1442 | Accuracy: 0.9532\n",
      "Val   Loss: 0.1339 | Accuracy: 0.9575\n",
      "\n",
      "Epoch 4/20\n",
      "Train Loss: 0.1265 | Accuracy: 0.9603\n",
      "Val   Loss: 0.1261 | Accuracy: 0.9591\n",
      "\n",
      "Epoch 5/20\n",
      "Train Loss: 0.1054 | Accuracy: 0.9671\n",
      "Val   Loss: 0.1416 | Accuracy: 0.9550\n",
      "\n",
      "Epoch 6/20\n",
      "Train Loss: 0.0876 | Accuracy: 0.9748\n",
      "Val   Loss: 0.1130 | Accuracy: 0.9694\n",
      "\n",
      "Epoch 7/20\n",
      "Train Loss: 0.0730 | Accuracy: 0.9793\n",
      "Val   Loss: 0.3915 | Accuracy: 0.9136\n",
      "\n",
      "Epoch 8/20\n",
      "Train Loss: 0.0624 | Accuracy: 0.9817\n",
      "Val   Loss: 0.1937 | Accuracy: 0.9655\n",
      "\n",
      "Epoch 9/20\n",
      "Train Loss: 0.0641 | Accuracy: 0.9837\n",
      "Val   Loss: 0.2305 | Accuracy: 0.9591\n",
      "\n",
      "Epoch 10/20\n",
      "Train Loss: 0.0264 | Accuracy: 0.9933\n",
      "Val   Loss: 0.1787 | Accuracy: 0.9696\n",
      "\n",
      "Epoch 11/20\n",
      "Train Loss: 0.0130 | Accuracy: 0.9969\n",
      "Val   Loss: 0.1893 | Accuracy: 0.9735\n",
      "\n",
      "Epoch 12/20\n",
      "Train Loss: 0.0132 | Accuracy: 0.9964\n",
      "Val   Loss: 0.2630 | Accuracy: 0.9647\n",
      "\n",
      "Epoch 13/20\n",
      "Train Loss: 0.0060 | Accuracy: 0.9985\n",
      "Val   Loss: 0.1938 | Accuracy: 0.9747\n",
      "\n",
      "Epoch 14/20\n",
      "Train Loss: 0.0040 | Accuracy: 0.9990\n",
      "Val   Loss: 0.2089 | Accuracy: 0.9735\n",
      "\n",
      "Epoch 15/20\n",
      "Train Loss: 0.0049 | Accuracy: 0.9989\n",
      "Val   Loss: 0.2375 | Accuracy: 0.9717\n",
      "\n",
      "Epoch 16/20\n",
      "Train Loss: 0.0025 | Accuracy: 0.9993\n",
      "Val   Loss: 0.2440 | Accuracy: 0.9731\n",
      "\n",
      "Epoch 17/20\n",
      "Train Loss: 0.0018 | Accuracy: 0.9995\n",
      "Val   Loss: 0.2424 | Accuracy: 0.9725\n",
      "\n",
      "Epoch 18/20\n",
      "Train Loss: 0.0012 | Accuracy: 0.9997\n",
      "Val   Loss: 0.2453 | Accuracy: 0.9723\n",
      "Early stopping triggered.\n",
      "Best validation accuracy: 0.9747 at epoch 13\n",
      "Validation Accuracy: 0.9747\n",
      "\n",
      "ðŸ” Trial 5/10 with params: {'lr': 0.001, 'weight_decay': 0.0, 'clip_grad': True, 'patience': 3, 'epochs': 15}\n",
      "\n",
      "Epoch 1/15\n",
      "Train Loss: 0.2876 | Accuracy: 0.8875\n",
      "Val   Loss: 0.5216 | Accuracy: 0.8306\n",
      "\n",
      "Epoch 2/15\n",
      "Train Loss: 0.1788 | Accuracy: 0.9340\n",
      "Val   Loss: 0.3929 | Accuracy: 0.8762\n",
      "\n",
      "Epoch 3/15\n",
      "Train Loss: 0.1468 | Accuracy: 0.9451\n",
      "Val   Loss: 0.1675 | Accuracy: 0.9384\n",
      "\n",
      "Epoch 4/15\n",
      "Train Loss: 0.1269 | Accuracy: 0.9542\n",
      "Val   Loss: 0.1168 | Accuracy: 0.9594\n",
      "\n",
      "Epoch 5/15\n",
      "Train Loss: 0.1130 | Accuracy: 0.9580\n",
      "Val   Loss: 0.1155 | Accuracy: 0.9610\n",
      "\n",
      "Epoch 6/15\n",
      "Train Loss: 0.0960 | Accuracy: 0.9666\n",
      "Val   Loss: 0.1222 | Accuracy: 0.9569\n",
      "\n",
      "Epoch 7/15\n",
      "Train Loss: 0.0872 | Accuracy: 0.9676\n",
      "Val   Loss: 0.2943 | Accuracy: 0.9136\n",
      "\n",
      "Epoch 8/15\n",
      "Train Loss: 0.0746 | Accuracy: 0.9732\n",
      "Val   Loss: 0.5632 | Accuracy: 0.8204\n",
      "Early stopping triggered.\n",
      "Best validation accuracy: 0.9610 at epoch 5\n",
      "Validation Accuracy: 0.9610\n",
      "\n",
      "ðŸ” Trial 6/10 with params: {'lr': 0.001, 'weight_decay': 0.0, 'clip_grad': True, 'patience': 5, 'epochs': 10}\n",
      "\n",
      "Epoch 1/10\n",
      "Train Loss: 0.2948 | Accuracy: 0.8834\n",
      "Val   Loss: 0.2746 | Accuracy: 0.8914\n",
      "\n",
      "Epoch 2/10\n",
      "Train Loss: 0.1832 | Accuracy: 0.9334\n",
      "Val   Loss: 0.6971 | Accuracy: 0.8087\n",
      "\n",
      "Epoch 3/10\n",
      "Train Loss: 0.1524 | Accuracy: 0.9423\n",
      "Val   Loss: 0.1806 | Accuracy: 0.9353\n",
      "\n",
      "Epoch 4/10\n",
      "Train Loss: 0.1300 | Accuracy: 0.9515\n",
      "Val   Loss: 0.1104 | Accuracy: 0.9614\n",
      "\n",
      "Epoch 5/10\n",
      "Train Loss: 0.1150 | Accuracy: 0.9600\n",
      "Val   Loss: 0.2656 | Accuracy: 0.9132\n",
      "\n",
      "Epoch 6/10\n",
      "Train Loss: 0.1008 | Accuracy: 0.9635\n",
      "Val   Loss: 0.3042 | Accuracy: 0.9091\n",
      "\n",
      "Epoch 7/10\n",
      "Train Loss: 0.0890 | Accuracy: 0.9686\n",
      "Val   Loss: 0.1902 | Accuracy: 0.9427\n",
      "\n",
      "Epoch 8/10\n",
      "Train Loss: 0.0550 | Accuracy: 0.9800\n",
      "Val   Loss: 0.1626 | Accuracy: 0.9483\n",
      "\n",
      "Epoch 9/10\n",
      "Train Loss: 0.0434 | Accuracy: 0.9858\n",
      "Val   Loss: 0.1324 | Accuracy: 0.9645\n",
      "\n",
      "Epoch 10/10\n",
      "Train Loss: 0.0347 | Accuracy: 0.9899\n",
      "Val   Loss: 0.2849 | Accuracy: 0.9380\n",
      "Best validation accuracy: 0.9645 at epoch 9\n",
      "Validation Accuracy: 0.9645\n",
      "\n",
      "ðŸ” Trial 7/10 with params: {'lr': 0.0001, 'weight_decay': 0.0, 'clip_grad': False, 'patience': 5, 'epochs': 15}\n",
      "\n",
      "Epoch 1/15\n",
      "Train Loss: 0.3003 | Accuracy: 0.8901\n",
      "Val   Loss: 0.2504 | Accuracy: 0.9010\n",
      "\n",
      "Epoch 2/15\n",
      "Train Loss: 0.1857 | Accuracy: 0.9358\n",
      "Val   Loss: 0.1377 | Accuracy: 0.9495\n",
      "\n",
      "Epoch 3/15\n",
      "Train Loss: 0.1383 | Accuracy: 0.9516\n",
      "Val   Loss: 0.2640 | Accuracy: 0.9183\n",
      "\n",
      "Epoch 4/15\n",
      "Train Loss: 0.1196 | Accuracy: 0.9587\n",
      "Val   Loss: 0.2448 | Accuracy: 0.9187\n",
      "\n",
      "Epoch 5/15\n",
      "Train Loss: 0.1094 | Accuracy: 0.9612\n",
      "Val   Loss: 0.1186 | Accuracy: 0.9581\n",
      "\n",
      "Epoch 6/15\n",
      "Train Loss: 0.0916 | Accuracy: 0.9685\n",
      "Val   Loss: 0.2564 | Accuracy: 0.9152\n",
      "\n",
      "Epoch 7/15\n",
      "Train Loss: 0.0765 | Accuracy: 0.9738\n",
      "Val   Loss: 0.1817 | Accuracy: 0.9460\n",
      "\n",
      "Epoch 8/15\n",
      "Train Loss: 0.0667 | Accuracy: 0.9772\n",
      "Val   Loss: 0.1108 | Accuracy: 0.9663\n",
      "\n",
      "Epoch 9/15\n",
      "Train Loss: 0.0633 | Accuracy: 0.9789\n",
      "Val   Loss: 0.1609 | Accuracy: 0.9532\n",
      "\n",
      "Epoch 10/15\n",
      "Train Loss: 0.0529 | Accuracy: 0.9826\n",
      "Val   Loss: 0.2220 | Accuracy: 0.9333\n",
      "\n",
      "Epoch 11/15\n",
      "Train Loss: 0.0429 | Accuracy: 0.9857\n",
      "Val   Loss: 0.1279 | Accuracy: 0.9669\n",
      "\n",
      "Epoch 12/15\n",
      "Train Loss: 0.0206 | Accuracy: 0.9932\n",
      "Val   Loss: 0.1947 | Accuracy: 0.9532\n",
      "\n",
      "Epoch 13/15\n",
      "Train Loss: 0.0150 | Accuracy: 0.9954\n",
      "Val   Loss: 0.1292 | Accuracy: 0.9686\n",
      "\n",
      "Epoch 14/15\n",
      "Train Loss: 0.0143 | Accuracy: 0.9953\n",
      "Val   Loss: 0.1213 | Accuracy: 0.9688\n",
      "\n",
      "Epoch 15/15\n",
      "Train Loss: 0.0062 | Accuracy: 0.9983\n",
      "Val   Loss: 0.1119 | Accuracy: 0.9729\n",
      "Best validation accuracy: 0.9729 at epoch 15\n",
      "Validation Accuracy: 0.9729\n",
      "\n",
      "ðŸ” Trial 8/10 with params: {'lr': 0.001, 'weight_decay': 1e-05, 'clip_grad': False, 'patience': 5, 'epochs': 10}\n",
      "\n",
      "Epoch 1/10\n",
      "Train Loss: 0.2960 | Accuracy: 0.8822\n",
      "Val   Loss: 0.3958 | Accuracy: 0.8485\n",
      "\n",
      "Epoch 2/10\n",
      "Train Loss: 0.1798 | Accuracy: 0.9333\n",
      "Val   Loss: 0.8989 | Accuracy: 0.7988\n",
      "\n",
      "Epoch 3/10\n",
      "Train Loss: 0.1512 | Accuracy: 0.9433\n",
      "Val   Loss: 0.1577 | Accuracy: 0.9485\n",
      "\n",
      "Epoch 4/10\n",
      "Train Loss: 0.1298 | Accuracy: 0.9521\n",
      "Val   Loss: 0.7621 | Accuracy: 0.8029\n",
      "\n",
      "Epoch 5/10\n",
      "Train Loss: 0.1173 | Accuracy: 0.9570\n",
      "Val   Loss: 0.2239 | Accuracy: 0.9339\n",
      "\n",
      "Epoch 6/10\n",
      "Train Loss: 0.1034 | Accuracy: 0.9614\n",
      "Val   Loss: 0.5424 | Accuracy: 0.8530\n",
      "\n",
      "Epoch 7/10\n",
      "Train Loss: 0.0657 | Accuracy: 0.9755\n",
      "Val   Loss: 0.1165 | Accuracy: 0.9663\n",
      "\n",
      "Epoch 8/10\n",
      "Train Loss: 0.0538 | Accuracy: 0.9816\n",
      "Val   Loss: 0.1419 | Accuracy: 0.9540\n",
      "\n",
      "Epoch 9/10\n",
      "Train Loss: 0.0461 | Accuracy: 0.9837\n",
      "Val   Loss: 0.1078 | Accuracy: 0.9669\n",
      "\n",
      "Epoch 10/10\n",
      "Train Loss: 0.0407 | Accuracy: 0.9873\n",
      "Val   Loss: 0.2846 | Accuracy: 0.9132\n",
      "Best validation accuracy: 0.9669 at epoch 9\n",
      "Validation Accuracy: 0.9669\n",
      "\n",
      "ðŸ” Trial 9/10 with params: {'lr': 0.0005, 'weight_decay': 1e-05, 'clip_grad': True, 'patience': 3, 'epochs': 10}\n",
      "\n",
      "Epoch 1/10\n",
      "Train Loss: 0.3021 | Accuracy: 0.8881\n",
      "Val   Loss: 0.2062 | Accuracy: 0.9187\n",
      "\n",
      "Epoch 2/10\n",
      "Train Loss: 0.1748 | Accuracy: 0.9372\n",
      "Val   Loss: 0.3731 | Accuracy: 0.8943\n",
      "\n",
      "Epoch 3/10\n",
      "Train Loss: 0.1398 | Accuracy: 0.9496\n",
      "Val   Loss: 0.1241 | Accuracy: 0.9567\n",
      "\n",
      "Epoch 4/10\n",
      "Train Loss: 0.1169 | Accuracy: 0.9581\n",
      "Val   Loss: 0.1277 | Accuracy: 0.9565\n",
      "\n",
      "Epoch 5/10\n",
      "Train Loss: 0.1028 | Accuracy: 0.9639\n",
      "Val   Loss: 0.2756 | Accuracy: 0.9158\n",
      "\n",
      "Epoch 6/10\n",
      "Train Loss: 0.0868 | Accuracy: 0.9705\n",
      "Val   Loss: 0.3474 | Accuracy: 0.9166\n",
      "Early stopping triggered.\n",
      "Best validation accuracy: 0.9567 at epoch 3\n",
      "Validation Accuracy: 0.9567\n",
      "\n",
      "ðŸ” Trial 10/10 with params: {'lr': 0.0005, 'weight_decay': 1e-05, 'clip_grad': True, 'patience': 5, 'epochs': 15}\n",
      "\n",
      "Epoch 1/15\n",
      "Train Loss: 0.2943 | Accuracy: 0.8915\n",
      "Val   Loss: 0.2370 | Accuracy: 0.9029\n",
      "\n",
      "Epoch 2/15\n",
      "Train Loss: 0.1707 | Accuracy: 0.9383\n",
      "Val   Loss: 0.5804 | Accuracy: 0.8237\n",
      "\n",
      "Epoch 3/15\n",
      "Train Loss: 0.1306 | Accuracy: 0.9530\n",
      "Val   Loss: 0.1755 | Accuracy: 0.9532\n",
      "\n",
      "Epoch 4/15\n",
      "Train Loss: 0.1162 | Accuracy: 0.9590\n",
      "Val   Loss: 0.2161 | Accuracy: 0.9333\n",
      "\n",
      "Epoch 5/15\n",
      "Train Loss: 0.1028 | Accuracy: 0.9634\n",
      "Val   Loss: 0.3057 | Accuracy: 0.9080\n",
      "\n",
      "Epoch 6/15\n",
      "Train Loss: 0.0873 | Accuracy: 0.9704\n",
      "Val   Loss: 0.3377 | Accuracy: 0.9019\n",
      "\n",
      "Epoch 7/15\n",
      "Train Loss: 0.0494 | Accuracy: 0.9843\n",
      "Val   Loss: 0.3086 | Accuracy: 0.9300\n",
      "\n",
      "Epoch 8/15\n",
      "Train Loss: 0.0397 | Accuracy: 0.9872\n",
      "Val   Loss: 0.1512 | Accuracy: 0.9661\n",
      "\n",
      "Epoch 9/15\n",
      "Train Loss: 0.0329 | Accuracy: 0.9898\n",
      "Val   Loss: 0.1268 | Accuracy: 0.9696\n",
      "\n",
      "Epoch 10/15\n",
      "Train Loss: 0.0319 | Accuracy: 0.9914\n",
      "Val   Loss: 0.1927 | Accuracy: 0.9604\n",
      "\n",
      "Epoch 11/15\n",
      "Train Loss: 0.0252 | Accuracy: 0.9927\n",
      "Val   Loss: 0.1968 | Accuracy: 0.9700\n",
      "\n",
      "Epoch 12/15\n",
      "Train Loss: 0.0210 | Accuracy: 0.9941\n",
      "Val   Loss: 0.1843 | Accuracy: 0.9667\n",
      "\n",
      "Epoch 13/15\n",
      "Train Loss: 0.0089 | Accuracy: 0.9978\n",
      "Val   Loss: 0.1735 | Accuracy: 0.9735\n",
      "\n",
      "Epoch 14/15\n",
      "Train Loss: 0.0074 | Accuracy: 0.9982\n",
      "Val   Loss: 0.1890 | Accuracy: 0.9758\n",
      "\n",
      "Epoch 15/15\n",
      "Train Loss: 0.0076 | Accuracy: 0.9984\n",
      "Val   Loss: 0.1738 | Accuracy: 0.9752\n",
      "Best validation accuracy: 0.9758 at epoch 14\n",
      "Validation Accuracy: 0.9758\n",
      "âœ… New best model saved!\n",
      "\n",
      "ðŸ† Best Hyperparameters:\n",
      "{'lr': 0.0005, 'weight_decay': 1e-05, 'clip_grad': True, 'patience': 5, 'epochs': 15}\n",
      "Best Validation Accuracy: 0.9758\n",
      "Model saved to: best_model.pt\n"
     ]
    }
   ],
   "source": [
    "# Start search\n",
    "max_trials = min(10, len(search_space))\n",
    "for i, values in enumerate(search_space[:max_trials]):\n",
    "    params = dict(zip(param_grid.keys(), values))\n",
    "    print(f\"\\nðŸ” Trial {i + 1}/{max_trials} with params: {params}\")\n",
    "\n",
    "    model = ConvTransformerClassifier(n_classes=2, d_model=128, nhead=4, num_layers=2) \n",
    "    trained_model, val_acc = train_model(model, dataloaders, device=\"cuda\", **params)\n",
    "\n",
    "    print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_params = params\n",
    "\n",
    "        # Save the best model\n",
    "        torch.save(trained_model.state_dict(), best_model_path)\n",
    "        print(\"âœ… New best model saved!\")\n",
    "\n",
    "print(\"\\nðŸ† Best Hyperparameters:\")\n",
    "print(best_params)\n",
    "print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")\n",
    "print(f\"Model saved to: {best_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4a597d5-fbb7-4312-9f26-d3c7af31451d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m model.load_state_dict(torch.load(\u001b[33m'\u001b[39m\u001b[33mbest_model.pt\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m      3\u001b[39m model.eval()  \u001b[38;5;66;03m# Set to evaluation mode\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mevaluate_model\u001b[39m\u001b[34m(model, dataloader, device)\u001b[39m\n\u001b[32m     10\u001b[39m X, y = X.to(device), y.to(device)\n\u001b[32m     12\u001b[39m start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m end_time = time.time()\n\u001b[32m     16\u001b[39m inference_time = end_time - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/sg2121/fyp/aimusicdetector/venvNew/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/sg2121/fyp/aimusicdetector/venvNew/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mConvTransformerClassifier.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     33\u001b[39m     \u001b[38;5;66;03m# x: (B, 1, 128, 256)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B, C, H', W') => e.g. (B, 128, 8, 16)\u001b[39;00m\n\u001b[32m     35\u001b[39m     B, C, H, W = x.shape\n\u001b[32m     37\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.flatten(x)            \u001b[38;5;66;03m# (B, C, H*W)  â†’ sequence\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/sg2121/fyp/aimusicdetector/venvNew/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/sg2121/fyp/aimusicdetector/venvNew/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/sg2121/fyp/aimusicdetector/venvNew/lib/python3.12/site-packages/torch/nn/modules/container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/sg2121/fyp/aimusicdetector/venvNew/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/sg2121/fyp/aimusicdetector/venvNew/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/sg2121/fyp/aimusicdetector/venvNew/lib/python3.12/site-packages/torch/nn/modules/conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/sg2121/fyp/aimusicdetector/venvNew/lib/python3.12/site-packages/torch/nn/modules/conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "model = ConvTransformerClassifier(n_classes=2, d_model=128, nhead=4, num_layers=2)\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "model.eval()  # Set to evaluation mode\n",
    "evaluate_model(model, dataloaders[\"test\"], device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb6e3c5-9c6f-4cd3-a3e3-f73536c20e34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
