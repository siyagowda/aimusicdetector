{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ec86650-2c1c-4a81-9ddf-081016942431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, get_scheduler\n",
    "\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4bd8f69-b85b-4c57-9fd4-a526c12168ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained DistilBERT tokenizer and model for binary classification\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5eb4839-30a0-4a47-b616-1417a1bde112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a3b280f-4875-42ad-b05e-e5cd0544bfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 674\n",
      "Validation set size: 84\n",
      "Test set size: 85\n"
     ]
    }
   ],
   "source": [
    "# Directory paths for the lyrics\n",
    "ai_lyrics_path = \"/data/sg2121/fypdataset/dataset/lyrics/ai\"\n",
    "human_lyrics_path = \"/data/sg2121/fypdataset/dataset/lyrics/human\"\n",
    "\n",
    "# Read all file paths\n",
    "ai_files = [os.path.join(ai_lyrics_path, f) for f in os.listdir(ai_lyrics_path) if f.endswith('.txt')]\n",
    "human_files = [os.path.join(human_lyrics_path, f) for f in os.listdir(human_lyrics_path) if f.endswith('.txt')]\n",
    "\n",
    "# Combine all files into a single list with labels (0 for AI, 1 for Human)\n",
    "all_files = [(file, 0) for file in ai_files] + [(file, 1) for file in human_files]\n",
    "\n",
    "# Split into train and temp (temp is for validation and test)\n",
    "train_files, temp_files = train_test_split(all_files, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split temp into validation and test sets\n",
    "val_files, test_files = train_test_split(temp_files, test_size=0.5, random_state=42)\n",
    "\n",
    "# Example of how you might check the splits\n",
    "print(f\"Training set size: {len(train_files)}\")\n",
    "print(f\"Validation set size: {len(val_files)}\")\n",
    "print(f\"Test set size: {len(test_files)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b4b5266b-99d9-4ac7-86dd-7dfaacc3b7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LyricsDataset(Dataset):\n",
    "    def __init__(self, file_paths, tokenizer, max_length=512):\n",
    "        self.file_paths = file_paths\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path, label = self.file_paths[idx]\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': torch.tensor(label, dtype=torch.long),\n",
    "            'filename': os.path.basename(file_path)  # This is important\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6245a977-8c91-4e6b-a07c-661ece9cf1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input IDs: tensor([  101, 15333,  2033, 16220,  6906,  2072, 29316,  1037,  5673,  4372,\n",
      "         8774,  4630,  4372,  7151, 10207, 10364,  2139,  4606,  5285,  4630,\n",
      "        10861,  4649, 10364, 18033,  2121,  2474, 29086,  2474, 29086,  1010,\n",
      "         2474, 29086,  1010,  2474, 29086,  1039,  1005,  9765, 25353,  8737,\n",
      "         2050,  1010,  2474, 29086,  1010,  2474, 29086, 10613,  3540, 19445,\n",
      "         1010,  2474, 29086,  1010,  2474, 29086,  1010,  2474, 29086,  1039,\n",
      "         1005,  9765, 25353,  8737,  2050,  1010,  2474, 29086,  1010,  2474,\n",
      "        29086, 10613,  3540, 19445,  1010, 15317, 22825,  1037,  2033,  4189,\n",
      "         2063,  9610,  2121,  1010,  5003,  9004, 11022,  2139,  8872,  3170,\n",
      "         1037,  5003,  8915,  6593,  2063,  2143,  2139, 12731,  2140, 15317,\n",
      "        25636,  2063, 27830,  4886,  3672, 24209,  1005,  1037,  2474,  8632,\n",
      "         3802,  7367,  2015,  2061, 21823,  6132, 14980,  3372,  4372, 21418,\n",
      "        14774,  1010, 25175,  6335,  6904,  4904, 14674, 10125,  2099,  3802,\n",
      "         1039,  1005,  9765, 14674,  4372, 15723, 12662,  2063,  2139,  1012,\n",
      "         1012,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0])\n",
      "Sample attention mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Sample label: 1\n"
     ]
    }
   ],
   "source": [
    "# Create datasets for training, validation, and testing\n",
    "train_dataset = LyricsDataset(train_files, tokenizer)\n",
    "val_dataset = LyricsDataset(val_files, tokenizer)\n",
    "test_dataset = LyricsDataset(test_files, tokenizer)\n",
    "\n",
    "# Create DataLoader for each dataset\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "# Print a sample to check the dataset\n",
    "sample = train_dataset[0]\n",
    "print(f\"Sample input IDs: {sample['input_ids']}\")\n",
    "print(f\"Sample attention mask: {sample['attention_mask']}\")\n",
    "print(f\"Sample label: {sample['labels']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "339c49dd-e77d-40e8-90a4-c336aa0da749",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 85/85 [00:12<00:00,  7.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Loss: 0.0036571966484189034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 11/11 [00:00<00:00, 22.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.8289, Accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 85/85 [00:12<00:00,  6.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 completed. Loss: 0.003831487614661455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 11/11 [00:00<00:00, 22.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.9162, Accuracy: 0.7381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 85/85 [00:12<00:00,  6.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 completed. Loss: 0.002079684752970934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 11/11 [00:00<00:00, 22.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.9843, Accuracy: 0.7619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set up the optimizer and training loop\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",                     # \"linear\", \"cosine\", \"cosine_with_restarts\", etc.\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,               # Optional: small warmup period to avoid large initial updates\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):  # Number of epochs\n",
    "    model.train()\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        # Ensure batch is a dictionary of tensors\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update learning rate\n",
    "        lr_scheduler.step() \n",
    "    \n",
    "    print(f\"Epoch {epoch + 1} completed. Loss: {loss.item()}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dataloader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            val_loss += outputs.loss.item()\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            correct_predictions += (preds == labels).sum().item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    accuracy = correct_predictions / len(val_files)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}, Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9fba58e6-d6c1-4528-ac40-141ae5388246",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 11/11 [00:00<00:00, 19.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "results = []\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "for batch in tqdm(test_dataloader):\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    filenames = batch['filename']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        probs = F.softmax(outputs.logits, dim=1)\n",
    "        preds = torch.argmax(probs, dim=1)  # Predicted label\n",
    "\n",
    "        # Calculate the number of correct predictions\n",
    "        correct_predictions += (preds == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "        # Save results\n",
    "        for i in range(len(filenames)):\n",
    "            results.append({\n",
    "                \"filename\": filenames[i],\n",
    "                \"prob_ai\": probs[i][0].item(),\n",
    "                \"prob_human\": probs[i][1].item(),\n",
    "                \"true_label\": labels[i].item(),\n",
    "                \"pred_label\": preds[i].item()\n",
    "            })\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"lyrics_test_predictions.csv\", index=False)\n",
    "\n",
    "# Calculate and print accuracy\n",
    "accuracy = correct_predictions / total_predictions\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345db706-88da-4d89-8f60-0973d59ad8b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
