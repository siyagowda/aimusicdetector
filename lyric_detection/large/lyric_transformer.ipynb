{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fde6f51-23eb-4cd9-bd5e-a1399b7d03d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEELO\n"
     ]
    }
   ],
   "source": [
    "print(\"HEELO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ec86650-2c1c-4a81-9ddf-081016942431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, get_scheduler\n",
    "\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4bd8f69-b85b-4c57-9fd4-a526c12168ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained DistilBERT tokenizer and model for binary classification\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def build_model():\n",
    "    return DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5eb4839-30a0-4a47-b616-1417a1bde112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72a06458-c78c-4273-8a84-f0d55415187d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 14157\n",
      "Validation set size: 2958\n",
      "Test set size: 2967\n"
     ]
    }
   ],
   "source": [
    "# Directory paths for the segments and lyrics\n",
    "ai_segments_path = \"/vol/bitbucket/sg2121/fypdataset/dataset_large2/normal_data/ai_segments\"\n",
    "human_segments_path = \"/vol/bitbucket/sg2121/fypdataset/dataset_large2/normal_data/human\"\n",
    "ai_lyrics_path = \"/vol/bitbucket/sg2121/fypdataset/dataset_large2/lyrics/ai_clean\"\n",
    "human_lyrics_path = \"/vol/bitbucket/sg2121/fypdataset/dataset_large2/lyrics/human_clean\"\n",
    "\n",
    "# Helper function to read file paths from a text file\n",
    "def read_file_paths(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        return [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Read all file paths from the text files\n",
    "train_files = read_file_paths('/vol/bitbucket/sg2121/fyp/aimusicdetector/train_test_split/bitbucket/train_files_large.txt')\n",
    "val_files = read_file_paths('/vol/bitbucket/sg2121/fyp/aimusicdetector/train_test_split/bitbucket/val_files_large.txt')\n",
    "test_files = read_file_paths('/vol/bitbucket/sg2121/fyp/aimusicdetector/train_test_split/bitbucket/test_files_large.txt')\n",
    "\n",
    "# Function to convert segment file path to lyric file path\n",
    "def convert_to_lyric_path(file_path, is_ai):\n",
    "    if is_ai:\n",
    "        if file_path.startswith(ai_segments_path):\n",
    "            base_lyrics_path = ai_lyrics_path\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        if file_path.startswith(human_segments_path):\n",
    "            base_lyrics_path = human_lyrics_path\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # Construct full lyric file path\n",
    "    file_name = os.path.basename(file_path).replace('.mp3', '_lyrics.txt')\n",
    "    lyric_path = os.path.join(base_lyrics_path, file_name)\n",
    "\n",
    "    # Now check if the path exists\n",
    "    if not os.path.exists(lyric_path):\n",
    "        return None\n",
    "\n",
    "    return lyric_path\n",
    "\n",
    "\n",
    "\n",
    "# Process the file lists and create tuples of (lyric_path, label)\n",
    "def process_file_paths(file_paths, is_ai):\n",
    "    return [\n",
    "        (lyric_path, 0 if is_ai else 1)\n",
    "        for file_path in file_paths\n",
    "        if (lyric_path := convert_to_lyric_path(file_path, is_ai)) is not None\n",
    "    ]\n",
    "\n",
    "\n",
    "# Convert all file paths from the train, validation, and test sets\n",
    "ai_train_files = process_file_paths(train_files, is_ai=True)\n",
    "human_train_files = process_file_paths(train_files, is_ai=False)\n",
    "\n",
    "ai_val_files = process_file_paths(val_files, is_ai=True)\n",
    "human_val_files = process_file_paths(val_files, is_ai=False)\n",
    "\n",
    "ai_test_files = process_file_paths(test_files, is_ai=True)\n",
    "human_test_files = process_file_paths(test_files, is_ai=False)\n",
    "\n",
    "ai_train_files = [(path, label) for path, label in ai_train_files if path is not None]\n",
    "human_train_files = [(path, label) for path, label in human_train_files if path is not None]\n",
    "\n",
    "ai_val_files = [(path, label) for path, label in ai_val_files if path is not None]\n",
    "human_val_files = [(path, label) for path, label in human_val_files if path is not None]\n",
    "\n",
    "ai_test_files = [(path, label) for path, label in ai_test_files if path is not None]\n",
    "human_test_files = [(path, label) for path, label in human_test_files if path is not None]\n",
    "\n",
    "\n",
    "# Combine all files into a single list for each split\n",
    "train_files_combined = ai_train_files + human_train_files\n",
    "val_files_combined = ai_val_files + human_val_files\n",
    "test_files_combined = ai_test_files + human_test_files\n",
    "\n",
    "# Shuffle the data if needed\n",
    "random.shuffle(train_files_combined)\n",
    "random.shuffle(val_files_combined)\n",
    "random.shuffle(test_files_combined)\n",
    "\n",
    "# Example of how you might check the splits\n",
    "print(f\"Training set size: {len(train_files_combined)}\")\n",
    "print(f\"Validation set size: {len(val_files_combined)}\")\n",
    "print(f\"Test set size: {len(test_files_combined)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4b5266b-99d9-4ac7-86dd-7dfaacc3b7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LyricsDataset(Dataset):\n",
    "    def __init__(self, file_paths, tokenizer, max_length=512):\n",
    "        self.file_paths = file_paths\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path, label = self.file_paths[idx]\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': torch.tensor(label, dtype=torch.long),\n",
    "            'filename': os.path.basename(file_path)  # This is important\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84527086-90bc-45b1-9281-9b05435cad45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=2, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "\n",
    "    def should_stop(self, score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            return False\n",
    "        elif score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6245a977-8c91-4e6b-a07c-661ece9cf1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets for training, validation, and testing\n",
    "train_dataset = LyricsDataset(train_files_combined, tokenizer)\n",
    "val_dataset = LyricsDataset(val_files_combined, tokenizer)\n",
    "test_dataset = LyricsDataset(test_files_combined, tokenizer)\n",
    "\n",
    "# Create DataLoader for each dataset\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27ad65c0-975b-431d-99f5-40135fb0c4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, val_dataloader, device, num_epochs=3, lr=5e-5, weight_decay=0.01):\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    num_training_steps = num_epochs * len(train_dataloader)\n",
    "    \n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=\"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch + 1}\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch + 1} Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_dataloader, desc=\"Validating\"):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                val_loss += outputs.loss.item()\n",
    "                preds = torch.argmax(outputs.logits, dim=1)\n",
    "                correct_predictions += (preds == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_dataloader)\n",
    "        accuracy = correct_predictions / total_samples\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}, Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94185bd2-9bab-4526-916f-f0a8f3358ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████████████████████████████████████████████████████| 1770/1770 [03:25<00:00,  8.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training Loss: 0.3498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████████████████████████████████████████████████████████████| 370/370 [00:35<00:00, 10.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3111, Accuracy: 0.8675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████████████████████████████████████████████████████| 1770/1770 [02:11<00:00, 13.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Training Loss: 0.2228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████████████████████████████████████████████████████████████| 370/370 [00:17<00:00, 21.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3167, Accuracy: 0.8749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████████████████████████████████████████████████████| 1770/1770 [02:11<00:00, 13.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Training Loss: 0.0996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████████████████████████████████████████████████████████████| 370/370 [00:17<00:00, 21.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.4060, Accuracy: 0.8729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_dataloader, val_dataloader, device, num_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "945db6fc-c90c-49f3-9849-949cc60f6cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_early_stopping(model, train_dataloader, val_dataloader, device, num_epochs=3, lr=5e-5,\n",
    "                                    weight_decay=0.01, patience=2):\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    num_training_steps = num_epochs * len(train_dataloader)\n",
    "\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=\"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "\n",
    "    early_stopper = EarlyStopping(patience=patience)\n",
    "\n",
    "    best_val_accuracy = 0.0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch in train_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                preds = torch.argmax(outputs.logits, dim=1)\n",
    "                correct_predictions += (preds == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "\n",
    "        val_accuracy = correct_predictions / total_samples\n",
    "        print(f\"Epoch {epoch+1}: Validation Accuracy = {val_accuracy:.4f}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_model_state = deepcopy(model.state_dict())\n",
    "\n",
    "        if early_stopper.should_stop(val_accuracy):\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    # Load best weights before returning\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return model, best_val_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "345db706-88da-4d89-8f60-0973d59ad8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_dataloader, device, output_csv_path=\"predictions.csv\", save_model_path=\"best_model.pt\"):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=\"Evaluating on Test Set\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            filenames = batch['filename']\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            probs = F.softmax(outputs.logits, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "            correct_predictions += (preds == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "            for i in range(len(filenames)):\n",
    "                results.append({\n",
    "                    \"filename\": filenames[i],\n",
    "                    \"prob_ai\": probs[i][0].item(),\n",
    "                    \"prob_human\": probs[i][1].item(),\n",
    "                    \"true_label\": labels[i].item(),\n",
    "                    \"pred_label\": preds[i].item()\n",
    "                })\n",
    "\n",
    "    # Save results to CSV\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Predictions saved to {output_csv_path}\")\n",
    "\n",
    "    # Save model weights\n",
    "    torch.save(model.state_dict(), save_model_path)\n",
    "    print(f\"Model weights saved to {save_model_path}\")\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed552e8c-bc80-4195-81d3-06bce5ec6aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on Test Set: 100%|██████████████████████████████████████████████████████| 371/371 [00:34<00:00, 10.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to clean_lyrics_test_large_predictions.csv\n",
      "Model weights saved to clean_lyrics_model.pt\n",
      "Test Accuracy: 0.8746\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate_model(\n",
    "    model,\n",
    "    test_dataloader,\n",
    "    device,\n",
    "    output_csv_path=\"clean_lyrics_test_large_predictions.csv\",\n",
    "    save_model_path=\"clean_lyrics_model.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74af036d-a08c-4a24-9b5e-309a6144f666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "def random_search(model_class, train_dataloader, val_dataloader, device,\n",
    "                  param_distributions, n_trials=10, patience=2, save_path=\"best_model_random.pt\"):\n",
    "    best_accuracy = 0.0\n",
    "    best_params = None\n",
    "    best_model_state = None\n",
    "    results = []\n",
    "\n",
    "    keys = list(param_distributions.keys())\n",
    "\n",
    "    for i in range(n_trials):\n",
    "        # Sample random hyperparameters\n",
    "        sampled_params = {k: random.choice(param_distributions[k]) for k in keys}\n",
    "        print(f\"\\nTrial {i + 1}: {sampled_params}\")\n",
    "\n",
    "        model = model_class().to(device)\n",
    "\n",
    "        trained_model, val_accuracy = train_model_with_early_stopping(\n",
    "            model=model,\n",
    "            train_dataloader=train_dataloader,\n",
    "            val_dataloader=val_dataloader,\n",
    "            device=device,\n",
    "            num_epochs=sampled_params['num_epochs'],\n",
    "            lr=sampled_params['lr'],\n",
    "            weight_decay=sampled_params['weight_decay'],\n",
    "            patience=patience\n",
    "        )\n",
    "\n",
    "        results.append((sampled_params, val_accuracy))\n",
    "\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            best_params = sampled_params\n",
    "            best_model_state = deepcopy(trained_model.state_dict())\n",
    "\n",
    "    torch.save(best_model_state, save_path)\n",
    "    print(f\"\\nBest model saved to {save_path}\")\n",
    "    print(f\"Best Hyperparameters: {best_params}, Validation Accuracy: {best_accuracy:.4f}\")\n",
    "    return best_params, best_accuracy, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c351fa1d-d765-4a7e-b81d-cb121198d447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial 1: {'lr': 3e-05, 'weight_decay': 0.001, 'num_epochs': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Validation Accuracy = 0.8695\n",
      "Epoch 2: Validation Accuracy = 0.8769\n",
      "\n",
      "Trial 2: {'lr': 3e-05, 'weight_decay': 0.001, 'num_epochs': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Validation Accuracy = 0.8742\n",
      "Epoch 2: Validation Accuracy = 0.8753\n",
      "\n",
      "Trial 3: {'lr': 1e-05, 'weight_decay': 0.01, 'num_epochs': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Validation Accuracy = 0.8631\n",
      "Epoch 2: Validation Accuracy = 0.8773\n",
      "\n",
      "Trial 4: {'lr': 5e-05, 'weight_decay': 0.001, 'num_epochs': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Validation Accuracy = 0.8634\n",
      "Epoch 2: Validation Accuracy = 0.8692\n",
      "Epoch 3: Validation Accuracy = 0.8766\n",
      "\n",
      "Trial 5: {'lr': 3e-05, 'weight_decay': 0.01, 'num_epochs': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Validation Accuracy = 0.8631\n",
      "Epoch 2: Validation Accuracy = 0.8769\n",
      "Epoch 3: Validation Accuracy = 0.8712\n",
      "\n",
      "Trial 6: {'lr': 3e-05, 'weight_decay': 0.001, 'num_epochs': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Validation Accuracy = 0.8702\n",
      "Epoch 2: Validation Accuracy = 0.8577\n",
      "Epoch 3: Validation Accuracy = 0.8654\n",
      "Early stopping triggered.\n",
      "\n",
      "Trial 7: {'lr': 3e-05, 'weight_decay': 0.001, 'num_epochs': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Validation Accuracy = 0.8634\n",
      "Epoch 2: Validation Accuracy = 0.8759\n",
      "Epoch 3: Validation Accuracy = 0.8648\n",
      "Epoch 4: Validation Accuracy = 0.8614\n",
      "Early stopping triggered.\n",
      "\n",
      "Trial 8: {'lr': 1e-05, 'weight_decay': 0.001, 'num_epochs': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Validation Accuracy = 0.8763\n",
      "Epoch 2: Validation Accuracy = 0.8790\n",
      "Epoch 3: Validation Accuracy = 0.8769\n",
      "Epoch 4: Validation Accuracy = 0.8749\n",
      "Early stopping triggered.\n",
      "\n",
      "Trial 9: {'lr': 5e-05, 'weight_decay': 0.01, 'num_epochs': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Validation Accuracy = 0.8668\n",
      "Epoch 2: Validation Accuracy = 0.8600\n",
      "Epoch 3: Validation Accuracy = 0.8607\n",
      "Early stopping triggered.\n",
      "\n",
      "Trial 10: {'lr': 3e-05, 'weight_decay': 0.01, 'num_epochs': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Validation Accuracy = 0.8732\n",
      "Epoch 2: Validation Accuracy = 0.8807\n",
      "Epoch 3: Validation Accuracy = 0.8749\n",
      "Epoch 4: Validation Accuracy = 0.8719\n",
      "Early stopping triggered.\n",
      "\n",
      "Best model saved to best_model_randomsearch.pt\n",
      "Best Hyperparameters: {'lr': 3e-05, 'weight_decay': 0.01, 'num_epochs': 4}, Validation Accuracy: 0.8807\n"
     ]
    }
   ],
   "source": [
    "param_distributions = {\n",
    "    'lr': [5e-5, 3e-5, 1e-5],\n",
    "    'weight_decay': [0.01, 0.001],\n",
    "    'num_epochs': [2, 3, 4]\n",
    "}\n",
    "\n",
    "best_params, best_acc, search_results = random_search(\n",
    "    model_class=build_model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    device=device,\n",
    "    param_distributions=param_distributions,\n",
    "    n_trials=10,\n",
    "    patience=2,\n",
    "    save_path=\"best_model_randomsearch.pt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a499da6-d33a-416a-bdb7-017290e40957",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_params)\n",
    "print(best_acc)\n",
    "print(search_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
