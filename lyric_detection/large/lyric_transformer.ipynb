{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ec86650-2c1c-4a81-9ddf-081016942431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, get_scheduler\n",
    "\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4bd8f69-b85b-4c57-9fd4-a526c12168ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained DistilBERT tokenizer and model for binary classification\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5eb4839-30a0-4a47-b616-1417a1bde112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72a06458-c78c-4273-8a84-f0d55415187d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 9792\n",
      "Validation set size: 2157\n",
      "Test set size: 2104\n"
     ]
    }
   ],
   "source": [
    "# Directory paths for the segments and lyrics\n",
    "ai_segments_path = \"/data/sg2121/fypdataset/dataset_large/normal_data/ai_segments\"\n",
    "human_segments_path = \"/data/sg2121/fypdataset/dataset_large/normal_data/human\"\n",
    "ai_lyrics_path = \"/data/sg2121/fypdataset/dataset_large/lyrics/ai\"\n",
    "human_lyrics_path = \"/data/sg2121/fypdataset/dataset_large/lyrics/human\"\n",
    "\n",
    "# Helper function to read file paths from a text file\n",
    "def read_file_paths(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        return [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Read all file paths from the text files\n",
    "train_files = read_file_paths('/data/sg2121/aimusicdetector/train_test_split/train_files_large.txt')\n",
    "val_files = read_file_paths('/data/sg2121/aimusicdetector/train_test_split/val_files_large.txt')\n",
    "test_files = read_file_paths('/data/sg2121/aimusicdetector/train_test_split/test_files_large.txt')\n",
    "\n",
    "# Function to convert segment file path to lyric file path\n",
    "def convert_to_lyric_path(file_path, is_ai):\n",
    "    if is_ai:\n",
    "        if file_path.startswith(ai_segments_path):\n",
    "            base_lyrics_path = ai_lyrics_path\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        if file_path.startswith(human_segments_path):\n",
    "            base_lyrics_path = human_lyrics_path\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # Construct full lyric file path\n",
    "    file_name = os.path.basename(file_path).replace('.mp3', '_lyrics.txt')\n",
    "    lyric_path = os.path.join(base_lyrics_path, file_name)\n",
    "\n",
    "    # Now check if the path exists\n",
    "    if not os.path.exists(lyric_path):\n",
    "        return None\n",
    "\n",
    "    return lyric_path\n",
    "\n",
    "\n",
    "\n",
    "# Process the file lists and create tuples of (lyric_path, label)\n",
    "def process_file_paths(file_paths, is_ai):\n",
    "    return [\n",
    "        (lyric_path, 0 if is_ai else 1)\n",
    "        for file_path in file_paths\n",
    "        if (lyric_path := convert_to_lyric_path(file_path, is_ai)) is not None\n",
    "    ]\n",
    "\n",
    "\n",
    "# Convert all file paths from the train, validation, and test sets\n",
    "ai_train_files = process_file_paths(train_files, is_ai=True)\n",
    "human_train_files = process_file_paths(train_files, is_ai=False)\n",
    "\n",
    "ai_val_files = process_file_paths(val_files, is_ai=True)\n",
    "human_val_files = process_file_paths(val_files, is_ai=False)\n",
    "\n",
    "ai_test_files = process_file_paths(test_files, is_ai=True)\n",
    "human_test_files = process_file_paths(test_files, is_ai=False)\n",
    "\n",
    "ai_train_files = [(path, label) for path, label in ai_train_files if path is not None]\n",
    "human_train_files = [(path, label) for path, label in human_train_files if path is not None]\n",
    "\n",
    "ai_val_files = [(path, label) for path, label in ai_val_files if path is not None]\n",
    "human_val_files = [(path, label) for path, label in human_val_files if path is not None]\n",
    "\n",
    "ai_test_files = [(path, label) for path, label in ai_test_files if path is not None]\n",
    "human_test_files = [(path, label) for path, label in human_test_files if path is not None]\n",
    "\n",
    "\n",
    "# Combine all files into a single list for each split\n",
    "train_files_combined = ai_train_files + human_train_files\n",
    "val_files_combined = ai_val_files + human_val_files\n",
    "test_files_combined = ai_test_files + human_test_files\n",
    "\n",
    "# Shuffle the data if needed\n",
    "random.shuffle(train_files_combined)\n",
    "random.shuffle(val_files_combined)\n",
    "random.shuffle(test_files_combined)\n",
    "\n",
    "# Example of how you might check the splits\n",
    "print(f\"Training set size: {len(train_files_combined)}\")\n",
    "print(f\"Validation set size: {len(val_files_combined)}\")\n",
    "print(f\"Test set size: {len(test_files_combined)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4b5266b-99d9-4ac7-86dd-7dfaacc3b7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LyricsDataset(Dataset):\n",
    "    def __init__(self, file_paths, tokenizer, max_length=512):\n",
    "        self.file_paths = file_paths\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path, label = self.file_paths[idx]\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': torch.tensor(label, dtype=torch.long),\n",
    "            'filename': os.path.basename(file_path)  # This is important\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6245a977-8c91-4e6b-a07c-661ece9cf1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets for training, validation, and testing\n",
    "train_dataset = LyricsDataset(train_files_combined, tokenizer)\n",
    "val_dataset = LyricsDataset(val_files_combined, tokenizer)\n",
    "test_dataset = LyricsDataset(test_files_combined, tokenizer)\n",
    "\n",
    "# Create DataLoader for each dataset\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "339c49dd-e77d-40e8-90a4-c336aa0da749",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 1224/1224 [03:01<00:00,  6.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Loss: 0.3247314989566803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 270/270 [00:13<00:00, 20.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.4721, Accuracy: 0.5059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 1224/1224 [03:25<00:00,  5.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 completed. Loss: 0.11007271707057953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 270/270 [00:16<00:00, 16.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.4469, Accuracy: 0.5134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 1224/1224 [03:56<00:00,  5.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 completed. Loss: 0.5416213274002075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 270/270 [00:17<00:00, 15.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5975, Accuracy: 0.5200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set up the optimizer and training loop\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",                     # \"linear\", \"cosine\", \"cosine_with_restarts\", etc.\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,               # Optional: small warmup period to avoid large initial updates\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):  # Number of epochs\n",
    "    model.train()\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        # Ensure batch is a dictionary of tensors\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update learning rate\n",
    "        lr_scheduler.step() \n",
    "    \n",
    "    print(f\"Epoch {epoch + 1} completed. Loss: {loss.item()}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dataloader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            val_loss += outputs.loss.item()\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            correct_predictions += (preds == labels).sum().item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    accuracy = correct_predictions / len(val_files)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}, Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fba58e6-d6c1-4528-ac40-141ae5388246",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 263/263 [00:17<00:00, 15.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "results = []\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "for batch in tqdm(test_dataloader):\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    filenames = batch['filename']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        probs = F.softmax(outputs.logits, dim=1)\n",
    "        preds = torch.argmax(probs, dim=1)  # Predicted label\n",
    "\n",
    "        # Calculate the number of correct predictions\n",
    "        correct_predictions += (preds == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "        # Save results\n",
    "        for i in range(len(filenames)):\n",
    "            results.append({\n",
    "                \"filename\": filenames[i],\n",
    "                \"prob_ai\": probs[i][0].item(),\n",
    "                \"prob_human\": probs[i][1].item(),\n",
    "                \"true_label\": labels[i].item(),\n",
    "                \"pred_label\": preds[i].item()\n",
    "            })\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"clean_lyrics_test_large_predictions.csv\", index=False)\n",
    "\n",
    "# Calculate and print accuracy\n",
    "accuracy = correct_predictions / total_predictions\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345db706-88da-4d89-8f60-0973d59ad8b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
