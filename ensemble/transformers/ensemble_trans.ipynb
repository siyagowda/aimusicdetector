{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c534e799-11cf-4ceb-80cd-db4ba1223931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO\n"
     ]
    }
   ],
   "source": [
    "print(\"HELLO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42ecda73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import itertools\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af8e278f-4584-4914-b64d-812884e301b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7995\n",
      "7995\n",
      "7995\n",
      "2967\n"
     ]
    }
   ],
   "source": [
    "# Load the predictions from the CSV files\n",
    "mel_spec_csv = \"/vol/bitbucket/sg2121/fyp/aimusicdetector/music_cnn/large/mel-spec/mel-spec_test_large_with_aug_predictions.csv\"\n",
    "cqt_csv = \"/vol/bitbucket/sg2121/fyp/aimusicdetector/music_cnn/large/cqt/cqt_test_large_with_aug_predictions.csv\"\n",
    "mfcc_csv = \"/vol/bitbucket/sg2121/fyp/aimusicdetector/music_cnn/large/mfcc/mfcc_test_large_with_aug_predictions.csv\"\n",
    "plp_csv = \"/vol/bitbucket/sg2121/fyp/aimusicdetector/music_cnn/large/plp/plp_test_large_with_aug_predictions.csv\"\n",
    "chrm_csv = \"/vol/bitbucket/sg2121/fyp/aimusicdetector/music_cnn/large/chromagram/chromagram_test_large_with_aug_predictions.csv\"\n",
    "clean_lyrics_csv = \"/vol/bitbucket/sg2121/fyp/aimusicdetector/lyric_detection/large/clean_lyrics_test_large_predictions.csv\"\n",
    "\n",
    "mel_spec_trans_csv = \"/vol/bitbucket/sg2121/fyp/aimusicdetector/music_transformer/large/mel-spec/tensors_test_large_with_aug_predictions.csv\"\n",
    "mfcc_trans_csv = \"/vol/bitbucket/sg2121/fyp/aimusicdetector/music_transformer/large/mfcc/tensors_test_large_with_aug_predictions.csv\"\n",
    "\n",
    "# Read the CSV files into pandas DataFrames\n",
    "df_mel = pd.read_csv(mel_spec_csv)\n",
    "df_clean_lyrics = pd.read_csv(clean_lyrics_csv)\n",
    "df_mfcc = pd.read_csv(mfcc_csv)\n",
    "df_plp = pd.read_csv(plp_csv)\n",
    "df_cqt = pd.read_csv(cqt_csv)\n",
    "df_chrm = pd.read_csv(chrm_csv)\n",
    "\n",
    "df_mel_t = pd.read_csv(mel_spec_trans_csv)\n",
    "df_mfcc_t = pd.read_csv(mfcc_trans_csv)\n",
    "\n",
    "print(len(df_mel.index))\n",
    "\n",
    "df_mel['base_filename'] = df_mel['filename'].str.replace(r'-Mel_Spectrogram\\.png$', '', regex=True)\n",
    "df_clean_lyrics['base_filename'] = df_clean_lyrics['filename'].str.replace(r'_lyrics\\.txt$', '', regex=True)\n",
    "df_mfcc['base_filename'] = df_mfcc['filename'].str.replace(r'-MFCC\\.png$', '', regex=True)\n",
    "df_plp['base_filename'] = df_plp['filename'].replace(r'_plp\\.png$', '', regex=True)\n",
    "df_cqt['base_filename'] = df_cqt['filename'].str.replace(r'-CQT\\.png$', '', regex=True)\n",
    "df_chrm['base_filename'] = df_chrm['filename'].str.replace(r'-Chromagram\\.png$', '', regex=True)\n",
    "\n",
    "df_mel_t['base_filename'] = df_mel_t['filename'].str.replace(r'.pt$', '', regex=True)\n",
    "df_mfcc_t['base_filename'] = df_mfcc_t['filename'].str.replace(r'.pt$', '', regex=True)\n",
    "\n",
    "print(len(df_mel.index))\n",
    "print(len(df_mfcc_t.index))\n",
    "print(len(df_clean_lyrics.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32cb36fa-9fb0-44df-94a7-d78ac8ed463d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['filename_lyrics', 'prob_ai_lyrics', 'prob_human_lyrics',\n",
      "       'true_label_lyrics', 'pred_label_lyrics', 'base_filename'],\n",
      "      dtype='object')\n",
      "Index(['filename_mfcc', 'prob_ai_mfcc', 'prob_human_mfcc', 'true_label_mfcc',\n",
      "       'pred_label_mfcc', 'base_filename'],\n",
      "      dtype='object')\n",
      "Index(['filename_cqt', 'prob_ai_cqt', 'prob_human_cqt', 'true_label_cqt',\n",
      "       'pred_label_cqt', 'base_filename'],\n",
      "      dtype='object')\n",
      "Index(['filename_melt', 'prob_ai_melt', 'prob_human_melt', 'true_label_melt',\n",
      "       'pred_label_melt', 'base_filename'],\n",
      "      dtype='object')\n",
      "Index(['filename_mfcct', 'prob_ai_mfcct', 'prob_human_mfcct',\n",
      "       'true_label_mfcct', 'pred_label_mfcct', 'base_filename'],\n",
      "      dtype='object')\n",
      "7995\n"
     ]
    }
   ],
   "source": [
    "def rename_columns(df, suffix):\n",
    "    return df.rename(columns={col: f\"{col}{suffix}\" for col in df.columns if col != 'base_filename'})\n",
    "\n",
    "# Add suffixes to avoid column name clashes\n",
    "df_mel = rename_columns(df_mel, '_mel')\n",
    "df_clean_lyrics = rename_columns(df_clean_lyrics, '_lyrics')\n",
    "df_mfcc = rename_columns(df_mfcc, '_mfcc')\n",
    "df_plp = rename_columns(df_plp, '_plp')\n",
    "df_cqt = rename_columns(df_cqt, '_cqt')\n",
    "df_chrm = rename_columns(df_chrm, '_chrm')\n",
    "\n",
    "df_mel_t = rename_columns(df_mel_t, '_melt')\n",
    "df_mfcc_t = rename_columns(df_mfcc_t, '_mfcct')\n",
    "\n",
    "merged_df = df_mel.copy()\n",
    "# Merge sequentially on 'base_filename'\n",
    "for df in [df_clean_lyrics, df_mfcc, df_cqt, df_mel_t, df_mfcc_t]: #, df_plp, df_chrm]\n",
    "    print(df.columns)  # Check before merge\n",
    "    assert 'base_filename' in df.columns\n",
    "    merged_df = pd.merge(merged_df, df, on='base_filename', how='left')\n",
    "    \n",
    "#print(merged_df.head())\n",
    "print(len(merged_df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "668166ad-1765-45f0-8e5f-136150bf54f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_weighted_ensemble(df, weights=None):\n",
    "    # Find all prob columns for AI and Human\n",
    "    ai_cols = [col for col in df.columns if col.startswith('prob_ai_')]\n",
    "    human_cols = [col for col in df.columns if col.startswith('prob_human_')]\n",
    "    \n",
    "    assert len(ai_cols) == len(human_cols), \"Mismatch in number of AI and Human columns\"\n",
    "    \n",
    "    model_keys = [col.replace('prob_ai_', '') for col in ai_cols]\n",
    "    \n",
    "    # If no weights provided, use equal weighting\n",
    "    if weights is None:\n",
    "        weights = {key: 1 / len(model_keys) for key in model_keys}\n",
    "\n",
    "    assert abs(sum(weights.values()) - 1.0) < 1e-6, \"Weights must sum to 1\"\n",
    "    for key in model_keys:\n",
    "        assert key in weights, f\"Missing weight for model: {key}\"\n",
    "\n",
    "    def compute_weighted_prob(row, prob_prefix, weights, keys):\n",
    "        total_weight = 0.0\n",
    "        weighted_sum = 0.0\n",
    "        for key in keys:\n",
    "            col_name = f\"{prob_prefix}_{key}\"\n",
    "            value = row.get(col_name)\n",
    "            if pd.notna(value):\n",
    "                weighted_sum += value * weights[key]\n",
    "                total_weight += weights[key]\n",
    "        return weighted_sum / total_weight if total_weight > 0 else np.nan\n",
    "    \n",
    "    # Apply to each row\n",
    "    df['weighted_prob_ai'] = df.apply(lambda row: compute_weighted_prob(row, 'prob_ai', weights, model_keys), axis=1)\n",
    "    df['weighted_prob_human'] = df.apply(lambda row: compute_weighted_prob(row, 'prob_human', weights, model_keys), axis=1)\n",
    "    \n",
    "    # Final prediction\n",
    "    df['final_pred_label'] = df.apply(\n",
    "        lambda row: 0 if row['weighted_prob_ai'] > row['weighted_prob_human'] else 1,\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ca5bd6b-b88d-4c99-9027-41af6d5de2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 base_filename  weighted_prob_ai  weighted_prob_human  \\\n",
      "0                        H279N          0.399961             0.600039   \n",
      "1                       H8167N          0.399819             0.600181   \n",
      "2            S4594RN_segment_1          0.599754             0.400246   \n",
      "3   U524RN_segment_1_stretched          0.600004             0.399996   \n",
      "4  U1301RN_segment_2_stretched          0.599983             0.400017   \n",
      "\n",
      "   final_pred_label  \n",
      "0                 1  \n",
      "1                 1  \n",
      "2                 0  \n",
      "3                 0  \n",
      "4                 0  \n",
      "                 base_filename  weighted_prob_ai  weighted_prob_human  \\\n",
      "0                        H279N      8.344359e-10         1.000000e+00   \n",
      "1                       H8167N      4.625185e-06         9.999954e-01   \n",
      "2            S4594RN_segment_1      9.997881e-01         2.119026e-04   \n",
      "3   U524RN_segment_1_stretched      1.000000e+00         5.279717e-10   \n",
      "4  U1301RN_segment_2_stretched      9.999729e-01         2.710462e-05   \n",
      "\n",
      "   final_pred_label  \n",
      "0                 1  \n",
      "1                 1  \n",
      "2                 0  \n",
      "3                 0  \n",
      "4                 0  \n"
     ]
    }
   ],
   "source": [
    "merged_df = apply_weighted_ensemble(merged_df)\n",
    "print(merged_df[['base_filename', 'weighted_prob_ai', 'weighted_prob_human', 'final_pred_label']].head())\n",
    "\n",
    "custom_weights = {\n",
    "    'mel': 0.45,\n",
    "    'lyrics': 0.1,\n",
    "    'mfcc': 0.3,\n",
    "    'plp': 0.0,\n",
    "    'cqt': 0.15,\n",
    "    'chrm': 0.0,\n",
    "    'melt': 0.0,\n",
    "    'mfcct': 0.0\n",
    "}\n",
    "\n",
    "merged_df = apply_weighted_ensemble(merged_df, weights=custom_weights)\n",
    "\n",
    "print(merged_df[['base_filename', 'weighted_prob_ai', 'weighted_prob_human', 'final_pred_label']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "622bf40d-3b51-4734-ad04-d503320d7057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensembled Model Accuracy: 0.9781\n",
      "Precision for ai: 0.9859\n",
      "Recall for ai: 0.9727\n",
      "F1-score for ai: 0.9793\n",
      "False Positive Rate for ai: 0.0157\n",
      "Precision for human: 0.9695\n",
      "Recall for human: 0.9843\n",
      "F1-score for human: 0.9768\n",
      "False Positive Rate for human: 0.0273\n",
      "Ensembled results saved to ensembled_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy, precision, recall based on the final prediction\n",
    "y_true = merged_df['true_label_mel']\n",
    "y_pred = merged_df['final_pred_label']\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "# Compute basic metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=None, labels=[0, 1])\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "\n",
    "fpr_ai = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "fpr_human = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "\n",
    "# Log the results\n",
    "print(f\"Ensembled Model Accuracy: {accuracy:.4f}\")\n",
    "for i, label in enumerate(['ai', 'human']):\n",
    "    print(f\"Precision for {label}: {precision[i]:.4f}\")\n",
    "    print(f\"Recall for {label}: {recall[i]:.4f}\")\n",
    "    print(f\"F1-score for {label}: {f1[i]:.4f}\")\n",
    "    print(f\"False Positive Rate for {label}: {fpr_ai if label == 'ai' else fpr_human:.4f}\")\n",
    "\n",
    "\n",
    "# Save the ensembled results to a new CSV file\n",
    "ensembled_csv_file = \"ensembled_predictions.csv\"\n",
    "merged_df[['base_filename', 'weighted_prob_ai', 'weighted_prob_human', 'final_pred_label', 'true_label_mel']].to_csv(ensembled_csv_file, index=False)\n",
    "merged_df.to_csv(\"full_csv.csv\", index=False)\n",
    "\n",
    "print(f\"Ensembled results saved to {ensembled_csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f11efbc3-64d5-4238-ab8d-35f7567ec863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating weight combinations\n"
     ]
    }
   ],
   "source": [
    "# Define step size and base modalities\n",
    "modalities = ['mel', 'lyrics', 'mfcc', 'cqt', 'melt', 'mfcct'] #, 'plp', 'chrm']\n",
    "step = 0.05\n",
    "\n",
    "# Generate grid of weights summing to 1\n",
    "def generate_weight_combinations(modalities, step=0.1):\n",
    "    ranges = [np.arange(0, 1 + step, step) for _ in modalities]\n",
    "    all_combinations = list(itertools.product(*ranges))\n",
    "    valid_combinations = [\n",
    "        combo for combo in all_combinations if abs(sum(combo) - 1.0) < 1e-6\n",
    "    ]\n",
    "    return [dict(zip(modalities, combo)) for combo in valid_combinations]\n",
    "\n",
    "print(\"Generating weight combinations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fb0cfafe-5e10-499d-a2da-11d6004d4c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53130\n"
     ]
    }
   ],
   "source": [
    "# Prepare grid\n",
    "weight_combinations = generate_weight_combinations(modalities, step=step)\n",
    "\n",
    "print(len(weight_combinations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "66b842a2-39c5-423d-bc0a-12eda035f4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching best weights: 100%|████████████████████████████████████████████| 53130/53130 [3:05:18<00:00,  4.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Ensemble Weights:\n",
      "mel: 0.45\n",
      "lyrics: 0.10\n",
      "mfcc: 0.30\n",
      "cqt: 0.15\n",
      "melt: 0.00\n",
      "mfcct: 0.00\n",
      "\n",
      "Metrics for Best Weights:\n",
      "Accuracy: 0.9781\n",
      "Precision: 0.9777\n",
      "Recall: 0.9785\n",
      "F1: 0.9780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare ground truth\n",
    "y_true = merged_df['true_label_mel'].values \n",
    "\n",
    "best_score = 0\n",
    "best_weights = None\n",
    "best_metrics = None\n",
    "\n",
    "for weights in tqdm(weight_combinations, desc=\"Searching best weights\"):\n",
    "    \n",
    "    df_copy = merged_df.copy()\n",
    "    df_copy = apply_weighted_ensemble(df_copy, weights)\n",
    "\n",
    "    y_pred = df_copy['final_pred_label'].values\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
    "\n",
    "    score = accuracy \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_weights = weights\n",
    "        best_metrics = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }\n",
    "\n",
    "\n",
    "# Show best weights and metrics\n",
    "print(\"\\nBest Ensemble Weights:\")\n",
    "for k, v in best_weights.items():\n",
    "    print(f\"{k}: {v:.2f}\")\n",
    "print(\"\\nMetrics for Best Weights:\")\n",
    "for k, v in best_metrics.items():\n",
    "    print(f\"{k.capitalize()}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5cd075ec-1fc6-49aa-89ee-ed709fa67c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Ensemble Weights:\n",
      "mel: 0.45\n",
      "lyrics: 0.10\n",
      "mfcc: 0.30\n",
      "cqt: 0.15\n",
      "melt: 0.00\n",
      "mfcct: 0.00\n",
      "\n",
      "Best Metrics:\n",
      "Accuracy: 0.9781\n",
      "Precision macro: 0.9777\n",
      "Recall macro: 0.9785\n",
      "F1 macro: 0.9780\n",
      "\n",
      "Per-Class Metrics:\n",
      "\n",
      "Class: Human\n",
      "  Precision: 0.9859\n",
      "  Recall: 0.9727\n",
      "  F1: 0.9793\n",
      "  Accuracy: 0.9781\n",
      "  FPR: 0.0273\n",
      "\n",
      "Class: AI\n",
      "  Precision: 0.9695\n",
      "  Recall: 0.9843\n",
      "  F1: 0.9768\n",
      "  Accuracy: 0.9781\n",
      "  FPR: 0.0157\n"
     ]
    }
   ],
   "source": [
    "df_copy = merged_df.copy()\n",
    "df_copy = apply_weighted_ensemble(df_copy, best_weights)\n",
    "\n",
    "y_pred = df_copy['final_pred_label'].values\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
    "\n",
    "# Confusion matrix: [[TN, FP], [FN, TP]]\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Per-class metrics\n",
    "per_class = {\n",
    "    \"Human\": {\n",
    "        \"Precision\": tn / (tn + fn) if (tn + fn) > 0 else 0,\n",
    "        \"Recall\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"F1\": (2 * tn) / (2 * tn + fn + fp) if (2 * tn + fn + fp) > 0 else 0,\n",
    "        \"Accuracy\": (tn + tp) / (tn + fp + fn + tp),\n",
    "        \"FPR\": fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "    },\n",
    "    \"AI\": {\n",
    "        \"Precision\": tp / (tp + fp) if (tp + fp) > 0 else 0,\n",
    "        \"Recall\": tp / (tp + fn) if (tp + fn) > 0 else 0,\n",
    "        \"F1\": (2 * tp) / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 0,\n",
    "        \"Accuracy\": (tn + tp) / (tn + fp + fn + tp),\n",
    "        \"FPR\": fn / (fn + tp) if (fn + tp) > 0 else 0,\n",
    "    }\n",
    "}\n",
    "\n",
    "score = accuracy \n",
    "best_score = score\n",
    "best_metrics = {\n",
    "    'accuracy': accuracy,\n",
    "    'precision_macro': precision,\n",
    "    'recall_macro': recall,\n",
    "    'f1_macro': f1,\n",
    "    'per_class': per_class\n",
    "}\n",
    "\n",
    "# Show best weights and metrics\n",
    "print(\"\\nBest Ensemble Weights:\")\n",
    "for k, v in best_weights.items():\n",
    "    print(f\"{k}: {v:.2f}\")\n",
    "print(\"\\nBest Metrics:\")\n",
    "for k, v in best_metrics.items():\n",
    "    if k != 'per_class':\n",
    "        print(f\"{k.replace('_', ' ').capitalize()}: {v:.4f}\")\n",
    "    else:\n",
    "        print(\"\\nPer-Class Metrics:\")\n",
    "        for cls, metrics in v.items():\n",
    "            print(f\"\\nClass: {cls}\")\n",
    "            for metric_name, val in metrics.items():\n",
    "                print(f\"  {metric_name}: {val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "548c6024-e777-4cbf-a195-23fee4f84ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best weights saved to best_ensemble_weights.json\n",
      "Final ensembled predictions saved to best_weights_ensembled_predictions.csv\n",
      "Full data (with all features) saved to full_ensembled_output.csv\n"
     ]
    }
   ],
   "source": [
    "# Save best weights to a JSON file\n",
    "with open(\"best_ensemble_weights.json\", \"w\") as f:\n",
    "    json.dump(best_weights, f, indent=4)\n",
    "\n",
    "print(\"Best weights saved to best_ensemble_weights.json\")\n",
    "\n",
    "merged_df = apply_weighted_ensemble(merged_df, weights=best_weights)\n",
    "\n",
    "# Save only the key prediction outputs\n",
    "ensembled_csv_file = \"best_weights_ensembled_predictions.csv\"\n",
    "merged_df[['base_filename', 'weighted_prob_ai', 'weighted_prob_human', 'final_pred_label', 'true_label_mel']].to_csv(ensembled_csv_file, index=False)\n",
    "\n",
    "# Save the full DataFrame\n",
    "merged_df.to_csv(\"full_ensembled_output.csv\", index=False)\n",
    "\n",
    "print(f\"Final ensembled predictions saved to {ensembled_csv_file}\")\n",
    "print(f\"Full data (with all features) saved to full_ensembled_output.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779ccb1f-f772-4d9f-b8f5-065906fb05f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
